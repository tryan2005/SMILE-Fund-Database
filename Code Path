import sqlite3, pandas as pd

# make a file-backed DB so it persists between sessions
con = sqlite3.connect("smilefund.db")
cur = con.cursor()

cur.execute("""
CREATE TABLE IF NOT EXISTS dim_company (
  company_id INTEGER PRIMARY KEY AUTOINCREMENT,
  cik TEXT NOT NULL UNIQUE,
  name TEXT NOT NULL,
  ticker TEXT,
  sector TEXT,
  industry TEXT,
  fiscal_year_end_month INTEGER CHECK (fiscal_year_end_month BETWEEN 1 AND 12)
);
""")

cur.executemany("""
INSERT INTO dim_company (cik, name, ticker, sector, industry, fiscal_year_end_month)
VALUES (?, ?, ?, ?, ?, ?)
""", [
 ('0001652044','Alphabet Inc.','GOOG','Communication Services','Interactive Media & Services',12),
 ('0000789019','Microsoft Corporation','MSFT','Technology','Software—Infrastructure',6),
 ('0000320193','Apple Inc.','AAPL','Technology','Technology Hardware',9),
])

con.commit()

# inspect
pd.read_sql_query("SELECT * FROM dim_company ORDER BY company_id;", con)
%pip install psycopg2-binary SQLAlchemy pandas
import os
os.chdir("C:/Users/VTN183/OneDrive - University of Tennessee/smilefund_project/")

import os
import pandas as pd

proj = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
os.makedirs(os.path.join(proj, "warehouse", "parquet"), exist_ok=True)
base = os.path.join(proj, "warehouse", "parquet")
base

import pandas as pd

dim_company = pd.DataFrame([
    {"company_id": 1, "cik": "0001652044", "name": "Alphabet Inc.", "ticker": "GOOG",
     "sector": "Communication Services", "industry": "Interactive Media & Services",
     "fiscal_year_end_month": 12},
    {"company_id": 2, "cik": "0000789019", "name": "Microsoft Corporation", "ticker": "MSFT",
     "sector": "Technology", "industry": "Software—Infrastructure",
     "fiscal_year_end_month": 6},
    {"company_id": 3, "cik": "0000320193", "name": "Apple Inc.", "ticker": "AAPL",
     "sector": "Technology", "industry": "Technology Hardware",
     "fiscal_year_end_month": 9},
])

# (optional) set explicit dtypes; helpful later
dim_company = dim_company.astype({
    "company_id": "int64",
    "cik": "string",
    "name": "string",
    "ticker": "string",
    "sector": "string",
    "industry": "string",
    "fiscal_year_end_month": "int64",
})
path_company = os.path.join(base, "dim_company.parquet")
dim_company.to_parquet(path_company, index=False, engine="pyarrow")
pd.read_parquet(path_company)
dim_calendar = pd.DataFrame([
    {"calendar_id": 1, "date": "2023-03-31", "fiscal_year": 2023, "fiscal_quarter": "Q1", "fiscal_month": 3, "period_type": "QTR"},
    {"calendar_id": 2, "date": "2023-06-30", "fiscal_year": 2023, "fiscal_quarter": "Q2", "fiscal_month": 6, "period_type": "QTR"},
    {"calendar_id": 3, "date": "2023-09-30", "fiscal_year": 2023, "fiscal_quarter": "Q3", "fiscal_month": 9, "period_type": "QTR"},
    {"calendar_id": 4, "date": "2023-12-31", "fiscal_year": 2023, "fiscal_quarter": "Q4", "fiscal_month": 12, "period_type": "FY"},
]).astype({
    "calendar_id": "int64","date": "string","fiscal_year": "int64","fiscal_quarter": "string",
    "fiscal_month": "int64","period_type": "string"
})

path_calendar = os.path.join(base, "dim_calendar.parquet")
dim_calendar.to_parquet(path_calendar, index=False)
pd.read_parquet(path_calendar)
dim_metric = pd.DataFrame([
    {"metric_id": 1, "metric_name": "Revenue", "xbrl_tag": "Revenues", "normal_balance": "Credit"},
    {"metric_id": 2, "metric_name": "Net Income", "xbrl_tag": "NetIncomeLoss", "normal_balance": "Credit"},
    {"metric_id": 3, "metric_name": "Total Assets", "xbrl_tag": "Assets", "normal_balance": "Debit"},
]).astype({"metric_id":"int64","metric_name":"string","xbrl_tag":"string","normal_balance":"string"})

path_metric = os.path.join(base, "dim_metric.parquet")
dim_metric.to_parquet(path_metric, index=False)
pd.read_parquet(path_metric)
dim_filing = pd.DataFrame([
    {"filing_id": 1, "form_type": "10-K", "filing_date": "2024-02-03",
     "accepted_date": "2024-02-03", "period_end_date": "2023-12-31",
     "accession_number": "0001652044-24-000050", "filing_url": "https://..."},
]).astype({
    "filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
    "period_end_date":"string","accession_number":"string","filing_url":"string"
})

path_filing = os.path.join(base, "dim_filing.parquet")
dim_filing.to_parquet(path_filing, index=False)
pd.read_parquet(path_filing)
dim_unit = pd.DataFrame([
    {"unit_id":1,"unit_code":"USD","category":"currency","iso_currency":"USD","decimals_hint":2,"description":"U.S. dollars"},
    {"unit_id":2,"unit_code":"shares","category":"shares","iso_currency":None,"decimals_hint":0,"description":"Common shares"},
    {"unit_id":3,"unit_code":"USD_per_share","category":"per_share","iso_currency":"USD","decimals_hint":2,"description":"Dollars per share"},
]).astype({"unit_id":"int64","unit_code":"string","category":"string","iso_currency":"string",
          "decimals_hint":"int64","description":"string"})

path_unit = os.path.join(base, "dim_unit.parquet")
dim_unit.to_parquet(path_unit, index=False)
pd.read_parquet(path_unit)
fact_financials = pd.DataFrame([
    {"financial_id": 1, "company_id": 1, "metric_id": 1, "calendar_id": 4,
     "value": 307394000000.00, "unit_id": 1, "filing_id": 1, "consolidated_flag": "Consolidated"},
    {"financial_id": 2, "company_id": 1, "metric_id": 2, "calendar_id": 4,
     "value": 73795000000.00, "unit_id": 1, "filing_id": 1, "consolidated_flag": "Consolidated"},
]).astype({
    "financial_id":"int64","company_id":"int64","metric_id":"int64","calendar_id":"int64",
    "value":"float64","unit_id":"int64","filing_id":"int64","consolidated_flag":"string"
})

path_fin = os.path.join(base, "fact_financials.parquet")
fact_financials.to_parquet(path_fin, index=False)
pd.read_parquet(path_fin)
fact_holdings = pd.DataFrame([
    {"holding_id":1,"company_id":1,"asof_date":"2025-03-31","weight":0.04200,"shares":145,"market_value":30200,"source":"SMILE_FUND"},
    {"holding_id":2,"company_id":2,"asof_date":"2025-03-31","weight":0.03800,"shares":110,"market_value":29000,"source":"SMILE_FUND"},
]).astype({
    "holding_id":"int64","company_id":"int64","asof_date":"string","weight":"float64",
    "shares":"float64","market_value":"float64","source":"string"
})

path_hold = os.path.join(base, "fact_holdings.parquet")
fact_holdings.to_parquet(path_hold, index=False)
pd.read_parquet(path_hold)
import pandas as pd

path = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\Approved List 9.14.25(Approved List 9.14.csv"
df = pd.read_csv(path)
df.head()
import os, pandas as pd
base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
path_company  = os.path.join(base, "dim_company.parquet")
path_holdings = os.path.join(base, "fact_holdings.parquet")
csv_path = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\Approved List 9.14.25(Approved List 9.14.csv"
df = pd.read_csv(csv_path)

# lower/strip col names
df.columns = [c.strip().lower() for c in df.columns]

# map common variants to our target names
RENAME_MAP = {
    "ticker": "ticker",
    "symbol": "ticker",
    "company": "name",
    "company name": "name",
    "name": "name",
    "sector": "sector",
    "gics sector": "sector",
    "weight": "weight",
    "weight %": "weight",
    "portfolio weight": "weight",
    "asof": "asof_date",
    "as_of": "asof_date",
    "as of": "asof_date",
    "date": "asof_date"
}

df = df.rename(columns={c: RENAME_MAP.get(c, c) for c in df.columns})

# keep only the columns we know how to use (others are ignored for now)
keep_cols = [c for c in ["ticker","name","sector","weight","asof_date"] if c in df.columns]
df = df[keep_cols].copy()
df.head()
import pandas as pd
import re

# df already loaded with a 'ticker' column
tickers_raw = df["ticker"].astype(str)

def normalize_ticker(s: str) -> str:
    s = s.strip().upper()
    s = s.replace(" ", "")
    # convert share-class separator "/" to "." (e.g., BRK/B -> BRK.B)
    s = re.sub(r"/", ".", s)
    return s

df["ticker_norm"] = tickers_raw.map(normalize_ticker)

# show before/after for first few rows
preview = df[["ticker", "ticker_norm"]].head(10)
preview
import os, pandas as pd

base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
path_company = os.path.join(base, "dim_company.parquet")

# if the file exists, load it; else start empty with the right columns
if os.path.exists(path_company):
    dim_company = pd.read_parquet(path_company)
else:
    dim_company = pd.DataFrame(columns=[
        "company_id","cik","name","ticker","sector","industry","fiscal_year_end_month"
    ])
    dim_company = dim_company.astype({
        "company_id":"int64", "cik":"string","name":"string","ticker":"string",
        "sector":"string","industry":"string","fiscal_year_end_month":"float64"
    })

dim_company.head()
# make sure we have the normalized ticker column from the prior step
assert "ticker_norm" in df.columns, "Run the normalization step first."

# unique tickers from your CSV
incoming = (df[["ticker_norm"]]
            .dropna()
            .drop_duplicates()
            .rename(columns={"ticker_norm":"ticker"}))

# left-join to see which are missing from dim_company
check = incoming.merge(dim_company[["ticker","company_id"]], on="ticker", how="left")
to_add = check[check["company_id"].isna()].drop(columns=["company_id"]).copy()

print(f"New tickers to add: {len(to_add)}")
to_add.head(20)
if len(to_add) > 0:
    # next sequential company_id
    next_id = (dim_company["company_id"].max() + 1) if len(dim_company) else 1
    new_ids = list(range(next_id, next_id + len(to_add)))

    n = len(to_add)  # store length once

    new_rows = pd.DataFrame({
        "company_id": new_ids,
        "cik": ["" for _ in range(n)],
        "name": ["" for _ in range(n)],
        "ticker": to_add["ticker"].astype("string"),
        "sector": ["" for _ in range(n)],
        "industry": ["" for _ in range(n)],
        "fiscal_year_end_month": [None for _ in range(n)]
    })

    dim_company = pd.concat([dim_company, new_rows], ignore_index=True)
    dim_company = dim_company.drop_duplicates(subset=["ticker"], keep="first").reset_index(drop=True)

    dim_company.to_parquet(path_company, index=False)

# reload to confirm
dim_company = pd.read_parquet(path_company)
print(f"dim_company rows: {len(dim_company)}")
dim_company.tail(10)
import pandas as pd
import os
from datetime import date

base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
path_company  = os.path.join(base, "dim_company.parquet")
path_holdings = os.path.join(base, "fact_holdings.parquet")

# 1) reload company dim (has company_id + ticker)
dim_company = pd.read_parquet(path_company)[["company_id","ticker"]]

# 2) make a clean ticker list from your dataframe (the one with ticker_norm)
assert "ticker_norm" in df.columns, "Run the normalization step first."
tickers = (df[["ticker_norm"]]
           .dropna()
           .drop_duplicates()
           .rename(columns={"ticker_norm":"ticker"}))

# 3) attach company_id to each ticker
hold = tickers.merge(dim_company, on="ticker", how="left")

# 4) add minimal holding fields
asof = "2025-03-31"   # ← set the date you want (or str(date.today()))
preview_holdings = pd.DataFrame({
    "company_id": hold["company_id"].astype("Int64"),  # allow nulls to show if any missing
    "asof_date":  asof,
    "weight":     pd.NA,        # will fill later
    "shares":     pd.NA,
    "market_value": pd.NA,
    "source":     "SMILE_FUND"
})

preview_holdings.head(10)
# load existing file if present
if os.path.exists(path_holdings):
    existing = pd.read_parquet(path_holdings)
else:
    existing = pd.DataFrame(columns=[
        "holding_id","company_id","asof_date","weight","shares","market_value","source"
    ])

# build new block with temporary ids
new_block = preview_holdings.copy()
new_block = new_block.astype({
    "company_id":"int64",
    "asof_date":"string",
    "source":"string"
})

# combine and assign holding_id sequentially
combined = pd.concat([existing, new_block], ignore_index=True)

# de-dup just in case (one row per company per as-of date)
combined = combined.drop_duplicates(subset=["company_id","asof_date"], keep="last").reset_index(drop=True)

combined["holding_id"] = range(1, len(combined)+1)

# save
combined.to_parquet(path_holdings, index=False)

# quick look
pd.read_parquet(path_holdings).head(12)
import os, pandas as pd

base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
out_dir = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\outputs"
os.makedirs(out_dir, exist_ok=True)

path_company  = os.path.join(base, "dim_company.parquet")
path_holdings = os.path.join(base, "fact_holdings.parquet")

dim_company = pd.read_parquet(path_company)[["company_id","ticker","name","sector","industry"]]
holdings    = pd.read_parquet(path_holdings)

review = (holdings
          .merge(dim_company, on="company_id", how="left")
          .sort_values(["asof_date","ticker"]))

# quick on-screen check
review.head(15)
print("rows:", len(review), "| unique companies:", review["company_id"].nunique())

# save for eyeballing in Excel
review_csv = os.path.join(out_dir, "holdings_review.csv")
review_parq = os.path.join(out_dir, "holdings_review.parquet")
review.to_csv(review_csv, index=False)
review.to_parquet(review_parq, index=False)
print("Wrote:", review_csv)
import os, pandas as pd

proj_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
parq_base = os.path.join(proj_base, "warehouse", "parquet")
os.makedirs(parq_base, exist_ok=True)

path_filing   = os.path.join(parq_base, "dim_filing.parquet")

# create if missing
if not os.path.exists(path_filing):
    dim_filing = pd.DataFrame(columns=[
        "filing_id","form_type","filing_date","accepted_date",
        "period_end_date","accession_number","filing_url"
    ]).astype({
        "filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
        "period_end_date":"string","accession_number":"string","filing_url":"string"
    })
    dim_filing.to_parquet(path_filing, index=False)
else:
    dim_filing = pd.read_parquet(path_filing)

dim_filing.tail(3)
import os, glob

base_path = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"
files = glob.glob(os.path.join(base_path, "*.json"))

print(f"Found {len(files):,} JSON files")
print("Example:", files[:3])
import pandas as pd
from pathlib import Path

# --- 1️⃣  Revenue tags ---
revenue_tags = [
    "Revenues",
    "RevenueFromContractWithCustomerExcludingAssessedTax",
    "RevenueFromContractWithCustomerIncludingAssessedTax",
    "SalesRevenueNet",
    "SalesRevenueGoodsNet",
    "SalesRevenueServicesNet",
    "TotalRevenuesAndOtherIncome",
    "PremiumsEarnedNet",                # insurance
    "InterestIncomeOperating"           # banks
]

# --- 2️⃣  Net Income tags ---
net_income_tags = [
    "NetIncomeLoss",
    "ProfitLoss",
    "IncomeLossFromContinuingOperations",
    "NetIncomeLossAvailableToCommonStockholdersBasic",
    "NetIncomeLossAttributableToParent"
]

# --- 3️⃣  Asset tags ---
asset_tags = [
    "Assets",
    "AssetsCurrent",
    "TotalAssets",
    "AssetsFairValueDisclosure"
]

# --- 4️⃣  Combine into rows ---
records = []

for tag in revenue_tags:
    records.append({"xbrl_tag": tag, "metric_name": "Revenue", "metric_group": "Revenue", "normal_balance": "Credit"})

for tag in net_income_tags:
    records.append({"xbrl_tag": tag, "metric_name": "Net Income", "metric_group": "Income", "normal_balance": "Credit"})

for tag in asset_tags:
    records.append({"xbrl_tag": tag, "metric_name": "Assets", "metric_group": "Assets", "normal_balance": "Debit"})

# --- 5️⃣  Build DataFrame ---
df_metric = pd.DataFrame(records)
df_metric.insert(0, "metric_id", range(1, len(df_metric) + 1))

# --- 6️⃣  Export to Parquet ---
out_path = Path(r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet\dim_metric.parquet")
df_metric.to_parquet(out_path, index=False)

print("✅ dim_metric.parquet created with", len(df_metric), "rows")
df_metric.head()
import os, json, pandas as pd

# 1) paths
parq_base  = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
facts_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"

path_metric = os.path.join(parq_base, "dim_metric.parquet")

# 2) choose one company file (edit CIK if you want another)
cik = "0001652044"  # Alphabet; change to any CIK you like
cik_file = os.path.join(facts_base, f"CIK{cik}.json")
print("Using:", cik_file)

# 3) load company JSON
with open(cik_file, "r") as f:
    data = json.load(f)

entity = data.get("entityName", "(unknown)")
usgaap_tags = set(data.get("facts", {}).get("us-gaap", {}).keys())
print("Entity:", entity)
print("us-gaap tag count:", len(usgaap_tags))

# 4) load your master metric map
dim_metric = pd.read_parquet(path_metric)

# 5) check tag presence by metric_group
def presence_for_group(group_name):
    subset = dim_metric[dim_metric["metric_group"] == group_name].copy()
    subset["present"] = subset["xbrl_tag"].apply(lambda t: t in usgaap_tags)
    return subset[["xbrl_tag","metric_name","metric_group","present"]].sort_values("xbrl_tag")

for grp in ["Revenue", "Income", "Assets"]:
    print(f"\n=== {grp} tags present? ===")
    display(presence_for_group(grp).head(50))
import os, json, pandas as pd

# paths + target
facts_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"
cik = "0001652044"                       # Alphabet (change if you want another company)
tag = "Revenues"                         # from your presence table
cik_file = os.path.join(facts_base, f"CIK{cik}.json")

with open(cik_file, "r") as f:
    data = json.load(f)

# get all unit series for the tag (e.g., USD, USDm, shares, etc.)
series = data.get("facts", {}).get("us-gaap", {}).get(tag, {}).get("units", {})
if not series:
    raise ValueError(f"No units found for {tag}")

# prefer USD if present; otherwise take the first numeric-looking unit
preferred_units = ["USD", "usd", "USDm", "USDth", "USD $"]  # common variants
unit_key = next((u for u in preferred_units if u in series), next(iter(series.keys())))
rows = pd.DataFrame(series[unit_key])

# keep **annual** rows (fp == 'FY')
annual = rows[rows["fp"].str.upper().eq("FY")].copy()

# choose the latest fiscal year available
annual["fy"] = pd.to_numeric(annual["fy"], errors="coerce")
latest = annual.sort_values("fy").tail(1).iloc[0]

preview = {
    "entity": data.get("entityName"),
    "cik": data.get("cik"),
    "tag": tag,
    "unit": unit_key,
    "fy": int(latest["fy"]),
    "period_end": latest["end"],         # YYYY-MM-DD
    "value": float(latest["val"]),
    "form": latest.get("form", ""),
    "accession": latest.get("accn", ""),
    "filed": latest.get("filed", latest["end"])
}
preview
import os, pandas as pd

proj_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
parq_base = os.path.join(proj_base, "warehouse", "parquet")

# ---- load all tables (create empty if missing)
def read_or_empty(path, cols, dtypes):
    if os.path.exists(path):
        return pd.read_parquet(path)
    df = pd.DataFrame(columns=cols).astype(dtypes)
    df.to_parquet(path, index=False)
    return df

path_company = os.path.join(parq_base, "dim_company.parquet")
path_metric  = os.path.join(parq_base, "dim_metric.parquet")
path_unit    = os.path.join(parq_base, "dim_unit.parquet")
path_cal     = os.path.join(parq_base, "dim_calendar.parquet")
path_filing  = os.path.join(parq_base, "dim_filing.parquet")
path_fact    = os.path.join(parq_base, "fact_financials.parquet")

dim_company = read_or_empty(path_company,
    ["company_id","cik","name","ticker","sector","industry","fiscal_year_end_month"],
    {"company_id":"int64","cik":"string","name":"string","ticker":"string",
     "sector":"string","industry":"string","fiscal_year_end_month":"float64"}
)
dim_metric = pd.read_parquet(path_metric)  # you created this already
dim_unit   = read_or_empty(path_unit,
    ["unit_id","unit_code","category","iso_currency","decimals_hint","description"],
    {"unit_id":"int64","unit_code":"string","category":"string","iso_currency":"string",
     "decimals_hint":"float64","description":"string"}
)
dim_calendar = read_or_empty(path_cal,
    ["calendar_id","date","fiscal_year","fiscal_quarter","fiscal_month","period_type"],
    {"calendar_id":"int64","date":"string","fiscal_year":"int64","fiscal_quarter":"string",
     "fiscal_month":"int64","period_type":"string"}
)
dim_filing = read_or_empty(path_filing,
    ["filing_id","form_type","filing_date","accepted_date","period_end_date","accession_number","filing_url"],
    {"filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
     "period_end_date":"string","accession_number":"string","filing_url":"string"}
)
if os.path.exists(path_fact):
    fact_fin = pd.read_parquet(path_fact)
else:
    fact_fin = pd.DataFrame(columns=[
        "financial_id","company_id","metric_id","calendar_id","value","unit_id","filing_id","consolidated_flag"
    ]).astype({"financial_id":"int64","company_id":"int64","metric_id":"int64","calendar_id":"int64",
               "value":"float64","unit_id":"int64","filing_id":"int64","consolidated_flag":"string"})

# ---- tiny helpers (inline) to "ensure" IDs exist
def ensure_company_by_cik(cik_str, ticker=""):
    global dim_company
    hit = dim_company.loc[dim_company["cik"]==cik_str]
    if not hit.empty: 
        return int(hit.iloc[0]["company_id"])
    next_id = (dim_company["company_id"].max()+1) if len(dim_company) else 1
    row = {"company_id":next_id,"cik":cik_str,"name":"","ticker":ticker,
           "sector":"","industry":"","fiscal_year_end_month":None}
    dim_company = pd.concat([dim_company, pd.DataFrame([row])], ignore_index=True)
    dim_company.to_parquet(path_company, index=False)
    return next_id

def ensure_metric_by_tag(xbrl_tag, metric_name=None, normal_balance="Credit", metric_group=None):
    global dim_metric
    hit = dim_metric.loc[dim_metric["xbrl_tag"]==xbrl_tag]
    if not hit.empty: return int(hit.iloc[0]["metric_id"])
    next_id = (dim_metric["metric_id"].max()+1) if len(dim_metric) else 1
    row = {"metric_id":next_id,"xbrl_tag":xbrl_tag,
           "metric_name":metric_name or xbrl_tag,
           "metric_group":metric_group or metric_name or xbrl_tag,
           "normal_balance":normal_balance}
    dim_metric = pd.concat([dim_metric, pd.DataFrame([row])], ignore_index=True)
    dim_metric.to_parquet(path_metric, index=False)
    return next_id

def ensure_unit(unit_code, category="currency", iso="USD", decimals=2, desc=""):
    global dim_unit
    hit = dim_unit.loc[dim_unit["unit_code"]==unit_code]
    if not hit.empty: return int(hit.iloc[0]["unit_id"])
    next_id = (dim_unit["unit_id"].max()+1) if len(dim_unit) else 1
    row = {"unit_id":next_id,"unit_code":unit_code,"category":category,
           "iso_currency":iso,"decimals_hint":decimals,"description":desc}
    dim_unit = pd.concat([dim_unit, pd.DataFrame([row])], ignore_index=True)
    dim_unit.to_parquet(path_unit, index=False)
    return next_id

def ensure_calendar(period_end, fiscal_year, fiscal_quarter, fiscal_month, period_type="FY"):
    global dim_calendar
    hit = dim_calendar.loc[(dim_calendar["date"]==period_end) & (dim_calendar["period_type"]==period_type)]
    if not hit.empty: return int(hit.iloc[0]["calendar_id"])
    next_id = (dim_calendar["calendar_id"].max()+1) if len(dim_calendar) else 1
    row = {"calendar_id":next_id,"date":period_end,"fiscal_year":int(fiscal_year),
           "fiscal_quarter":fiscal_quarter,"fiscal_month":int(fiscal_month),"period_type":period_type}
    dim_calendar = pd.concat([dim_calendar, pd.DataFrame([row])], ignore_index=True)
    dim_calendar.to_parquet(path_cal, index=False)
    return next_id

def ensure_filing(form_type, filing_date, period_end, accession=""):
    global dim_filing
    hit = dim_filing.loc[(dim_filing["form_type"]==form_type)&
                         (dim_filing["filing_date"]==filing_date)&
                         (dim_filing["period_end_date"]==period_end)]
    if accession:
        hit = dim_filing.loc[dim_filing["accession_number"]==accession] if hit.empty else hit
    if not hit.empty: return int(hit.iloc[0]["filing_id"])
    next_id = (dim_filing["filing_id"].max()+1) if len(dim_filing) else 1
    row = {"filing_id":next_id,"form_type":form_type,"filing_date":filing_date,
           "accepted_date":filing_date,"period_end_date":period_end,
           "accession_number":accession,"filing_url":""}
    dim_filing = pd.concat([dim_filing, pd.DataFrame([row])], ignore_index=True)
    dim_filing.to_parquet(path_filing, index=False)
    return next_id

# ---- take values from your preview dict (which you printed above)
pv = preview.copy()
cik_str = str(pv["cik"]).zfill(10)     # zero-pad CIK to 10 characters
fy = int(pv["fy"])
fm = int(pv["period_end"][5:7])        # month from YYYY-MM-DD
fq = "Q4"                              # convention for FY rows

company_id  = ensure_company_by_cik(cik_str)
metric_id   = ensure_metric_by_tag(pv["tag"], metric_name="Revenue", normal_balance="Credit", metric_group="Revenue")
unit_id     = ensure_unit(pv["unit"], category="currency", iso="USD", decimals=2, desc="U.S. dollars")
calendar_id = ensure_calendar(pv["period_end"], fy, fq, fm, period_type="FY")
filing_id   = ensure_filing(pv["form"], pv["filed"], pv["period_end"], pv.get("accession",""))

# ---- write/merge into fact_financials
natural_key = ["company_id","metric_id","calendar_id","filing_id"]
new_row = pd.DataFrame([{
    "company_id": company_id,
    "metric_id": metric_id,
    "calendar_id": calendar_id,
    "value": float(pv["value"]),
    "unit_id": unit_id,
    "filing_id": filing_id,
    "consolidated_flag": "Consolidated"
}])

fact_fin = pd.concat([fact_fin.drop(columns=[], errors="ignore"), new_row], ignore_index=True)
fact_fin = fact_fin.drop_duplicates(subset=natural_key, keep="last").reset_index(drop=True)
fact_fin["financial_id"] = range(1, len(fact_fin)+1)
fact_fin.to_parquet(path_fact, index=False)

print("✅ Wrote one fact row. Current fact_financials size:", len(fact_fin))
fact_fin.tail(3)
import os, json, pandas as pd

parq_base  = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
facts_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"
cik = "0001652044"  # GOOG

# load metric map and company json
dim_metric = pd.read_parquet(os.path.join(parq_base, "dim_metric.parquet"))
with open(os.path.join(facts_base, f"CIK{cik}.json"), "r") as f:
    data = json.load(f)

usgaap = data.get("facts", {}).get("us-gaap", {})
income_tags = dim_metric.loc[dim_metric["metric_group"]=="Income", "xbrl_tag"].tolist()

# choose the first available tag in this company file
tag_income = next((t for t in income_tags if t in usgaap), None)
assert tag_income, "No income tag from your list appears in this file."

series = usgaap[tag_income]["units"]
unit_key = "USD" if "USD" in series else next(iter(series.keys()))
rows = pd.DataFrame(series[unit_key])

annual = rows[rows["fp"].str.upper().eq("FY")].copy()
annual["fy"] = pd.to_numeric(annual["fy"], errors="coerce")
latest = annual.sort_values("fy").tail(1).iloc[0]

preview_income = {
    "entity": data.get("entityName"),
    "cik": str(data.get("cik")).zfill(10),
    "tag": tag_income,
    "unit": unit_key,
    "fy": int(latest["fy"]),
    "period_end": latest["end"],
    "value": float(latest["val"]),
    "form": latest.get("form",""),
    "accession": latest.get("accn",""),
    "filed": latest.get("filed", latest["end"]),
}
preview_income
# reuse the small ensure-* helpers you already ran earlier in the notebook
# (company, metric, unit, calendar, filing). If they’re not in memory anymore,
# re-run the earlier cell that defined ensure_company_by_cik, ensure_metric_by_tag, etc.

pv = preview_income
fy = int(pv["fy"])
fm = int(pv["period_end"][5:7])
fq = "Q4"

company_id  = ensure_company_by_cik(pv["cik"])
metric_id   = ensure_metric_by_tag(pv["tag"], metric_name="Net Income", normal_balance="Credit", metric_group="Income")
unit_id     = ensure_unit(pv["unit"], category="currency", iso="USD", decimals=2, desc="U.S. dollars")
calendar_id = ensure_calendar(pv["period_end"], fy, fq, fm, period_type="FY")
filing_id   = ensure_filing(pv["form"], pv["filed"], pv["period_end"], pv.get("accession",""))

# load fact table, append, dedupe by natural key
path_fact = os.path.join(parq_base, "fact_financials.parquet")
fact_fin = pd.read_parquet(path_fact) if os.path.exists(path_fact) else pd.DataFrame(columns=[
    "financial_id","company_id","metric_id","calendar_id","value","unit_id","filing_id","consolidated_flag"
]).astype({"financial_id":"int64","company_id":"int64","metric_id":"int64","calendar_id":"int64","value":"float64","unit_id":"int64","filing_id":"int64","consolidated_flag":"string"})

new_row = pd.DataFrame([{
    "company_id": company_id,
    "metric_id": metric_id,
    "calendar_id": calendar_id,
    "value": float(pv["value"]),
    "unit_id": unit_id,
    "filing_id": filing_id,
    "consolidated_flag": "Consolidated"
}])

natural_key = ["company_id","metric_id","calendar_id","filing_id"]
fact_fin = pd.concat([fact_fin, new_row], ignore_index=True)
fact_fin = fact_fin.drop_duplicates(subset=natural_key, keep="last").reset_index(drop=True)
fact_fin["financial_id"] = range(1, len(fact_fin)+1)
fact_fin.to_parquet(path_fact, index=False)

print("Added/updated Net Income row. Rows now:", len(fact_fin))
