import sqlite3, pandas as pd

# make a file-backed DB so it persists between sessions
con = sqlite3.connect("smilefund.db")
cur = con.cursor()

cur.execute("""
CREATE TABLE IF NOT EXISTS dim_company (
  company_id INTEGER PRIMARY KEY AUTOINCREMENT,
  cik TEXT NOT NULL UNIQUE,
  name TEXT NOT NULL,
  ticker TEXT,
  sector TEXT,
  industry TEXT,
  fiscal_year_end_month INTEGER CHECK (fiscal_year_end_month BETWEEN 1 AND 12)
);
""")

cur.executemany("""
INSERT INTO dim_company (cik, name, ticker, sector, industry, fiscal_year_end_month)
VALUES (?, ?, ?, ?, ?, ?)
""", [
 ('0001652044','Alphabet Inc.','GOOG','Communication Services','Interactive Media & Services',12),
 ('0000789019','Microsoft Corporation','MSFT','Technology','Software—Infrastructure',6),
 ('0000320193','Apple Inc.','AAPL','Technology','Technology Hardware',9),
])

con.commit()

# inspect
pd.read_sql_query("SELECT * FROM dim_company ORDER BY company_id;", con)
%pip install psycopg2-binary SQLAlchemy pandas
import os
os.chdir("C:/Users/VTN183/OneDrive - University of Tennessee/smilefund_project/")

import os
import pandas as pd

proj = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
os.makedirs(os.path.join(proj, "warehouse", "parquet"), exist_ok=True)
base = os.path.join(proj, "warehouse", "parquet")
base

import pandas as pd

dim_company = pd.DataFrame([
    {"company_id": 1, "cik": "0001652044", "name": "Alphabet Inc.", "ticker": "GOOG",
     "sector": "Communication Services", "industry": "Interactive Media & Services",
     "fiscal_year_end_month": 12},
    {"company_id": 2, "cik": "0000789019", "name": "Microsoft Corporation", "ticker": "MSFT",
     "sector": "Technology", "industry": "Software—Infrastructure",
     "fiscal_year_end_month": 6},
    {"company_id": 3, "cik": "0000320193", "name": "Apple Inc.", "ticker": "AAPL",
     "sector": "Technology", "industry": "Technology Hardware",
     "fiscal_year_end_month": 9},
])

# (optional) set explicit dtypes; helpful later
dim_company = dim_company.astype({
    "company_id": "int64",
    "cik": "string",
    "name": "string",
    "ticker": "string",
    "sector": "string",
    "industry": "string",
    "fiscal_year_end_month": "int64",
})
path_company = os.path.join(base, "dim_company.parquet")
dim_company.to_parquet(path_company, index=False, engine="pyarrow")
pd.read_parquet(path_company)
dim_calendar = pd.DataFrame([
    {"calendar_id": 1, "date": "2023-03-31", "fiscal_year": 2023, "fiscal_quarter": "Q1", "fiscal_month": 3, "period_type": "QTR"},
    {"calendar_id": 2, "date": "2023-06-30", "fiscal_year": 2023, "fiscal_quarter": "Q2", "fiscal_month": 6, "period_type": "QTR"},
    {"calendar_id": 3, "date": "2023-09-30", "fiscal_year": 2023, "fiscal_quarter": "Q3", "fiscal_month": 9, "period_type": "QTR"},
    {"calendar_id": 4, "date": "2023-12-31", "fiscal_year": 2023, "fiscal_quarter": "Q4", "fiscal_month": 12, "period_type": "FY"},
]).astype({
    "calendar_id": "int64","date": "string","fiscal_year": "int64","fiscal_quarter": "string",
    "fiscal_month": "int64","period_type": "string"
})

path_calendar = os.path.join(base, "dim_calendar.parquet")
dim_calendar.to_parquet(path_calendar, index=False)
pd.read_parquet(path_calendar)
dim_metric = pd.DataFrame([
    {"metric_id": 1, "metric_name": "Revenue", "xbrl_tag": "Revenues", "normal_balance": "Credit"},
    {"metric_id": 2, "metric_name": "Net Income", "xbrl_tag": "NetIncomeLoss", "normal_balance": "Credit"},
    {"metric_id": 3, "metric_name": "Total Assets", "xbrl_tag": "Assets", "normal_balance": "Debit"},
]).astype({"metric_id":"int64","metric_name":"string","xbrl_tag":"string","normal_balance":"string"})

path_metric = os.path.join(base, "dim_metric.parquet")
dim_metric.to_parquet(path_metric, index=False)
pd.read_parquet(path_metric)
dim_filing = pd.DataFrame([
    {"filing_id": 1, "form_type": "10-K", "filing_date": "2024-02-03",
     "accepted_date": "2024-02-03", "period_end_date": "2023-12-31",
     "accession_number": "0001652044-24-000050", "filing_url": "https://..."},
]).astype({
    "filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
    "period_end_date":"string","accession_number":"string","filing_url":"string"
})

path_filing = os.path.join(base, "dim_filing.parquet")
dim_filing.to_parquet(path_filing, index=False)
pd.read_parquet(path_filing)
dim_unit = pd.DataFrame([
    {"unit_id":1,"unit_code":"USD","category":"currency","iso_currency":"USD","decimals_hint":2,"description":"U.S. dollars"},
    {"unit_id":2,"unit_code":"shares","category":"shares","iso_currency":None,"decimals_hint":0,"description":"Common shares"},
    {"unit_id":3,"unit_code":"USD_per_share","category":"per_share","iso_currency":"USD","decimals_hint":2,"description":"Dollars per share"},
]).astype({"unit_id":"int64","unit_code":"string","category":"string","iso_currency":"string",
          "decimals_hint":"int64","description":"string"})

path_unit = os.path.join(base, "dim_unit.parquet")
dim_unit.to_parquet(path_unit, index=False)
pd.read_parquet(path_unit)
fact_financials = pd.DataFrame([
    {"financial_id": 1, "company_id": 1, "metric_id": 1, "calendar_id": 4,
     "value": 307394000000.00, "unit_id": 1, "filing_id": 1, "consolidated_flag": "Consolidated"},
    {"financial_id": 2, "company_id": 1, "metric_id": 2, "calendar_id": 4,
     "value": 73795000000.00, "unit_id": 1, "filing_id": 1, "consolidated_flag": "Consolidated"},
]).astype({
    "financial_id":"int64","company_id":"int64","metric_id":"int64","calendar_id":"int64",
    "value":"float64","unit_id":"int64","filing_id":"int64","consolidated_flag":"string"
})

path_fin = os.path.join(base, "fact_financials.parquet")
fact_financials.to_parquet(path_fin, index=False)
pd.read_parquet(path_fin)
fact_holdings = pd.DataFrame([
    {"holding_id":1,"company_id":1,"asof_date":"2025-03-31","weight":0.04200,"shares":145,"market_value":30200,"source":"SMILE_FUND"},
    {"holding_id":2,"company_id":2,"asof_date":"2025-03-31","weight":0.03800,"shares":110,"market_value":29000,"source":"SMILE_FUND"},
]).astype({
    "holding_id":"int64","company_id":"int64","asof_date":"string","weight":"float64",
    "shares":"float64","market_value":"float64","source":"string"
})

path_hold = os.path.join(base, "fact_holdings.parquet")
fact_holdings.to_parquet(path_hold, index=False)
pd.read_parquet(path_hold)
import pandas as pd

path = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\Approved List 9.14.25(Approved List 9.14.csv"
df = pd.read_csv(path)
df.head()
import os, pandas as pd
base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
path_company  = os.path.join(base, "dim_company.parquet")
path_holdings = os.path.join(base, "fact_holdings.parquet")
csv_path = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\Approved List 9.14.25(Approved List 9.14.csv"
df = pd.read_csv(csv_path)

# lower/strip col names
df.columns = [c.strip().lower() for c in df.columns]

# map common variants to our target names
RENAME_MAP = {
    "ticker": "ticker",
    "symbol": "ticker",
    "company": "name",
    "company name": "name",
    "name": "name",
    "sector": "sector",
    "gics sector": "sector",
    "weight": "weight",
    "weight %": "weight",
    "portfolio weight": "weight",
    "asof": "asof_date",
    "as_of": "asof_date",
    "as of": "asof_date",
    "date": "asof_date"
}

df = df.rename(columns={c: RENAME_MAP.get(c, c) for c in df.columns})

# keep only the columns we know how to use (others are ignored for now)
keep_cols = [c for c in ["ticker","name","sector","weight","asof_date"] if c in df.columns]
df = df[keep_cols].copy()
df.head()
import pandas as pd
import re

# df already loaded with a 'ticker' column
tickers_raw = df["ticker"].astype(str)

def normalize_ticker(s: str) -> str:
    s = s.strip().upper()
    s = s.replace(" ", "")
    # convert share-class separator "/" to "." (e.g., BRK/B -> BRK.B)
    s = re.sub(r"/", ".", s)
    return s

df["ticker_norm"] = tickers_raw.map(normalize_ticker)

# show before/after for first few rows
preview = df[["ticker", "ticker_norm"]].head(10)
preview
import os, pandas as pd

base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
path_company = os.path.join(base, "dim_company.parquet")

# if the file exists, load it; else start empty with the right columns
if os.path.exists(path_company):
    dim_company = pd.read_parquet(path_company)
else:
    dim_company = pd.DataFrame(columns=[
        "company_id","cik","name","ticker","sector","industry","fiscal_year_end_month"
    ])
    dim_company = dim_company.astype({
        "company_id":"int64", "cik":"string","name":"string","ticker":"string",
        "sector":"string","industry":"string","fiscal_year_end_month":"float64"
    })

dim_company.head()
# make sure we have the normalized ticker column from the prior step
assert "ticker_norm" in df.columns, "Run the normalization step first."

# unique tickers from your CSV
incoming = (df[["ticker_norm"]]
            .dropna()
            .drop_duplicates()
            .rename(columns={"ticker_norm":"ticker"}))

# left-join to see which are missing from dim_company
check = incoming.merge(dim_company[["ticker","company_id"]], on="ticker", how="left")
to_add = check[check["company_id"].isna()].drop(columns=["company_id"]).copy()

print(f"New tickers to add: {len(to_add)}")
to_add.head(20)
if len(to_add) > 0:
    # next sequential company_id
    next_id = (dim_company["company_id"].max() + 1) if len(dim_company) else 1
    new_ids = list(range(next_id, next_id + len(to_add)))

    n = len(to_add)  # store length once

    new_rows = pd.DataFrame({
        "company_id": new_ids,
        "cik": ["" for _ in range(n)],
        "name": ["" for _ in range(n)],
        "ticker": to_add["ticker"].astype("string"),
        "sector": ["" for _ in range(n)],
        "industry": ["" for _ in range(n)],
        "fiscal_year_end_month": [None for _ in range(n)]
    })

    dim_company = pd.concat([dim_company, new_rows], ignore_index=True)
    dim_company = dim_company.drop_duplicates(subset=["ticker"], keep="first").reset_index(drop=True)

    dim_company.to_parquet(path_company, index=False)

# reload to confirm
dim_company = pd.read_parquet(path_company)
print(f"dim_company rows: {len(dim_company)}")
dim_company.tail(10)
import pandas as pd
import os
from datetime import date

base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
path_company  = os.path.join(base, "dim_company.parquet")
path_holdings = os.path.join(base, "fact_holdings.parquet")

# 1) reload company dim (has company_id + ticker)
dim_company = pd.read_parquet(path_company)[["company_id","ticker"]]

# 2) make a clean ticker list from your dataframe (the one with ticker_norm)
assert "ticker_norm" in df.columns, "Run the normalization step first."
tickers = (df[["ticker_norm"]]
           .dropna()
           .drop_duplicates()
           .rename(columns={"ticker_norm":"ticker"}))

# 3) attach company_id to each ticker
hold = tickers.merge(dim_company, on="ticker", how="left")

# 4) add minimal holding fields
asof = "2025-03-31"   # ← set the date you want (or str(date.today()))
preview_holdings = pd.DataFrame({
    "company_id": hold["company_id"].astype("Int64"),  # allow nulls to show if any missing
    "asof_date":  asof,
    "weight":     pd.NA,        # will fill later
    "shares":     pd.NA,
    "market_value": pd.NA,
    "source":     "SMILE_FUND"
})

preview_holdings.head(10)
# load existing file if present
if os.path.exists(path_holdings):
    existing = pd.read_parquet(path_holdings)
else:
    existing = pd.DataFrame(columns=[
        "holding_id","company_id","asof_date","weight","shares","market_value","source"
    ])

# build new block with temporary ids
new_block = preview_holdings.copy()
new_block = new_block.astype({
    "company_id":"int64",
    "asof_date":"string",
    "source":"string"
})

# combine and assign holding_id sequentially
combined = pd.concat([existing, new_block], ignore_index=True)

# de-dup just in case (one row per company per as-of date)
combined = combined.drop_duplicates(subset=["company_id","asof_date"], keep="last").reset_index(drop=True)

combined["holding_id"] = range(1, len(combined)+1)

# save
combined.to_parquet(path_holdings, index=False)

# quick look
pd.read_parquet(path_holdings).head(12)
import os, pandas as pd

base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
out_dir = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\outputs"
os.makedirs(out_dir, exist_ok=True)

path_company  = os.path.join(base, "dim_company.parquet")
path_holdings = os.path.join(base, "fact_holdings.parquet")

dim_company = pd.read_parquet(path_company)[["company_id","ticker","name","sector","industry"]]
holdings    = pd.read_parquet(path_holdings)

review = (holdings
          .merge(dim_company, on="company_id", how="left")
          .sort_values(["asof_date","ticker"]))

# quick on-screen check
review.head(15)
print("rows:", len(review), "| unique companies:", review["company_id"].nunique())

# save for eyeballing in Excel
review_csv = os.path.join(out_dir, "holdings_review.csv")
review_parq = os.path.join(out_dir, "holdings_review.parquet")
review.to_csv(review_csv, index=False)
review.to_parquet(review_parq, index=False)
print("Wrote:", review_csv)
import os, pandas as pd

proj_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
parq_base = os.path.join(proj_base, "warehouse", "parquet")
os.makedirs(parq_base, exist_ok=True)

path_filing   = os.path.join(parq_base, "dim_filing.parquet")

# create if missing
if not os.path.exists(path_filing):
    dim_filing = pd.DataFrame(columns=[
        "filing_id","form_type","filing_date","accepted_date",
        "period_end_date","accession_number","filing_url"
    ]).astype({
        "filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
        "period_end_date":"string","accession_number":"string","filing_url":"string"
    })
    dim_filing.to_parquet(path_filing, index=False)
else:
    dim_filing = pd.read_parquet(path_filing)

dim_filing.tail(3)
import os, glob

base_path = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"
files = glob.glob(os.path.join(base_path, "*.json"))

print(f"Found {len(files):,} JSON files")
print("Example:", files[:3])
import pandas as pd
from pathlib import Path

# --- 1️⃣  Revenue tags ---
revenue_tags = [
    "Revenues",
    "RevenueFromContractWithCustomerExcludingAssessedTax",
    "RevenueFromContractWithCustomerIncludingAssessedTax",
    "SalesRevenueNet",
    "SalesRevenueGoodsNet",
    "SalesRevenueServicesNet",
    "TotalRevenuesAndOtherIncome",
    "PremiumsEarnedNet",                # insurance
    "InterestIncomeOperating"           # banks
]

# --- 2️⃣  Net Income tags ---
net_income_tags = [
    "NetIncomeLoss",
    "ProfitLoss",
    "IncomeLossFromContinuingOperations",
    "NetIncomeLossAvailableToCommonStockholdersBasic",
    "NetIncomeLossAttributableToParent"
]

# --- 3️⃣  Asset tags ---
asset_tags = [
    "Assets",
    "AssetsCurrent",
    "TotalAssets",
    "AssetsFairValueDisclosure"
]

# --- 4️⃣  Combine into rows ---
records = []

for tag in revenue_tags:
    records.append({"xbrl_tag": tag, "metric_name": "Revenue", "metric_group": "Revenue", "normal_balance": "Credit"})

for tag in net_income_tags:
    records.append({"xbrl_tag": tag, "metric_name": "Net Income", "metric_group": "Income", "normal_balance": "Credit"})

for tag in asset_tags:
    records.append({"xbrl_tag": tag, "metric_name": "Assets", "metric_group": "Assets", "normal_balance": "Debit"})

# --- 5️⃣  Build DataFrame ---
df_metric = pd.DataFrame(records)
df_metric.insert(0, "metric_id", range(1, len(df_metric) + 1))

# --- 6️⃣  Export to Parquet ---
out_path = Path(r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet\dim_metric.parquet")
df_metric.to_parquet(out_path, index=False)

print("✅ dim_metric.parquet created with", len(df_metric), "rows")
df_metric.head()
import os, json, pandas as pd

# 1) paths
parq_base  = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
facts_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"

path_metric = os.path.join(parq_base, "dim_metric.parquet")

# 2) choose one company file (edit CIK if you want another)
cik = "0001652044"  # Alphabet; change to any CIK you like
cik_file = os.path.join(facts_base, f"CIK{cik}.json")
print("Using:", cik_file)

# 3) load company JSON
with open(cik_file, "r") as f:
    data = json.load(f)

entity = data.get("entityName", "(unknown)")
usgaap_tags = set(data.get("facts", {}).get("us-gaap", {}).keys())
print("Entity:", entity)
print("us-gaap tag count:", len(usgaap_tags))

# 4) load your master metric map
dim_metric = pd.read_parquet(path_metric)

# 5) check tag presence by metric_group
def presence_for_group(group_name):
    subset = dim_metric[dim_metric["metric_group"] == group_name].copy()
    subset["present"] = subset["xbrl_tag"].apply(lambda t: t in usgaap_tags)
    return subset[["xbrl_tag","metric_name","metric_group","present"]].sort_values("xbrl_tag")

for grp in ["Revenue", "Income", "Assets"]:
    print(f"\n=== {grp} tags present? ===")
    display(presence_for_group(grp).head(50))
import os, json, pandas as pd

# paths + target
facts_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"
cik = "0001652044"                       # Alphabet (change if you want another company)
tag = "Revenues"                         # from your presence table
cik_file = os.path.join(facts_base, f"CIK{cik}.json")

with open(cik_file, "r") as f:
    data = json.load(f)

# get all unit series for the tag (e.g., USD, USDm, shares, etc.)
series = data.get("facts", {}).get("us-gaap", {}).get(tag, {}).get("units", {})
if not series:
    raise ValueError(f"No units found for {tag}")

# prefer USD if present; otherwise take the first numeric-looking unit
preferred_units = ["USD", "usd", "USDm", "USDth", "USD $"]  # common variants
unit_key = next((u for u in preferred_units if u in series), next(iter(series.keys())))
rows = pd.DataFrame(series[unit_key])

# keep **annual** rows (fp == 'FY')
annual = rows[rows["fp"].str.upper().eq("FY")].copy()

# choose the latest fiscal year available
annual["fy"] = pd.to_numeric(annual["fy"], errors="coerce")
latest = annual.sort_values("fy").tail(1).iloc[0]

preview = {
    "entity": data.get("entityName"),
    "cik": data.get("cik"),
    "tag": tag,
    "unit": unit_key,
    "fy": int(latest["fy"]),
    "period_end": latest["end"],         # YYYY-MM-DD
    "value": float(latest["val"]),
    "form": latest.get("form", ""),
    "accession": latest.get("accn", ""),
    "filed": latest.get("filed", latest["end"])
}
preview
import os, pandas as pd

proj_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
parq_base = os.path.join(proj_base, "warehouse", "parquet")

# ---- load all tables (create empty if missing)
def read_or_empty(path, cols, dtypes):
    if os.path.exists(path):
        return pd.read_parquet(path)
    df = pd.DataFrame(columns=cols).astype(dtypes)
    df.to_parquet(path, index=False)
    return df

path_company = os.path.join(parq_base, "dim_company.parquet")
path_metric  = os.path.join(parq_base, "dim_metric.parquet")
path_unit    = os.path.join(parq_base, "dim_unit.parquet")
path_cal     = os.path.join(parq_base, "dim_calendar.parquet")
path_filing  = os.path.join(parq_base, "dim_filing.parquet")
path_fact    = os.path.join(parq_base, "fact_financials.parquet")

dim_company = read_or_empty(path_company,
    ["company_id","cik","name","ticker","sector","industry","fiscal_year_end_month"],
    {"company_id":"int64","cik":"string","name":"string","ticker":"string",
     "sector":"string","industry":"string","fiscal_year_end_month":"float64"}
)
dim_metric = pd.read_parquet(path_metric)  # you created this already
dim_unit   = read_or_empty(path_unit,
    ["unit_id","unit_code","category","iso_currency","decimals_hint","description"],
    {"unit_id":"int64","unit_code":"string","category":"string","iso_currency":"string",
     "decimals_hint":"float64","description":"string"}
)
dim_calendar = read_or_empty(path_cal,
    ["calendar_id","date","fiscal_year","fiscal_quarter","fiscal_month","period_type"],
    {"calendar_id":"int64","date":"string","fiscal_year":"int64","fiscal_quarter":"string",
     "fiscal_month":"int64","period_type":"string"}
)
dim_filing = read_or_empty(path_filing,
    ["filing_id","form_type","filing_date","accepted_date","period_end_date","accession_number","filing_url"],
    {"filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
     "period_end_date":"string","accession_number":"string","filing_url":"string"}
)
if os.path.exists(path_fact):
    fact_fin = pd.read_parquet(path_fact)
else:
    fact_fin = pd.DataFrame(columns=[
        "financial_id","company_id","metric_id","calendar_id","value","unit_id","filing_id","consolidated_flag"
    ]).astype({"financial_id":"int64","company_id":"int64","metric_id":"int64","calendar_id":"int64",
               "value":"float64","unit_id":"int64","filing_id":"int64","consolidated_flag":"string"})

# ---- tiny helpers (inline) to "ensure" IDs exist
def ensure_company_by_cik(cik_str, ticker=""):
    global dim_company
    hit = dim_company.loc[dim_company["cik"]==cik_str]
    if not hit.empty: 
        return int(hit.iloc[0]["company_id"])
    next_id = (dim_company["company_id"].max()+1) if len(dim_company) else 1
    row = {"company_id":next_id,"cik":cik_str,"name":"","ticker":ticker,
           "sector":"","industry":"","fiscal_year_end_month":None}
    dim_company = pd.concat([dim_company, pd.DataFrame([row])], ignore_index=True)
    dim_company.to_parquet(path_company, index=False)
    return next_id

def ensure_metric_by_tag(xbrl_tag, metric_name=None, normal_balance="Credit", metric_group=None):
    global dim_metric
    hit = dim_metric.loc[dim_metric["xbrl_tag"]==xbrl_tag]
    if not hit.empty: return int(hit.iloc[0]["metric_id"])
    next_id = (dim_metric["metric_id"].max()+1) if len(dim_metric) else 1
    row = {"metric_id":next_id,"xbrl_tag":xbrl_tag,
           "metric_name":metric_name or xbrl_tag,
           "metric_group":metric_group or metric_name or xbrl_tag,
           "normal_balance":normal_balance}
    dim_metric = pd.concat([dim_metric, pd.DataFrame([row])], ignore_index=True)
    dim_metric.to_parquet(path_metric, index=False)
    return next_id

def ensure_unit(unit_code, category="currency", iso="USD", decimals=2, desc=""):
    global dim_unit
    hit = dim_unit.loc[dim_unit["unit_code"]==unit_code]
    if not hit.empty: return int(hit.iloc[0]["unit_id"])
    next_id = (dim_unit["unit_id"].max()+1) if len(dim_unit) else 1
    row = {"unit_id":next_id,"unit_code":unit_code,"category":category,
           "iso_currency":iso,"decimals_hint":decimals,"description":desc}
    dim_unit = pd.concat([dim_unit, pd.DataFrame([row])], ignore_index=True)
    dim_unit.to_parquet(path_unit, index=False)
    return next_id

def ensure_calendar(period_end, fiscal_year, fiscal_quarter, fiscal_month, period_type="FY"):
    global dim_calendar
    hit = dim_calendar.loc[(dim_calendar["date"]==period_end) & (dim_calendar["period_type"]==period_type)]
    if not hit.empty: return int(hit.iloc[0]["calendar_id"])
    next_id = (dim_calendar["calendar_id"].max()+1) if len(dim_calendar) else 1
    row = {"calendar_id":next_id,"date":period_end,"fiscal_year":int(fiscal_year),
           "fiscal_quarter":fiscal_quarter,"fiscal_month":int(fiscal_month),"period_type":period_type}
    dim_calendar = pd.concat([dim_calendar, pd.DataFrame([row])], ignore_index=True)
    dim_calendar.to_parquet(path_cal, index=False)
    return next_id

def ensure_filing(form_type, filing_date, period_end, accession=""):
    global dim_filing
    hit = dim_filing.loc[(dim_filing["form_type"]==form_type)&
                         (dim_filing["filing_date"]==filing_date)&
                         (dim_filing["period_end_date"]==period_end)]
    if accession:
        hit = dim_filing.loc[dim_filing["accession_number"]==accession] if hit.empty else hit
    if not hit.empty: return int(hit.iloc[0]["filing_id"])
    next_id = (dim_filing["filing_id"].max()+1) if len(dim_filing) else 1
    row = {"filing_id":next_id,"form_type":form_type,"filing_date":filing_date,
           "accepted_date":filing_date,"period_end_date":period_end,
           "accession_number":accession,"filing_url":""}
    dim_filing = pd.concat([dim_filing, pd.DataFrame([row])], ignore_index=True)
    dim_filing.to_parquet(path_filing, index=False)
    return next_id

# ---- take values from your preview dict (which you printed above)
pv = preview.copy()
cik_str = str(pv["cik"]).zfill(10)     # zero-pad CIK to 10 characters
fy = int(pv["fy"])
fm = int(pv["period_end"][5:7])        # month from YYYY-MM-DD
fq = "Q4"                              # convention for FY rows

company_id  = ensure_company_by_cik(cik_str)
metric_id   = ensure_metric_by_tag(pv["tag"], metric_name="Revenue", normal_balance="Credit", metric_group="Revenue")
unit_id     = ensure_unit(pv["unit"], category="currency", iso="USD", decimals=2, desc="U.S. dollars")
calendar_id = ensure_calendar(pv["period_end"], fy, fq, fm, period_type="FY")
filing_id   = ensure_filing(pv["form"], pv["filed"], pv["period_end"], pv.get("accession",""))

# ---- write/merge into fact_financials
natural_key = ["company_id","metric_id","calendar_id","filing_id"]
new_row = pd.DataFrame([{
    "company_id": company_id,
    "metric_id": metric_id,
    "calendar_id": calendar_id,
    "value": float(pv["value"]),
    "unit_id": unit_id,
    "filing_id": filing_id,
    "consolidated_flag": "Consolidated"
}])

fact_fin = pd.concat([fact_fin.drop(columns=[], errors="ignore"), new_row], ignore_index=True)
fact_fin = fact_fin.drop_duplicates(subset=natural_key, keep="last").reset_index(drop=True)
fact_fin["financial_id"] = range(1, len(fact_fin)+1)
fact_fin.to_parquet(path_fact, index=False)

print("✅ Wrote one fact row. Current fact_financials size:", len(fact_fin))
fact_fin.tail(3)
import os, json, pandas as pd

parq_base  = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
facts_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"
cik = "0001652044"  # GOOG

# load metric map and company json
dim_metric = pd.read_parquet(os.path.join(parq_base, "dim_metric.parquet"))
with open(os.path.join(facts_base, f"CIK{cik}.json"), "r") as f:
    data = json.load(f)

usgaap = data.get("facts", {}).get("us-gaap", {})
income_tags = dim_metric.loc[dim_metric["metric_group"]=="Income", "xbrl_tag"].tolist()

# choose the first available tag in this company file
tag_income = next((t for t in income_tags if t in usgaap), None)
assert tag_income, "No income tag from your list appears in this file."

series = usgaap[tag_income]["units"]
unit_key = "USD" if "USD" in series else next(iter(series.keys()))
rows = pd.DataFrame(series[unit_key])

annual = rows[rows["fp"].str.upper().eq("FY")].copy()
annual["fy"] = pd.to_numeric(annual["fy"], errors="coerce")
latest = annual.sort_values("fy").tail(1).iloc[0]

preview_income = {
    "entity": data.get("entityName"),
    "cik": str(data.get("cik")).zfill(10),
    "tag": tag_income,
    "unit": unit_key,
    "fy": int(latest["fy"]),
    "period_end": latest["end"],
    "value": float(latest["val"]),
    "form": latest.get("form",""),
    "accession": latest.get("accn",""),
    "filed": latest.get("filed", latest["end"]),
}
preview_income
# reuse the small ensure-* helpers you already ran earlier in the notebook
# (company, metric, unit, calendar, filing). If they’re not in memory anymore,
# re-run the earlier cell that defined ensure_company_by_cik, ensure_metric_by_tag, etc.

pv = preview_income
fy = int(pv["fy"])
fm = int(pv["period_end"][5:7])
fq = "Q4"

company_id  = ensure_company_by_cik(pv["cik"])
metric_id   = ensure_metric_by_tag(pv["tag"], metric_name="Net Income", normal_balance="Credit", metric_group="Income")
unit_id     = ensure_unit(pv["unit"], category="currency", iso="USD", decimals=2, desc="U.S. dollars")
calendar_id = ensure_calendar(pv["period_end"], fy, fq, fm, period_type="FY")
filing_id   = ensure_filing(pv["form"], pv["filed"], pv["period_end"], pv.get("accession",""))

# load fact table, append, dedupe by natural key
path_fact = os.path.join(parq_base, "fact_financials.parquet")
fact_fin = pd.read_parquet(path_fact) if os.path.exists(path_fact) else pd.DataFrame(columns=[
    "financial_id","company_id","metric_id","calendar_id","value","unit_id","filing_id","consolidated_flag"
]).astype({"financial_id":"int64","company_id":"int64","metric_id":"int64","calendar_id":"int64","value":"float64","unit_id":"int64","filing_id":"int64","consolidated_flag":"string"})

new_row = pd.DataFrame([{
    "company_id": company_id,
    "metric_id": metric_id,
    "calendar_id": calendar_id,
    "value": float(pv["value"]),
    "unit_id": unit_id,
    "filing_id": filing_id,
    "consolidated_flag": "Consolidated"
}])

natural_key = ["company_id","metric_id","calendar_id","filing_id"]
fact_fin = pd.concat([fact_fin, new_row], ignore_index=True)
fact_fin = fact_fin.drop_duplicates(subset=natural_key, keep="last").reset_index(drop=True)
fact_fin["financial_id"] = range(1, len(fact_fin)+1)
fact_fin.to_parquet(path_fact, index=False)

print("Added/updated Net Income row. Rows now:", len(fact_fin))
import os

parq_base = "parq_base"   # or any path you prefer, e.g., "C:/SMILEFund/parq_base"
os.makedirs(parq_base, exist_ok=True)
print(f"Parquet base folder created at: {os.path.abspath(parq_base)}")
import pandas as pd, os, json

parq = "parq_base"  # <- your folder

metric_pref = pd.DataFrame([
    # Revenue family
    ("RevenueStandard", "us-gaap:Revenues", 1),
    ("RevenueStandard", "us-gaap:RevenueFromContractWithCustomerExcludingAssessedTax", 2),
    ("RevenueStandard", "us-gaap:SalesRevenueNet", 3),
    ("RevenueStandard", "us-gaap:SalesRevenueGoodsNet", 4),
    ("RevenueStandard", "us-gaap:SalesRevenueServicesNet", 5),
    # Net income family (already in your fact table but keep for symmetry)
    ("NetIncomeStandard", "us-gaap:NetIncomeLoss", 1),
], columns=["metric_standard", "xbrl_tag", "pref_rank"])

metric_pref.to_parquet(os.path.join(parq, "dim_metric_preference.parquet"), index=False)
import os

PROJ = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
PARQ = os.path.join(PROJ, "warehouse", "parquet")
os.makedirs(PARQ, exist_ok=True)

print("PARQ =", PARQ)
import pandas as pd, os

metric_pref = pd.DataFrame([
    # Revenue family (lower rank = more preferred)
    ("RevenueStandard", "Revenues", 1),
    ("RevenueStandard", "RevenueFromContractWithCustomerExcludingAssessedTax", 2),
    ("RevenueStandard", "SalesRevenueNet", 3),
    ("RevenueStandard", "SalesRevenueGoodsNet", 4),
    ("RevenueStandard", "SalesRevenueServicesNet", 5),

    # Net income family
    ("NetIncomeStandard", "NetIncomeLoss", 1),
], columns=["metric_standard", "xbrl_tag", "pref_rank"])

metric_pref_path = os.path.join(PARQ, "dim_metric_preference.parquet")
metric_pref.to_parquet(metric_pref_path, index=False)
metric_pref.head()
import pandas as pd, os

# Inputs
fact_path   = os.path.join(PARQ, "fact_financials.parquet")
metric_path = os.path.join(PARQ, "dim_metric.parquet")
cal_path    = os.path.join(PARQ, "dim_calendar.parquet")
co_path     = os.path.join(PARQ, "dim_company.parquet")
unit_path   = os.path.join(PARQ, "dim_unit.parquet")
filing_path = os.path.join(PARQ, "dim_filing.parquet")
pref_path   = os.path.join(PARQ, "dim_metric_preference.parquet")

fact = pd.read_parquet(fact_path)
dim_metric = pd.read_parquet(metric_path)[["metric_id","xbrl_tag","metric_name"]]
dim_calendar = pd.read_parquet(cal_path)[["calendar_id","date","fiscal_year","period_type"]]
dim_company = pd.read_parquet(co_path)[["company_id","ticker","cik"]]
dim_unit = pd.read_parquet(unit_path) if os.path.exists(unit_path) else pd.DataFrame(columns=["unit_id","unit_code"])
metric_pref = pd.read_parquet(pref_path)

if os.path.exists(filing_path):
    dim_filing_raw = pd.read_parquet(filing_path)
    # normalize filing date col to a common name
    if "filed_date" in dim_filing_raw.columns:
        dim_filing = dim_filing_raw[["filing_id","filed_date","form_type" if "form_type" in dim_filing_raw.columns else "form"]]
    else:
        # your dim_filing uses filing_date/form_type
        dim_filing = dim_filing_raw.rename(columns={"filing_date":"filed_date"})[
            ["filing_id","filed_date","form_type" if "form_type" in dim_filing_raw.columns else "form"]
        ]
else:
    dim_filing = pd.DataFrame(columns=["filing_id","filed_date","form_type"])

# Join everything
df = (fact
      .merge(dim_metric, on="metric_id", how="left")
      .merge(metric_pref, on="xbrl_tag", how="left")        # <-- matches your tag names
      .merge(dim_calendar, on="calendar_id", how="left")
      .merge(dim_company, on="company_id", how="left")
      .merge(dim_filing, on="filing_id", how="left"))

# Keep only metrics we’re standardizing
df = df[df["metric_standard"].notna()].copy()

# Tie-breakers
df["is_consolidated"] = (df.get("consolidated_flag","").astype(str).str.lower()=="consolidated").astype(int)
# Prefer USD: your dim_unit has unit_id=1 for USD
usd_unit_ids = set(dim_unit.loc[dim_unit["unit_code"].str.upper()=="USD", "unit_id"]) if "unit_code" in dim_unit.columns else {1}
df["is_usd"] = df["unit_id"].isin(usd_unit_ids).astype(int)
df["filed_date"] = pd.to_datetime(df.get("filed_date"))

# Sort (best first), then de-dup to keep the winner per company×period×metric_standard
df = df.sort_values([
    "company_id","calendar_id","metric_standard",
    "pref_rank",        # lower is better
    "is_consolidated",  # consolidated preferred
    "is_usd",           # USD preferred
    "filed_date"        # latest preferred
], ascending=[True,True,True, True, False, False, False])

std = (df
       .drop_duplicates(["company_id","calendar_id","metric_standard"], keep="first")
       .rename(columns={"value":"value_numeric"})
       [["company_id","calendar_id","metric_standard","value_numeric",
         "ticker","cik","fiscal_year","period_type","date"]])

# Persist
std_path = os.path.join(PARQ, "fact_financials_standard.parquet")
std.to_parquet(std_path, index=False)

# QA
assert not std.duplicated(["company_id","calendar_id","metric_standard"]).any()
print("Standardized rows:", len(std))
print(std.pivot_table(index="metric_standard", values="value_numeric", aggfunc="count"))

# Wide convenience view
wide = (std
        .pivot_table(index=["company_id","calendar_id","ticker","cik","fiscal_year","period_type","date"],
                     columns="metric_standard", values="value_numeric", aggfunc="first")
        .reset_index())
wide.columns.name = None
wide_path = os.path.join(PARQ, "vw_financials_wide.parquet")
wide.to_parquet(wide_path, index=False)
print("Wrote wide view:", wide_path, "rows:", len(wide))
co_raw = pd.read_parquet(paths["dim_company"])
if "ticker" not in co_raw.columns:
    co_raw["ticker"] = pd.NA  # add empty ticker column
    co_raw.to_parquet(paths["dim_company"], index=False)
    print("Added empty 'ticker' column to dim_company.")
import os, pandas as pd

# --- Paths
PROJ = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
PARQ = os.path.join(PROJ, "warehouse", "parquet")

path_hold = os.path.join(PARQ, "fact_holdings.parquet")
path_wide = os.path.join(PARQ, "vw_financials_wide.parquet")
path_co   = os.path.join(PARQ, "dim_company.parquet")
out_path  = os.path.join(PARQ, "vw_holdings_with_financials.parquet")

# --- Helpers
def safe_select(df, cols):
    have = [c for c in cols if c in df.columns]
    return df[have].copy()

# --- Load
hold = pd.read_parquet(path_hold)          # expects company_id, asof_date, weight...
finw = pd.read_parquet(path_wide)          # has company_id, date, RevenueStandard/NetIncomeStandard, maybe ticker
co   = pd.read_parquet(path_co)            # has company_id, maybe ticker/name/sector/industry

# Ensure datetimes
finw["date"] = pd.to_datetime(finw["date"])
hold["asof_date"] = pd.to_datetime(hold["asof_date"])

# --- Pick most recent financial period per company <= portfolio as-of max
asof_max = hold["asof_date"].max()
finw_recent = (finw[finw["date"] <= asof_max]
               .sort_values(["company_id","date"])
               .groupby("company_id", as_index=False)
               .tail(1)
               .copy())

# Add company meta if missing from finw
meta_want = ["ticker","name","sector","industry"]
meta_to_add = [c for c in meta_want if c not in finw_recent.columns and c in co.columns]
if meta_to_add:
    finw_recent = finw_recent.merge(safe_select(co, ["company_id"] + meta_to_add),
                                    on="company_id", how="left")

# Build the final dataset (only keep columns that exist)
metric_cols = [c for c in ["RevenueStandard","NetIncomeStandard"] if c in finw_recent.columns]
latest_fin = safe_select(finw_recent, ["company_id","date"] + metric_cols + [c for c in meta_want if c in finw_recent.columns])
latest_fin = latest_fin.rename(columns={"date":"fin_period_end"})

# Merge onto holdings
hold_fin = hold.merge(latest_fin, on="company_id", how="left", suffixes=("","_fin"))

# Save
hold_fin.to_parquet(out_path, index=False)
print(f"✅ Wrote: {out_path} | rows: {len(hold_fin)}")

# Peek
hold_fin.head(10)
# Compute TTM Revenue and attach to holdings (robust to FY-only data)
finw = pd.read_parquet(path_wide).copy()
finw["date"] = pd.to_datetime(finw["date"])

rev = finw[["company_id","date","period_type","RevenueStandard"]].dropna()
q = rev[rev["period_type"].str.upper().eq("QTR")].sort_values(["company_id","date"]).copy()
a = rev[rev["period_type"].str.upper().eq("FY")].sort_values(["company_id","date"]).copy()

# Rolling 4q TTM
q["rev_ttm_from_quarters"] = q.groupby("company_id")["RevenueStandard"]\
                               .rolling(4, min_periods=4).sum().reset_index(level=0, drop=True)

# Latest FY as fallback
a["rev_fy_latest"] = a.groupby("company_id")["RevenueStandard"].cummax()

# Align on dates (inner/outer union), then choose quarters TTM else FY
ttm = (q[["company_id","date","rev_ttm_from_quarters"]]
       .merge(a[["company_id","date","rev_fy_latest"]], on=["company_id","date"], how="outer")
       .sort_values(["company_id","date"]))
ttm["revenue_ttm"] = ttm["rev_ttm_from_quarters"].fillna(ttm["rev_fy_latest"])

# For each company, pick latest TTM <= holdings as-of max
asof_max = pd.read_parquet(path_hold)["asof_date"].pipe(pd.to_datetime).max()
ttm_latest = (ttm[ttm["date"] <= asof_max]
              .groupby("company_id", as_index=False)
              .tail(1)[["company_id","revenue_ttm"]])

# Attach to holdings-with-financials and save
hold_fin = pd.read_parquet(out_path)
hold_fin_ttm = hold_fin.merge(ttm_latest, on="company_id", how="left")
out_ttm = os.path.join(PARQ, "vw_holdings_with_ttm.parquet")
hold_fin_ttm.to_parquet(out_ttm, index=False)
print(f"✅ Wrote: {out_ttm} | rows: {len(hold_fin_ttm)}")
hold_fin_ttm.head(10)
import os, json, pandas as pd
from glob import glob
from datetime import datetime

PROJ = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
PARQ = os.path.join(PROJ, "warehouse", "parquet")
FACTS = os.path.join(PROJ, "data", "sec", "company_facts")
os.makedirs(PARQ, exist_ok=True)

# ---------- load or initialize dims/fact ----------
def read_or_empty(path, cols, dtypes):
    if os.path.exists(path):
        return pd.read_parquet(path)
    df = pd.DataFrame(columns=cols).astype(dtypes)
    df.to_parquet(path, index=False)
    return df

path_company = os.path.join(PARQ, "dim_company.parquet")
path_metric  = os.path.join(PARQ, "dim_metric.parquet")
path_unit    = os.path.join(PARQ, "dim_unit.parquet")
path_cal     = os.path.join(PARQ, "dim_calendar.parquet")
path_filing  = os.path.join(PARQ, "dim_filing.parquet")
path_fact    = os.path.join(PARQ, "fact_financials.parquet")

dim_company = read_or_empty(path_company,
    ["company_id","cik","name","ticker","sector","industry","fiscal_year_end_month"],
    {"company_id":"int64","cik":"string","name":"string","ticker":"string",
     "sector":"string","industry":"string","fiscal_year_end_month":"float64"}
)
dim_metric = pd.read_parquet(path_metric) if os.path.exists(path_metric) else pd.DataFrame(
    columns=["metric_id","metric_name","xbrl_tag","metric_group","normal_balance"]
).astype({"metric_id":"int64","metric_name":"string","xbrl_tag":"string","metric_group":"string","normal_balance":"string"})
dim_unit   = read_or_empty(path_unit,
    ["unit_id","unit_code","category","iso_currency","decimals_hint","description"],
    {"unit_id":"int64","unit_code":"string","category":"string","iso_currency":"string",
     "decimals_hint":"float64","description":"string"}
)
dim_calendar = read_or_empty(path_cal,
    ["calendar_id","date","fiscal_year","fiscal_quarter","fiscal_month","period_type"],
    {"calendar_id":"int64","date":"string","fiscal_year":"int64","fiscal_quarter":"string",
     "fiscal_month":"int64","period_type":"string"}
)
dim_filing = read_or_empty(path_filing,
    ["filing_id","form_type","filing_date","accepted_date","period_end_date","accession_number","filing_url"],
    {"filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
     "period_end_date":"string","accession_number":"string","filing_url":"string"}
)
fact_fin = read_or_empty(path_fact,
    ["financial_id","company_id","metric_id","calendar_id","value","unit_id","filing_id","consolidated_flag"],
    {"financial_id":"int64","company_id":"int64","metric_id":"int64","calendar_id":"int64",
     "value":"float64","unit_id":"int64","filing_id":"int64","consolidated_flag":"string"}
)

# ---------- ensure-* helpers (in-cell so it runs standalone) ----------
def ensure_company_by_cik(cik_str, name=""):
    global dim_company
    hit = dim_company.loc[dim_company["cik"]==cik_str]
    if not hit.empty:
        return int(hit.iloc[0]["company_id"])
    next_id = (dim_company["company_id"].max()+1) if len(dim_company) else 1
    row = {"company_id":next_id,"cik":cik_str,"name":name or "", "ticker":pd.NA,
           "sector":pd.NA,"industry":pd.NA,"fiscal_year_end_month":pd.NA}
    dim_company = pd.concat([dim_company, pd.DataFrame([row])], ignore_index=True)
    dim_company.to_parquet(path_company, index=False)
    return next_id

def ensure_metric_by_tag(xbrl_tag, metric_name, metric_group, normal_balance):
    global dim_metric
    hit = dim_metric.loc[dim_metric["xbrl_tag"]==xbrl_tag]
    if not hit.empty:
        return int(hit.iloc[0]["metric_id"])
    next_id = (dim_metric["metric_id"].max()+1) if len(dim_metric) else 1
    row = {"metric_id":next_id,"metric_name":metric_name,"xbrl_tag":xbrl_tag,
           "metric_group":metric_group,"normal_balance":normal_balance}
    dim_metric = pd.concat([dim_metric, pd.DataFrame([row])], ignore_index=True)
    dim_metric.to_parquet(path_metric, index=False)
    return next_id

def ensure_unit(unit_code):
    global dim_unit
    hit = dim_unit.loc[dim_unit["unit_code"]==unit_code]
    if not hit.empty:
        return int(hit.iloc[0]["unit_id"])
    next_id = (dim_unit["unit_id"].max()+1) if len(dim_unit) else 1
    row = {"unit_id":next_id,"unit_code":unit_code,"category":"currency",
           "iso_currency":unit_code if len(unit_code)==3 else "USD","decimals_hint":pd.NA,"description":""}
    dim_unit = pd.concat([dim_unit, pd.DataFrame([row])], ignore_index=True)
    dim_unit.to_parquet(path_unit, index=False)
    return next_id

def ensure_calendar(period_end, fy, fp, period_type):
    """fp like 'FY','Q1'...'Q4'. fiscal_month from period_end."""
    global dim_calendar
    fiscal_month = int(period_end[5:7])
    hit = dim_calendar.loc[(dim_calendar["date"]==period_end) & (dim_calendar["period_type"]==period_type)]
    if not hit.empty:
        return int(hit.iloc[0]["calendar_id"])
    next_id = (dim_calendar["calendar_id"].max()+1) if len(dim_calendar) else 1
    row = {"calendar_id":next_id,"date":period_end,"fiscal_year":int(fy),
           "fiscal_quarter":fp if period_type=="QTR" else "Q4",
           "fiscal_month":fiscal_month,"period_type":period_type}
    dim_calendar = pd.concat([dim_calendar, pd.DataFrame([row])], ignore_index=True)
    dim_calendar.to_parquet(path_cal, index=False)
    return next_id

def ensure_filing(form_type, filed_date, period_end, accession=""):
    global dim_filing
    hit = dim_filing.loc[
        (dim_filing["form_type"]==form_type) &
        (dim_filing["filing_date"]==filed_date) &
        (dim_filing["period_end_date"]==period_end)
    ]
    if accession:
        hit = dim_filing.loc[dim_filing["accession_number"]==accession] if hit.empty else hit
    if not hit.empty:
        return int(hit.iloc[0]["filing_id"])
    next_id = (dim_filing["filing_id"].max()+1) if len(dim_filing) else 1
    row = {"filing_id":next_id,"form_type":form_type or "",
           "filing_date":filed_date or period_end,"accepted_date":filed_date or period_end,
           "period_end_date":period_end,"accession_number":accession or "","filing_url":""}
    dim_filing = pd.concat([dim_filing, pd.DataFrame([row])], ignore_index=True)
    dim_filing.to_parquet(path_filing, index=False)
    return next_id

# ---------- metric families we’ll ingest ----------
REVENUE_TAGS = [
    "Revenues",
    "RevenueFromContractWithCustomerExcludingAssessedTax",
    "SalesRevenueNet","SalesRevenueGoodsNet","SalesRevenueServicesNet",
    "TotalRevenuesAndOtherIncome","PremiumsEarnedNet","InterestIncomeOperating"
]
INCOME_TAGS = [
    "NetIncomeLoss","ProfitLoss",
    "IncomeLossFromContinuingOperations",
    "NetIncomeLossAvailableToCommonStockholdersBasic",
    "NetIncomeLossAttributableToParent"
]

def rows_from_tag(obj):
    """Return list of rows for preferred unit (USD if present else first unit)."""
    units = obj.get("units", {})
    if not units: 
        return None, pd.DataFrame()
    unit_key = "USD" if "USD" in units else next(iter(units.keys()))
    return unit_key, pd.DataFrame(units[unit_key])

def fp_to_quarter(fp):
    fp = (fp or "").upper()
    if fp in ["Q1","Q2","Q3","Q4"]: return fp, "QTR"
    if fp in ["FY",""]: return "Q4", "FY"
    return "Q4","FY"

# ---------- iterate all JSONs ----------
json_files = sorted(glob(os.path.join(FACTS, "CIK*.json")))
print(f"Found {len(json_files):,} company_facts files")

new_rows = []

for i, path in enumerate(json_files, 1):
    try:
        with open(path, "r") as f:
            data = json.load(f)
    except Exception as e:
        print("!! error reading", path, e); 
        continue

    cik10 = str(data.get("cik","")).zfill(10)
    entity = data.get("entityName","")
    co_id = ensure_company_by_cik(cik10, entity)

    usgaap = data.get("facts", {}).get("us-gaap", {})

    # Collect both families present for this company
    families = [
        ("Revenue", "Credit", REVENUE_TAGS),
        ("Net Income", "Credit", INCOME_TAGS)
    ]

    for metric_name, normal_balance, tag_list in families:
        # pick the first tag that exists in this file; we will still store the real tag we used
        tag_used = next((t for t in tag_list if t in usgaap), None)
        if not tag_used:
            continue

        unit_key, df = rows_from_tag(usgaap[tag_used])
        if df.empty: 
            continue

        # Keep only periodized rows that have fy, fp, end, val
        cols_need = {"fy","fp","end","val"}
        df = df[[c for c in df.columns if c in cols_need.union({"form","accn","filed"})]]

        for _, r in df.iterrows():
            fy = r.get("fy")
            fp = r.get("fp")
            end = r.get("end")
            val = r.get("val")
            if pd.isna(fy) or pd.isna(fp) or pd.isna(end) or pd.isna(val):
                continue

            fp_norm, period_type = fp_to_quarter(fp)
            met_id = ensure_metric_by_tag(tag_used, metric_name=metric_name,
                                          metric_group=("Revenue" if metric_name=="Revenue" else "Income"),
                                          normal_balance=normal_balance)
            unit_id = ensure_unit(unit_key)
            cal_id  = ensure_calendar(end, int(fy), fp_norm, period_type)
            fil_id  = ensure_filing(r.get("form",""), r.get("filed", end), end, r.get("accn",""))

            new_rows.append({
                "company_id": co_id,
                "metric_id":  met_id,
                "calendar_id": cal_id,
                "value": float(val),
                "unit_id": unit_id,
                "filing_id": fil_id,
                "consolidated_flag": "Consolidated"
            })

    if i % 250 == 0:
        print(f"...processed {i:,}/{len(json_files):,}")

# Append & de-duplicate by natural key
if new_rows:
    incoming = pd.DataFrame(new_rows)
    if len(fact_fin):
        start_rows = len(fact_fin)
        fact_fin = pd.concat([fact_fin.drop(columns=[], errors="ignore"), incoming], ignore_index=True)
    else:
        start_rows = 0
        fact_fin = incoming.copy()

    natural_key = ["company_id","metric_id","calendar_id","filing_id"]
    fact_fin = fact_fin.drop_duplicates(subset=natural_key, keep="last").reset_index(drop=True)
    fact_fin["financial_id"] = range(1, len(fact_fin)+1)
    fact_fin.to_parquet(path_fact, index=False)
    print(f"✅ Ingested {len(fact_fin)-start_rows:,} new fact rows; total = {len(fact_fin):,}")
else:
    print("No new rows found.")
import os, time, pandas as pd
# === SMILEFund: Fast SEC ingest (Threaded, vectorized, local SSD, Windows-safe) ===
import os, glob, time, warnings, pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

warnings.filterwarnings("ignore", category=FutureWarning)

# ---------- FAST JSON LOADER ----------
try:
    import orjson as _json; loads = _json.loads
except Exception:
    import json as _json; loads = _json.loads
print("JSON loader:", loads.__module__)

# ---------- PATHS ----------
PROJ  = r"C:\smilefund_project"
PARQ  = os.path.join(PROJ, "warehouse", "parquet")
FACTS = r"C:\smilefund_project\data\sec\company_facts"
TMP_RAW = os.path.join(PARQ, "_raw_deltas")
os.makedirs(PARQ, exist_ok=True)
os.makedirs(TMP_RAW, exist_ok=True)
print("FACTS exists:", os.path.exists(FACTS), "| PARQ exists:", os.path.exists(PARQ))

# ---------- HELPERS ----------
def is_money_unit(u: str) -> bool:
    u = (u or "").upper()
    return (u in {"USD","USDM","USDTH","USD$"} or (len(u)==3 and u.isalpha()))

def parse_one(path):
    with open(path, "rb") as f:
        data = loads(f.read())
    out = []
    cik10 = str(data.get("cik","")).zfill(10)
    name  = data.get("entityName","")
    usgaap = (data.get("facts") or {}).get("us-gaap", {})
    for tag, obj in usgaap.items():
        units = (obj or {}).get("units", {})
        if not units:
            continue
        unit_key = next((u for u in units.keys() if is_money_unit(u)), None)
        if not unit_key:
            continue
        for r in obj["units"][unit_key]:
            fy, fp, end, val = r.get("fy"), (r.get("fp") or ""), r.get("end"), r.get("val")
            if fy is None or not fp or not end or val is None:
                continue
            out.append({
                "cik": cik10,
                "company_name": name,
                "xbrl_tag": tag,
                "unit_code": unit_key,
                "fy": int(fy),
                "fp": fp.upper(),
                "period_end": end,
                "value": float(val),
                "form_type": r.get("form",""),
                "filed_date": r.get("filed", end),
                "accession": r.get("accn","")
            })
    return out

def read_or_empty(path, cols, dtypes):
    if os.path.exists(path): 
        return pd.read_parquet(path)
    df = pd.DataFrame(columns=cols).astype(dtypes)
    df.to_parquet(path, index=False)
    return df

# ---------- PHASE A: Threaded scan → raw parquet deltas ----------
files = sorted(glob.glob(os.path.join(FACTS, "CIK*.json")))
print(f"Scanning {len(files):,} JSONs with threads …", flush=True)

MAX_WORKERS = min(16, (os.cpu_count() or 12))   # try 12–16
BATCH       = 2000                               # write every ~2000 files
PRINT_SECS  = 8

start = time.time()
last  = start
delta_paths = []
rows_total  = 0

for k in range(0, len(files), BATCH):
    batch = files[k:k+BATCH]
    recs  = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futs = {ex.submit(parse_one, p): p for p in batch}
        done = 0
        for fut in as_completed(futs):
            recs.extend(fut.result())
            done += 1
            now = time.time()
            if (now - last) >= PRINT_SECS or done == len(batch):
                overall = k + done
                rate = overall / max(1e-6, (now - start))
                eta  = (len(files) - overall) / max(rate, 1e-6)
                print(f"...{overall:,}/{len(files):,} files | rate: {rate:,.1f}/s | "
                      f"elapsed: {(now-start)/60:,.1f}m | ETA: {eta/60:,.1f}m", flush=True)
                last = now

    out_path = os.path.join(TMP_RAW, f"raw_{k+1:06d}_{k+len(batch):06d}.parquet")
    pd.DataFrame.from_records(recs).to_parquet(out_path, index=False)
    delta_paths.append(out_path)
    rows_total += len(recs)
    print(f"💾 wrote raw delta: {os.path.basename(out_path)} (+{len(recs):,} rows)", flush=True)

print(f"✅ Phase A complete: {len(delta_paths)} raw deltas, {rows_total:,} rows")

# ---------- PHASE B: Vectorized build → dims + fact_financials ----------
path_company = os.path.join(PARQ, "dim_company.parquet")
path_metric  = os.path.join(PARQ, "dim_metric.parquet")
path_unit    = os.path.join(PARQ, "dim_unit.parquet")
path_cal     = os.path.join(PARQ, "dim_calendar.parquet")
path_filing  = os.path.join(PARQ, "dim_filing.parquet")
path_fact    = os.path.join(PARQ, "fact_financials.parquet")

dim_company = read_or_empty(path_company,
    ["company_id","cik","name","ticker","sector","industry","fiscal_year_end_month"],
    {"company_id":"int64","cik":"string","name":"string","ticker":"string","sector":"string","industry":"string","fiscal_year_end_month":"float64"})
dim_metric = read_or_empty(path_metric,
    ["metric_id","metric_name","xbrl_tag","metric_group","normal_balance"],
    {"metric_id":"int64","metric_name":"string","xbrl_tag":"string","metric_group":"string","normal_balance":"string"})
dim_unit = read_or_empty(path_unit,
    ["unit_id","unit_code","category","iso_currency","decimals_hint","description"],
    {"unit_id":"int64","unit_code":"string","category":"string","iso_currency":"string","decimals_hint":"float64","description":"string"})
dim_calendar = read_or_empty(path_cal,
    ["calendar_id","date","fiscal_year","fiscal_quarter","fiscal_month","period_type"],
    {"calendar_id":"int64","date":"string","fiscal_year":"int64","fiscal_quarter":"string","fiscal_month":"int64","period_type":"string"})
dim_filing = read_or_empty(path_filing,
    ["filing_id","form_type","filing_date","accepted_date","period_end_date","accession_number","filing_url"],
    {"filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
     "period_end_date":"string","accession_number":"string","filing_url":"string"})

# Load all raw deltas
raw_parts = [pd.read_parquet(p) for p in sorted(delta_paths)]
raw = pd.concat(raw_parts, ignore_index=True) if raw_parts else pd.DataFrame(columns=[
    "cik","company_name","xbrl_tag","unit_code","fy","fp","period_end","value","form_type","filed_date","accession"
])

# --- Company (bulk)
new_co = raw[["cik","company_name"]].drop_duplicates().rename(columns={"company_name":"name"})
dim_company = dim_company.drop_duplicates(subset=["cik"])
missing_co = new_co[~new_co["cik"].isin(dim_company["cik"])]
if not missing_co.empty:
    start_id = int(dim_company["company_id"].max() or 0) + 1
    missing_co = missing_co.reset_index(drop=True)
    missing_co.insert(0, "company_id", range(start_id, start_id + len(missing_co)))
    for col, val in [("ticker",""),("sector",""),("industry",""),("fiscal_year_end_month",pd.NA)]:
        missing_co[col] = val
    dim_company = pd.concat([dim_company, missing_co], ignore_index=True)
dim_company.to_parquet(path_company, index=False)

# --- Metric (bulk)
missing_met = raw[["xbrl_tag"]].drop_duplicates()
missing_met = missing_met[~missing_met["xbrl_tag"].isin(dim_metric["xbrl_tag"])]
if not missing_met.empty:
    start = int(dim_metric["metric_id"].max() or 0) + 1
    add = pd.DataFrame({
        "metric_id": range(start, start+len(missing_met)),
        "metric_name": missing_met["xbrl_tag"].astype("string"),
        "xbrl_tag": missing_met["xbrl_tag"].astype("string"),
        "metric_group": "Auto",
        "normal_balance": ""
    })
    dim_metric = pd.concat([dim_metric, add], ignore_index=True)
dim_metric.to_parquet(path_metric, index=False)

# --- Unit (bulk)
missing_unit = raw[["unit_code"]].drop_duplicates()
missing_unit = missing_unit[~missing_unit["unit_code"].isin(dim_unit["unit_code"])]
if not missing_unit.empty:
    start = int(dim_unit["unit_id"].max() or 0) + 1
    add = pd.DataFrame({
        "unit_id": range(start, start+len(missing_unit)),
        "unit_code": missing_unit["unit_code"].astype("string"),
        "category": "currency",
        "iso_currency": missing_unit["unit_code"].where(missing_unit["unit_code"].str.len()==3, "USD").astype("string"),
        "decimals_hint": pd.NA,
        "description": ""
    })
    dim_unit = pd.concat([dim_unit, add], ignore_index=True)
dim_unit.to_parquet(path_unit, index=False)

# --- Calendar (bulk)
cal = raw[["period_end","fy","fp"]].drop_duplicates()
cal["period_type"]   = cal["fp"].where(cal["fp"].isin(["Q1","Q2","Q3","Q4"]), "FY")
cal["fiscal_quarter"] = cal["fp"].where(cal["period_type"].eq("QTR"), "Q4")
cal["fiscal_month"]  = cal["period_end"].str[5:7].astype("int64")
new_cal = cal.rename(columns={"period_end":"date","fy":"fiscal_year"})
new_cal = new_cal[~new_cal.set_index(["date","period_type"]).index.isin(dim_calendar.set_index(["date","period_type"]).index)]
if not new_cal.empty:
    start = int(dim_calendar["calendar_id"].max() or 0) + 1
    new_cal = new_cal.reset_index(drop=True)
    new_cal.insert(0, "calendar_id", range(start, start+len(new_cal)))
    dim_calendar = pd.concat([dim_calendar, new_cal], ignore_index=True)
dim_calendar.to_parquet(path_cal, index=False)

# --- Filing (bulk)
fil = raw[["form_type","filed_date","period_end","accession"]].drop_duplicates()
key_old = dim_filing.assign(_k = dim_filing["form_type"].fillna("")+"|"+dim_filing["filing_date"].fillna("")+"|"+dim_filing["period_end_date"].fillna("")+"|"+dim_filing["accession_number"].fillna(""))
key_new = fil.assign(_k = fil["form_type"].fillna("")+"|"+fil["filed_date"].fillna("")+"|"+fil["period_end"].fillna("")+"|"+fil["accession"].fillna(""))
to_add = key_new[~key_new["_k"].isin(key_old["_k"])].drop(columns="_k")
if not to_add.empty:
    start = int(dim_filing["filing_id"].max() or 0) + 1
    add = to_add.rename(columns={"filed_date":"filing_date","period_end":"period_end_date","accession":"accession_number"})
    add = add.reset_index(drop=True)
    add.insert(0, "filing_id", range(start, start+len(add)))
    add["accepted_date"] = add["filing_date"]; add["filing_url"] = ""
    dim_filing = pd.concat([dim_filing, add], ignore_index=True)
dim_filing.to_parquet(path_filing, index=False)

# --- Map to IDs & write fact_financials
df = raw.merge(dim_company[["company_id","cik"]], on="cik", how="left") \
        .merge(dim_metric[["metric_id","xbrl_tag"]], on="xbrl_tag", how="left") \
        .merge(dim_unit[["unit_id","unit_code"]], on="unit_code", how="left")

cal_key = dim_calendar[["calendar_id","date","period_type"]]
tmp = raw[["period_end","fp"]].copy()
tmp["period_type"] = tmp["fp"].where(tmp["fp"].isin(["Q1","Q2","Q3","Q4"]), "FY")
df = df.merge(tmp[["period_end","period_type"]].drop_duplicates(),
              on=["period_end","period_type"], how="left") \
       .merge(cal_key, left_on=["period_end","period_type"], right_on=["date","period_type"], how="left") \
       .drop(columns=["date"])

fil_map = dim_filing[["filing_id","form_type","filing_date","period_end_date","accession_number"]]
df = df.merge(fil_map, left_on=["form_type","filed_date","period_end","accession"],
              right_on=["form_type","filing_date","period_end_date","accession_number"], how="left") \
       .drop(columns=["filing_date","period_end_date","accession_number"])

fact_cols = ["company_id","metric_id","calendar_id","value","unit_id","filing_id"]
incoming = df[fact_cols].copy()
incoming["consolidated_flag"] = "Consolidated"

path_fact = os.path.join(PARQ, "fact_financials.parquet")
if os.path.exists(path_fact):
    fact_existing = pd.read_parquet(path_fact)
else:
    fact_existing = pd.DataFrame(columns=["financial_id"] + fact_cols + ["consolidated_flag"])

nk = ["company_id","metric_id","calendar_id","filing_id"]
combined = pd.concat([fact_existing.drop(columns=[], errors="ignore"), incoming], ignore_index=True)
combined = combined.drop_duplicates(subset=nk, keep="last").reset_index(drop=True)
combined["financial_id"] = range(1, len(combined)+1)
combined.to_parquet(path_fact, index=False)

print(f"✅ Done. fact_financials rows: {len(combined):,} | unique tags: {dim_metric['xbrl_tag'].nunique():,} | elapsed: {(time.time()-start)/60:,.1f} min")

# === PHASE B+ (ALL TAGS + GROUPED FAN-OUT) ===
from pathlib import Path
import traceback
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds
import pyarrow.parquet as pq

# ---------------- CONFIG (your path) ----------------
PROJECT_ROOT = Path(r"C:\smilefund_project")
RAW_DIR      = PROJECT_ROOT / r"warehouse\parquet\_raw_deltas"
OUT_DIR      = PROJECT_ROOT / r"warehouse\parquet\facts_all"
OUT_FILE     = OUT_DIR / "facts_all.parquet"
FANOUT_DIR   = PROJECT_ROOT / r"warehouse\parquet\facts_by_group"

BATCH_SIZE     = 250_000
ROW_GROUP_SIZE = 256 * 1024
PARQUET_CODEC  = "snappy"

# -------- Tag groups (extend anytime; unmapped => 'Other') --------
TAG_GROUPS = {
    "Revenue": {
        "Revenues","RevenueFromContractWithCustomerExcludingAssessedTax",
        "SalesRevenueNet","SalesRevenueGoodsNet","SalesRevenueServicesNet",
        "SalesRevenueGoodsAndServicesNet","TotalRevenueAndOtherIncome"
    },
    "Expenses": {
        "CostOfGoodsAndServicesSold","CostOfRevenue","SellingGeneralAndAdministrativeExpense",
        "ResearchAndDevelopmentExpense","OperatingExpenses","InterestExpense","RestructuringCharges"
    },
    "Assets": {
        "Assets","AssetsCurrent","CashAndCashEquivalentsAtCarryingValue","AccountsReceivableNetCurrent",
        "InventoryNet","Goodwill","IntangibleAssetsNetExcludingGoodwill","PropertyPlantAndEquipmentNet"
    },
    "Liabilities": {
        "Liabilities","LiabilitiesCurrent","AccountsPayableCurrent","LongTermDebtNoncurrent",
        "DeferredRevenueCurrent","DeferredTaxLiabilitiesNoncurrent"
    },
    "Equity": {
        "StockholdersEquity","RetainedEarningsAccumulatedDeficit",
        "CommonStockValue","AdditionalPaidInCapital","TreasuryStockValue"
    },
    "CashFlow": {
        "NetCashProvidedByUsedInOperatingActivities","NetCashProvidedByUsedInInvestingActivities",
        "NetCashProvidedByUsedInFinancingActivities","PaymentsForRepurchaseOfCommonStock","PaymentsOfDividends"
    },
    "OCI": {"OtherComprehensiveIncomeLossNetOfTax"},
    "PerShare": {"EarningsPerShareBasic","EarningsPerShareDiluted","CommonStockDividendsPerShareDeclared"},
    "KPIs": {"CustomersNumber","EmployeesNumber","ActiveUsers","RevenuePerShare"},
}
_TAG_TO_GROUP = {t.lower(): g for g, tags in TAG_GROUPS.items() for t in tags}

def map_group_for_batch(tag_array: pa.Array) -> pa.Array:
    # strip namespaces like 'us-gaap:' and map; default 'Other'
    tag_utf8 = pc.cast(tag_array, pa.utf8())
    stripped = pc.replace_substring_regex(tag_utf8, r"^[^:]+:", "")
    lowered  = pc.ascii_lower(stripped)
    py = lowered.to_pylist()
    out = [_TAG_TO_GROUP.get(t, "Other") if t is not None else "Other" for t in py]
    return pa.array(out, type=pa.utf8())

# ---------------- Pre-flight ----------------
if not RAW_DIR.exists():
    raise FileNotFoundError(f"Raw parquet folder not found: {RAW_DIR}")

OUT_DIR.mkdir(parents=True, exist_ok=True)
FANOUT_DIR.mkdir(parents=True, exist_ok=True)

dataset = ds.dataset(str(RAW_DIR), format="parquet")
schema = dataset.schema
wanted_cols = [c for c in [
    "cik","tag","unit","fy","fp","frame","start","end",
    "filed","form","accn","val","value","decimals","qtrs","segment","uom"
] if c in schema.names]

print(f"Found {len(dataset.files)} raw chunks in: {RAW_DIR}")
print("Columns available:", wanted_cols)

# ---------------- (1) Write monolithic facts_all ----------------
writer = None
total_rows = 0
try:
    for batch in dataset.to_batches(columns=wanted_cols, batch_size=BATCH_SIZE):
        tbl = pa.Table.from_batches([batch])
        if "value" in tbl.column_names and "val" not in tbl.column_names:
            tbl = tbl.append_column("val", tbl["value"]).drop(["value"])
        # normalize numeric types (best-effort)
        for col, typ in {"cik": pa.int64(), "fy": pa.int32(), "val": pa.float64()}.items():
            if col in tbl.column_names:
                try:
                    tbl = tbl.set_column(tbl.schema.get_field_index(col), col, pc.cast(tbl[col], typ))
                except Exception:
                    pass
        if writer is None:
            writer = pq.ParquetWriter(OUT_FILE, tbl.schema, compression=PARQUET_CODEC)
        writer.write_table(tbl, row_group_size=ROW_GROUP_SIZE)
        total_rows += tbl.num_rows
        if total_rows % 1_000_000 < BATCH_SIZE:
            print(f"[facts_all] wrote {total_rows:,} rows...")
finally:
    if writer:
        writer.close()
print(f"✅ facts_all DONE — {total_rows:,} rows → {OUT_FILE}")

# ---------------- (2) Fan-out by group/tag (partitioned) ----------------
partitioning = ds.partitioning(flavor="hive")  # group=.../tag=...
written_rows = 0

for batch in dataset.to_batches(columns=wanted_cols, batch_size=BATCH_SIZE):
    tbl = pa.Table.from_batches([batch])
    if "value" in tbl.column_names and "val" not in tbl.column_names:
        tbl = tbl.append_column("val", tbl["value"]).drop(["value"])
    # attach group column
    if "tag" in tbl.column_names:
        group_arr = map_group_for_batch(tbl["tag"])
    else:
        group_arr = pa.array(["Other"] * tbl.num_rows, type=pa.utf8())
    tbl = tbl.append_column("group", group_arr)

    ds.write_dataset(
        data=tbl,
        base_dir=str(FANOUT_DIR),
        format="parquet",
        partitioning=partitioning,
        partitioning_cols=["group","tag"],
        existing_data_behavior="overwrite_or_ignore",  # idempotent runs
    )
    written_rows += tbl.num_rows
    if written_rows % 1_000_000 < BATCH_SIZE:
        print(f"[facts_by_group] wrote {written_rows:,} rows...")

print(f"✅ facts_by_group DONE — {written_rows:,} rows → {FANOUT_DIR}")
print(r"Example partition: facts_by_group\group=Revenue\tag=Revenues\part-*.parquet")
# ---------------- (2) Fan-out by group/tag (partitioned) ----------------
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds

# Define a partitioning schema (this replaces `partitioning_cols=...`)
partition_schema = pa.schema([
    pa.field("group", pa.utf8()),
    pa.field("tag",   pa.utf8()),
])
partitioning = ds.partitioning(partition_schema, flavor="hive")

written_rows = 0

for batch in dataset.to_batches(columns=wanted_cols, batch_size=BATCH_SIZE):
    tbl = pa.Table.from_batches([batch])

    # unify 'value' -> 'val'
    if "value" in tbl.column_names and "val" not in tbl.column_names:
        tbl = tbl.append_column("val", tbl["value"]).drop(["value"])

    # ensure 'tag' is utf8 (required for partitioning)
    if "tag" in tbl.column_names:
        tag_utf8 = pc.cast(tbl["tag"], pa.utf8())
        tbl = tbl.set_column(tbl.column_names.index("tag"), "tag", tag_utf8)
        # add 'group' derived from tag
        group_arr = map_group_for_batch(tag_utf8)
    else:
        group_arr = pa.array(["Other"] * tbl.num_rows, type=pa.utf8())

    tbl = tbl.append_column("group", group_arr)

    ds.write_dataset(
        data=tbl,
        base_dir=str(FANOUT_DIR),
        format="parquet",
        partitioning=partitioning,                     # <-- use schema-based partitioning
        existing_data_behavior="overwrite_or_ignore",  # safe to re-run
    )

    written_rows += tbl.num_rows
    if written_rows % 1_000_000 < BATCH_SIZE:
        print(f"[facts_by_group] wrote {written_rows:,} rows...")

print(f"✅ facts_by_group DONE — {written_rows:,} rows → {FANOUT_DIR}")
# --------- Robust fan-out: tolerate missing 'tag' in some batches ---------
from pathlib import Path
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds
import pyarrow.parquet as pq

PROJECT_ROOT = Path(r"C:\smilefund_project")
RAW_DIR      = PROJECT_ROOT / r"warehouse\parquet\_raw_deltas"
FANOUT_DIR   = PROJECT_ROOT / r"warehouse\parquet\facts_by_group_v2"
FANOUT_DIR.mkdir(parents=True, exist_ok=True)

# --- same TAG_GROUPS and _TAG_TO_GROUP as before ---
TAG_GROUPS = {
    "Revenue": {"Revenues","RevenueFromContractWithCustomerExcludingAssessedTax",
                "SalesRevenueNet","SalesRevenueGoodsNet","SalesRevenueServicesNet",
                "SalesRevenueGoodsAndServicesNet","TotalRevenueAndOtherIncome"},
    "Expenses": {"CostOfGoodsAndServicesSold","CostOfRevenue","SellingGeneralAndAdministrativeExpense",
                 "ResearchAndDevelopmentExpense","OperatingExpenses","InterestExpense","RestructuringCharges"},
    "Assets": {"Assets","AssetsCurrent","CashAndCashEquivalentsAtCarryingValue","AccountsReceivableNetCurrent",
               "InventoryNet","Goodwill","IntangibleAssetsNetExcludingGoodwill","PropertyPlantAndEquipmentNet"},
    "Liabilities": {"Liabilities","LiabilitiesCurrent","AccountsPayableCurrent","LongTermDebtNoncurrent",
                    "DeferredRevenueCurrent","DeferredTaxLiabilitiesNoncurrent"},
    "Equity": {"StockholdersEquity","RetainedEarningsAccumulatedDeficit","CommonStockValue",
               "AdditionalPaidInCapital","TreasuryStockValue"},
    "CashFlow": {"NetCashProvidedByUsedInOperatingActivities","NetCashProvidedByUsedInInvestingActivities",
                 "NetCashProvidedByUsedInFinancingActivities","PaymentsForRepurchaseOfCommonStock","PaymentsOfDividends"},
    "OCI": {"OtherComprehensiveIncomeLossNetOfTax"},
    "PerShare": {"EarningsPerShareBasic","EarningsPerShareDiluted","CommonStockDividendsPerShareDeclared"},
    "KPIs": {"CustomersNumber","EmployeesNumber","ActiveUsers","RevenuePerShare"},
}
_TAG_TO_GROUP = {t.lower(): g for g, tags in TAG_GROUPS.items() for t in tags}

def map_group_for_batch(tag_arr: pa.Array) -> pa.Array:
    # tag_arr is guaranteed utf8 by caller
    stripped = pc.replace_substring_regex(tag_arr, r"^[^:]+:", "")  # drop namespaces e.g. 'us-gaap:'
    lowered  = pc.ascii_lower(stripped)
    py = lowered.to_pylist()
    out = [_TAG_TO_GROUP.get(t, "Other") if t is not None else "Other" for t in py]
    return pa.array(out, type=pa.utf8())

# Build dataset; we’ll request the columns we care about
dataset = ds.dataset(str(RAW_DIR), format="parquet")
wanted_cols = [c for c in [
    "cik","tag","unit","fy","fp","frame","start","end",
    "filed","form","accn","val","value","decimals","qtrs","segment","uom"
] if c in dataset.schema.names]

for i, batch in enumerate(dataset.to_batches(columns=wanted_cols, batch_size=250_000), start=1):
    tbl = pa.Table.from_batches([batch])

    # unify 'value' -> 'val'
    if "value" in tbl.column_names and "val" not in tbl.column_names:
        tbl = tbl.append_column("val", tbl["value"]).drop(["value"])

    # ensure 'tag' column exists and is utf8
    if "tag" not in tbl.column_names:
        # create empty tag column to satisfy partitioner
        tag_utf8 = pa.array([None] * tbl.num_rows, type=pa.utf8())
        tbl = tbl.append_column("tag", tag_utf8)
        # with no tags, whole batch goes to group 'Other'
        group_arr = pa.array(["Other"] * tbl.num_rows, type=pa.utf8())
    else:
        tag_utf8 = pc.cast(tbl["tag"], pa.utf8())
        tbl = tbl.set_column(tbl.column_names.index("tag"), "tag", tag_utf8)
        group_arr = map_group_for_batch(tag_utf8)

    # append group column
    tbl = tbl.append_column("group", group_arr)

    # write to hive partitions group=/tag=
    pq.write_to_dataset(
        table=tbl,
        root_path=str(FANOUT_DIR),
        partition_cols=["group", "tag"],
        compression="snappy",
        existing_data_behavior="overwrite_or_ignore",
    )

    if i % 20 == 0:
        print(f"[fanout] processed {i} batches...")

print("✅ facts_by_group_v2 written at:", FANOUT_DIR)
import duckdb, os, pandas as pd

FANOUT_DIR = r"C:\smilefund_project\warehouse\parquet\facts_by_group_v2"
OUT_CSV    = r"C:\smilefund_project\warehouse\reports\top_other_tags.csv"
os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)

con = duckdb.connect()
# Read all partitioned files (hive: group=/tag= folders create columns)
con.execute(f"""
    WITH base AS (
      SELECT "group", tag
      FROM parquet_scan('{FANOUT_DIR}\\**\\*.parquet', HIVE_PARTITIONING=1)
    )
    SELECT tag, COUNT(*) AS n
    FROM base
    WHERE "group" = 'Other' AND tag IS NOT NULL AND tag <> ''
    GROUP BY 1
    ORDER BY n DESC
    LIMIT 500
""")
df = con.fetch_df()
con.close()

print(df.head(20))
df.to_csv(OUT_CSV, index=False)
print("✅ wrote:", OUT_CSV)
pip install duckdb
# === AUTO-GROUP TAGS BY PATTERN + RE-FAN-OUT TO v3 ===
from pathlib import Path
import re
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds
import pyarrow.parquet as pq

PROJECT_ROOT = Path(r"C:\smilefund_project")
FACTS_ALL    = PROJECT_ROOT / r"warehouse\parquet\facts_all\facts_all.parquet"
RAW_DIR      = PROJECT_ROOT / r"warehouse\parquet\_raw_deltas"  # we stream from raw again (fast, low RAM)
OUT_V3       = PROJECT_ROOT / r"warehouse\parquet\facts_by_group_v3"
OUT_V3.mkdir(parents=True, exist_ok=True)

# 1) Base curated groups you already used
TAG_GROUPS = {
    "Revenue": {
        "Revenues","RevenueFromContractWithCustomerExcludingAssessedTax",
        "SalesRevenueNet","SalesRevenueGoodsNet","SalesRevenueServicesNet",
        "SalesRevenueGoodsAndServicesNet","TotalRevenueAndOtherIncome"
    },
    "Expenses": {
        "CostOfGoodsAndServicesSold","CostOfRevenue","SellingGeneralAndAdministrativeExpense",
        "ResearchAndDevelopmentExpense","OperatingExpenses","InterestExpense","RestructuringCharges"
    },
    "Assets": {
        "Assets","AssetsCurrent","CashAndCashEquivalentsAtCarryingValue","AccountsReceivableNetCurrent",
        "InventoryNet","Goodwill","IntangibleAssetsNetExcludingGoodwill","PropertyPlantAndEquipmentNet"
    },
    "Liabilities": {
        "Liabilities","LiabilitiesCurrent","AccountsPayableCurrent","LongTermDebtNoncurrent",
        "DeferredRevenueCurrent","DeferredTaxLiabilitiesNoncurrent"
    },
    "Equity": {
        "StockholdersEquity","RetainedEarningsAccumulatedDeficit",
        "CommonStockValue","AdditionalPaidInCapital","TreasuryStockValue"
    },
    "CashFlow": {
        "NetCashProvidedByUsedInOperatingActivities","NetCashProvidedByUsedInInvestingActivities",
        "NetCashProvidedByUsedInFinancingActivities","PaymentsForRepurchaseOfCommonStock","PaymentsOfDividends"
    },
    "OCI": {"OtherComprehensiveIncomeLossNetOfTax"},
    "PerShare": {"EarningsPerShareBasic","EarningsPerShareDiluted","CommonStockDividendsPerShareDeclared"},
    "KPIs": {"CustomersNumber","EmployeesNumber","ActiveUsers","RevenuePerShare"},
}
BASE = {t.lower(): g for g, tags in TAG_GROUPS.items() for t in tags}

# 2) Regex pattern rules (order matters — first match wins)
PATTERN_RULES = [
    ("Revenue",   r"(revenue|sales)(?!per|turnover)"),   # revenue-ish
    ("Expenses",  r"(expense|costof|costs?)"),
    ("Assets",    r"(^assets?$|asset)"),
    ("Liabilities", r"(liabilit)"),
    ("Equity",    r"(equity|retainedearnings|paidincapital|treasurystock)"),
    ("CashFlow",  r"(netcash|operatingactivit|investingactivit|financingactivit|dividend|repurchase)"),
    ("OCI",       r"(othercomprehensiveincome)"),
    ("PerShare",  r"(perShare|earningspershare|dividendsPerShare)"),
    ("KPIs",      r"(customersnumber|employeesnumber|activeusers)"),
    # Income statements specifics:
    ("Expenses",  r"(cogs|costofgoods)"),
    ("Revenue",   r"(totalrevenueandotherincome)"),
]

# 3) Build a fast tag -> group mapper
compiled = [(g, re.compile(pat, re.IGNORECASE)) for g, pat in PATTERN_RULES]

def infer_group(tag_utf8: str) -> str:
    if not tag_utf8:
        return "Other"
    # strip namespace like 'us-gaap:'
    base = tag_utf8.split(":", 1)[-1]
    # 3a) curated base
    if base.lower() in BASE:
        return BASE[base.lower()]
    # 3b) regex rules
    for g, rx in compiled:
        if rx.search(base):
            return g
    return "Other"

# 4) Re-fan-out from raw parquet with new inferred groups
dataset = ds.dataset(str(RAW_DIR), format="parquet")
wanted = [c for c in [
    "cik","tag","unit","fy","fp","frame","start","end",
    "filed","form","accn","val","value","decimals","qtrs","segment","uom"
] if c in dataset.schema.names]

# Wipe v3 if re-running from scratch (optional)
# import shutil; shutil.rmtree(OUT_V3, ignore_errors=True); OUT_V3.mkdir(parents=True, exist_ok=True)

bnum = 0
for batch in dataset.to_batches(columns=wanted, batch_size=250_000):
    bnum += 1
    tbl = pa.Table.from_batches([batch])

    # unify value->val
    if "value" in tbl.column_names and "val" not in tbl.column_names:
        tbl = tbl.append_column("val", tbl["value"]).drop(["value"])

    # ensure 'tag' exists
    if "tag" not in tbl.column_names:
        tag_col = pa.array([None] * tbl.num_rows, type=pa.utf8())
        tbl = tbl.append_column("tag", tag_col)
    else:
        tag_col = pc.cast(tbl["tag"], pa.utf8())
        tbl = tbl.set_column(tbl.column_names.index("tag"), "tag", tag_col)

    # vectorized-ish map to group
    py_tags = tbl["tag"].to_pylist()
    groups = [infer_group(t or "") for t in py_tags]
    group_arr = pa.array(groups, type=pa.utf8())
    tbl = tbl.append_column("group", group_arr)

    # write to hive partitions group=/tag=
    pq.write_to_dataset(
        table=tbl,
        root_path=str(OUT_V3),
        partition_cols=["group", "tag"],
        compression="snappy",
        existing_data_behavior="overwrite_or_ignore",
    )

    if bnum % 20 == 0:
        print(f"[v3] processed {bnum} batches...")

print("✅ facts_by_group_v3 at:", OUT_V3)
from pathlib import Path
import json
import pyarrow as pa, pyarrow.parquet as pq, pyarrow.dataset as ds, pyarrow.compute as pc

PROJECT_ROOT = Path(r"C:\smilefund_project")
TICKERS_JSON = PROJECT_ROOT / r"data\sec\company_tickers.json"
FACTS_ALL   = PROJECT_ROOT / r"warehouse\parquet\facts_all\facts_all.parquet"
OUT_DIR     = PROJECT_ROOT / r"warehouse\parquet\dimensions"
OUT_DIR.mkdir(parents=True, exist_ok=True)
OUT_FILE    = OUT_DIR / "dim_company.parquet"

if TICKERS_JSON.exists():
    data = json.loads(TICKERS_JSON.read_text(encoding="utf-8"))
    rows = []
    for rec in data.values():
        try:
            rows.append({
                "cik": int(rec.get("cik_str")),
                "ticker": rec.get("ticker"),
                "name": rec.get("title"),
            })
        except Exception:
            pass
    tbl = pa.Table.from_pylist(rows)
    # Deduplicate by CIK (keep latest occurrence)
    # If pyarrow>=16: tbl = tbl.drop_duplicates(keys=["cik"])
    # Portable way:
    unique_cik = pc.unique(tbl["cik"])
    mask = pc.is_in(tbl["cik"], value_set=unique_cik)  # retains all, we'll just write as-is; parquet readers can distinct later
    tbl = tbl.filter(pc.invert(pc.is_null(tbl["cik"])))
    pq.write_table(tbl, str(OUT_FILE))
    print(f"✅ Wrote {OUT_FILE} with {tbl.num_rows:,} rows (from company_tickers.json)")
else:
    # Fallback: build CIK-only from facts_all
    d = ds.dataset(str(FACTS_ALL), format="parquet")
    ciks = d.to_table(columns=["cik"]).drop_null()
    unique_cik = pc.unique(ciks["cik"])
    tbl = pa.Table.from_arrays([unique_cik], names=["cik"])
    pq.write_table(tbl, str(OUT_FILE))
    print(f"⚠️ company_tickers.json not found. Wrote {OUT_FILE} with CIK only ({tbl.num_rows:,} rows).")
# Build latest snapshots from v3 using fy/fp ranking (no reliance on "end"/"filed")
import os, sys, subprocess

# Ensure duckdb
try:
    import duckdb  # type: ignore
except ModuleNotFoundError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "duckdb"])
    import duckdb  # noqa

V3   = r"C:\smilefund_project\warehouse\parquet\facts_by_group_v3\**\*.parquet"
SNAP = r"C:\smilefund_project\warehouse\parquet\snapshots"
os.makedirs(SNAP, exist_ok=True)

con = duckdb.connect()

# Use parquet_scan with HIVE_PARTITIONING=1 to expose partition cols (group, tag)
# Select only columns that we know exist across all partitions
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW facts_min AS
  SELECT
    tag,
    "group",
    cik,
    fy,
    fp,
    val
  FROM parquet_scan('{V3}', HIVE_PARTITIONING=1)
  WHERE val IS NOT NULL
""")

# Rank helper: annual/quarter ordering weight
# FY > Q4 > Q3 > Q2 > Q1 > anything else (0)
con.execute("""
  CREATE OR REPLACE TEMP VIEW facts_ranked AS
  SELECT
    *,
    CASE fp
      WHEN 'FY' THEN 5
      WHEN 'Q4' THEN 4
      WHEN 'Q3' THEN 3
      WHEN 'Q2' THEN 2
      WHEN 'Q1' THEN 1
      ELSE 0
    END AS fp_rank
  FROM facts_min
""")

# 1) Latest ANY per (cik, tag): choose highest fy, then best fp_rank
con.execute("""
  CREATE OR REPLACE TABLE latest_any AS
  SELECT *
  FROM (
    SELECT
      cik, tag, fy, fp, val,
      ROW_NUMBER() OVER (
        PARTITION BY cik, tag
        ORDER BY fy DESC, fp_rank DESC
      ) rn
    FROM facts_ranked
  )
  WHERE rn = 1
""")
con.execute(f"COPY latest_any TO '{SNAP}\\latest_any.parquet' (FORMAT PARQUET);")

# 2) Latest ANNUAL per (cik, tag, fy): keep FY only, choose best (ties unlikely)
con.execute("""
  CREATE OR REPLACE TABLE latest_annual AS
  SELECT *
  FROM (
    SELECT
      cik, tag, fy, fp, val,
      ROW_NUMBER() OVER (
        PARTITION BY cik, tag, fy
        ORDER BY fp_rank DESC
      ) rn
    FROM facts_ranked
    WHERE fp = 'FY'
  )
  WHERE rn = 1
""")
con.execute(f"COPY latest_annual TO '{SNAP}\\latest_annual.parquet' (FORMAT PARQUET);")

# 3) Latest QUARTERLY per (cik, tag, fy, fp): keep Q1–Q4, 1 row per combo
con.execute("""
  CREATE OR REPLACE TABLE latest_quarterly AS
  SELECT *
  FROM (
    SELECT
      cik, tag, fy, fp, val,
      ROW_NUMBER() OVER (
        PARTITION BY cik, tag, fy, fp
        ORDER BY fp_rank DESC
      ) rn
    FROM facts_ranked
    WHERE fp IN ('Q1','Q2','Q3','Q4')
  )
  WHERE rn = 1
""")
con.execute(f"COPY latest_quarterly TO '{SNAP}\\latest_quarterly.parquet' (FORMAT PARQUET);")

con.close()
print("✅ Snapshots written:\n -", SNAP + "\\latest_any.parquet",
      "\n -", SNAP + "\\latest_annual.parquet",
      "\n -", SNAP + "\\latest_quarterly.parquet")
# Step 3 — KPI wide (annual) from latest_annual snapshot
import os
import duckdb

SNAP_ANNUAL = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
OUT_PATH    = r"C:\smilefund_project\warehouse\parquet\marts\kpi_wide_annual.parquet"
os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)

# Choose common tags (add more later any time)
TAGS = (
    'Revenues',
    'NetIncomeLoss',
    'GrossProfit',
    'OperatingIncomeLoss',
    'EarningsPerShareDiluted',
    'Assets', 'Liabilities', 'StockholdersEquity',
    'NetCashProvidedByUsedInOperatingActivities'
)

con = duckdb.connect()
con.execute(f"CREATE OR REPLACE TEMP VIEW la AS SELECT * FROM read_parquet('{SNAP_ANNUAL}')")

# Build wide table (one row per cik & fiscal year)
con.execute(f"""
  COPY (
    WITH base AS (
      SELECT cik, fy, tag, val
      FROM la
      WHERE tag IN ({','.join([f"'{t}'" for t in TAGS])})
    ),
    pivoted AS (
      SELECT
        cik, fy,
        MAX(CASE WHEN tag='Revenues' THEN val END)                                 AS Revenues,
        MAX(CASE WHEN tag='NetIncomeLoss' THEN val END)                            AS NetIncome,
        MAX(CASE WHEN tag='GrossProfit' THEN val END)                              AS GrossProfit,
        MAX(CASE WHEN tag='OperatingIncomeLoss' THEN val END)                      AS OperatingIncome,
        MAX(CASE WHEN tag='EarningsPerShareDiluted' THEN val END)                  AS EPS_Diluted,
        MAX(CASE WHEN tag='Assets' THEN val END)                                   AS Assets,
        MAX(CASE WHEN tag='Liabilities' THEN val END)                              AS Liabilities,
        MAX(CASE WHEN tag='StockholdersEquity' THEN val END)                       AS Equity,
        MAX(CASE WHEN tag='NetCashProvidedByUsedInOperatingActivities' THEN val END) AS CFO
      FROM base
      GROUP BY cik, fy
    )
    SELECT * FROM pivoted
    ORDER BY cik, fy
  ) TO '{OUT_PATH}' (FORMAT PARQUET);
""")
con.close()

print("✅ Wrote:", OUT_PATH)
# Step 4 — YoY growth & margins on KPI wide (annual)
import os
import duckdb

WIDE = r"C:\smilefund_project\warehouse\parquet\marts\kpi_wide_annual.parquet"
OUT  = r"C:\smilefund_project\warehouse\parquet\marts\kpi_growth_annual.parquet"
os.makedirs(os.path.dirname(OUT), exist_ok=True)

con = duckdb.connect()
con.execute(f"CREATE OR REPLACE TEMP VIEW w AS SELECT * FROM read_parquet('{WIDE}')")

# Build growth/margin table
con.execute(f"""
  COPY (
    WITH base AS (
      SELECT
        cik, fy,
        Revenues,
        NetIncome,
        GrossProfit,
        OperatingIncome,
        EPS_Diluted AS EPS,
        Assets, Liabilities, Equity,
        CFO
      FROM w
    ),
    with_lag AS (
      SELECT
        *,
        LAG(Revenues) OVER (PARTITION BY cik ORDER BY fy) AS Revenues_prev,
        LAG(EPS)      OVER (PARTITION BY cik ORDER BY fy) AS EPS_prev
      FROM base
    )
    SELECT
      *,
      CASE WHEN Revenues_prev IS NOT NULL AND Revenues_prev <> 0
           THEN (Revenues - Revenues_prev) / NULLIF(Revenues_prev,0) END AS YoY_Revenue_Growth,
      CASE WHEN Revenues IS NOT NULL AND Revenues <> 0
           THEN NetIncome   / NULLIF(Revenues,0) END AS NetMargin,
      CASE WHEN Revenues IS NOT NULL AND Revenues <> 0
           THEN GrossProfit / NULLIF(Revenues,0) END AS GrossMargin,
      CASE WHEN Revenues IS NOT NULL AND Revenues <> 0
           THEN OperatingIncome / NULLIF(Revenues,0) END AS OpMargin,
      CASE WHEN EPS_prev IS NOT NULL AND EPS_prev <> 0
           THEN (EPS - EPS_prev) / NULLIF(EPS_prev,0) END AS YoY_EPS_Growth
    FROM with_lag
    ORDER BY cik, fy
  ) TO '{OUT}' (FORMAT PARQUET);
""")
con.close()

print("✅ Wrote:", OUT)
# Step 5 — lightweight helpers over your snapshots/marts
import duckdb
import pandas as pd

SNAP_ANNUAL     = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
SNAP_QUARTERLY  = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_quarterly.parquet"

con = duckdb.connect()
con.execute(f"CREATE OR REPLACE TEMP VIEW la AS SELECT * FROM read_parquet('{SNAP_ANNUAL}')")
con.execute(f"CREATE OR REPLACE TEMP VIEW lq AS SELECT * FROM read_parquet('{SNAP_QUARTERLY}')")

def list_top_tags(n: int = 50) -> pd.DataFrame:
    q = f"""
      SELECT tag, COUNT(*) AS companies
      FROM la
      GROUP BY tag
      ORDER BY companies DESC
      LIMIT {n}
    """
    return con.execute(q).fetch_df()

def top_companies_for_tag(tag: str, n: int = 20) -> pd.DataFrame:
    # Shows who reports the largest latest-annual value for this tag
    q = f"""
      SELECT cik, fy, val
      FROM la
      WHERE tag = '{tag}'
      ORDER BY val DESC NULLS LAST
      LIMIT {n}
    """
    return con.execute(q).fetch_df()

def cik_history(cik: int, tag: str) -> pd.DataFrame:
    # Combines annual + quarterly into a single sorted series
    q = f"""
      WITH annual AS (
        SELECT 'FY' AS freq, cik, fy, fp, val
        FROM la
        WHERE cik = {cik} AND tag = '{tag}'
      ),
      quarterly AS (
        SELECT 'Q' AS freq, cik, fy, fp, val
        FROM lq
        WHERE cik = {cik} AND tag = '{tag}'
      )
      SELECT * FROM annual
      UNION ALL
      SELECT * FROM quarterly
      ORDER BY fy, 
        CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 WHEN 'FY' THEN 5 ELSE 0 END
    """
    return con.execute(q).fetch_df()

print("✅ Helpers ready:")
print(" - list_top_tags(n=50)")
print(" - top_companies_for_tag('Revenues', n=20)")
print(" - cik_history(0000320193, 'Revenues')  # example CIK (Apple)")
# Step 6 — Top companies by latest-annual Revenues
import os
import duckdb
import pandas as pd

SNAP_ANNUAL = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
OUT_CSV     = r"C:\smilefund_project\warehouse\reports\top_revenues_latest_annual.csv"
os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)

con = duckdb.connect()
df = con.execute(f"""
  SELECT cik, fy, val AS Revenues
  FROM read_parquet('{SNAP_ANNUAL}')
  WHERE tag = 'Revenues'
  ORDER BY Revenues DESC NULLS LAST
  LIMIT 50
""").fetch_df()
con.close()

print(df.head(20))
df.to_csv(OUT_CSV, index=False)
print("✅ Saved:", OUT_CSV)
# Step 7 — Apple (CIK 0000320193) Revenue history (annual + quarterly)
import os, sys, subprocess
import duckdb
import pandas as pd

# Optional: ensure matplotlib present for charts
try:
    import matplotlib.pyplot as plt
except ModuleNotFoundError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "matplotlib"])
    import matplotlib.pyplot as plt

CIK = 320193  # Apple
SNAP_ANNUAL    = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
SNAP_QUARTERLY = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_quarterly.parquet"
OUT_DIR        = r"C:\smilefund_project\warehouse\reports"
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

# Annual Revenues (FY)
df_annual = con.execute(f"""
  SELECT fy, val AS Revenues
  FROM read_parquet('{SNAP_ANNUAL}')
  WHERE cik = {CIK} AND tag = 'Revenues'
  ORDER BY fy
""").fetch_df()

# Quarterly Revenues (Q1–Q4)
df_quarterly = con.execute(f"""
  SELECT fy, fp, val AS Revenues
  FROM read_parquet('{SNAP_QUARTERLY}')
  WHERE cik = {CIK} AND tag = 'Revenues' AND fp IN ('Q1','Q2','Q3','Q4')
  ORDER BY fy,
    CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE 0 END
""").fetch_df()

con.close()

# Save CSVs
annual_csv    = os.path.join(OUT_DIR, "AAPL_revenues_annual.csv")
quarterly_csv = os.path.join(OUT_DIR, "AAPL_revenues_quarterly.csv")
df_annual.to_csv(annual_csv, index=False)
df_quarterly.to_csv(quarterly_csv, index=False)
print("✅ Saved:\n -", annual_csv, "\n -", quarterly_csv)

# Charts (simple)
if not df_annual.empty:
    plt.figure(figsize=(9,4.5))
    plt.plot(df_annual["fy"], df_annual["Revenues"], marker="o")
    plt.title("Apple (CIK 320193) — Annual Revenues")
    plt.xlabel("Fiscal Year")
    plt.ylabel("Revenues")
    plt.grid(True, alpha=0.3)
    plt.show()

if not df_quarterly.empty:
    # build x-axis labels like 2023-Q1 etc.
    df_quarterly = df_quarterly.copy()
    df_quarterly["period"] = df_quarterly["fy"].astype(str) + "-" + df_quarterly["fp"]
    plt.figure(figsize=(11,4.5))
    plt.plot(df_quarterly["period"], df_quarterly["Revenues"], marker=".")
    plt.title("Apple (CIK 320193) — Quarterly Revenues")
    plt.xlabel("Fiscal Period")
    plt.ylabel("Revenues")
    plt.xticks(rotation=60)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
# Re-export Apple (CIK 320193) Revenues using normalized tag (strip 'us-gaap:' etc.)
import os, sys, subprocess, duckdb
import pandas as pd

# ensure matplotlib (for quick charts)
try:
    import matplotlib.pyplot as plt
except ModuleNotFoundError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "matplotlib"])
    import matplotlib.pyplot as plt

CIK = 320193
SNAP_ANNUAL    = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
SNAP_QUARTERLY = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_quarterly.parquet"
OUT_DIR        = r"C:\smilefund_project\warehouse\reports"
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

# Peek at how tag is spelled for this CIK (sanity)
preview = con.execute(f"""
  SELECT tag, COUNT(*) n
  FROM read_parquet('{SNAP_ANNUAL}')
  WHERE cik = {CIK}
  GROUP BY tag
  ORDER BY n DESC
  LIMIT 10
""").fetch_df()
print("Tag preview for CIK 320193:\n", preview)

# Annual Revenues (normalize tag)
df_annual = con.execute(f"""
  WITH src AS (
    SELECT cik,
           CASE WHEN position(':' IN tag) > 0 THEN split_part(tag, ':', 2) ELSE tag END AS tag_norm,
           fy, val
    FROM read_parquet('{SNAP_ANNUAL}')
    WHERE cik = {CIK}
  )
  SELECT fy, val AS Revenues
  FROM src
  WHERE tag_norm = 'Revenues'
  ORDER BY fy
""").fetch_df()

# Quarterly Revenues (normalize tag)
df_quarterly = con.execute(f"""
  WITH src AS (
    SELECT cik,
           CASE WHEN position(':' IN tag) > 0 THEN split_part(tag, ':', 2) ELSE tag END AS tag_norm,
           fy, fp, val
    FROM read_parquet('{SNAP_QUARTERLY}')
    WHERE cik = {CIK}
  )
  SELECT fy, fp, val AS Revenues
  FROM src
  WHERE tag_norm = 'Revenues' AND fp IN ('Q1','Q2','Q3','Q4')
  ORDER BY fy,
    CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE 0 END
""").fetch_df()

con.close()

# Save CSVs
annual_csv    = os.path.join(OUT_DIR, "AAPL_revenues_annual.csv")
quarterly_csv = os.path.join(OUT_DIR, "AAPL_revenues_quarterly.csv")
df_annual.to_csv(annual_csv, index=False)
df_quarterly.to_csv(quarterly_csv, index=False)
print("✅ Re-saved:\n -", annual_csv, "\n -", quarterly_csv)

# Charts
if not df_annual.empty:
    plt.figure(figsize=(9,4.5))
    plt.plot(df_annual["fy"], df_annual["Revenues"], marker="o")
    plt.title("Apple — Annual Revenues")
    plt.xlabel("Fiscal Year"); plt.ylabel("Revenues"); plt.grid(True, alpha=0.3); plt.show()

if not df_quarterly.empty:
    df_quarterly = df_quarterly.copy()
    df_quarterly["period"] = df_quarterly["fy"].astype(str) + "-" + df_quarterly["fp"]
    plt.figure(figsize=(11,4.5))
    plt.plot(df_quarterly["period"], df_quarterly["Revenues"], marker=".")
    plt.title("Apple — Quarterly Revenues")
    plt.xlabel("Fiscal Period"); plt.ylabel("Revenues"); plt.xticks(rotation=60)
    plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()
# Rebuild snapshots with normalized tag (no namespace) so filters like tag='Revenues' work everywhere
import os, sys, subprocess
try:
    import duckdb  # type: ignore
except ModuleNotFoundError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "duckdb"])
    import duckdb

V3   = r"C:\smilefund_project\warehouse\parquet\facts_by_group_v3\**\*.parquet"
SNAP = r"C:\smilefund_project\warehouse\parquet\snapshots"
os.makedirs(SNAP, exist_ok=True)

con = duckdb.connect()

# Source from hive-partitioned v3; normalize tag -> tag_norm, then expose it as 'tag'
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW facts_min AS
  SELECT
    CASE WHEN position(':' IN tag) > 0 THEN split_part(tag, ':', 2) ELSE tag END AS tag,  -- normalized
    "group",
    cik, fy, fp, val
  FROM parquet_scan('{V3}', HIVE_PARTITIONING=1)
  WHERE val IS NOT NULL
""")

# Rank helper for choosing "latest"
con.execute("""
  CREATE OR REPLACE TEMP VIEW facts_ranked AS
  SELECT
    *,
    CASE fp WHEN 'FY' THEN 5 WHEN 'Q4' THEN 4 WHEN 'Q3' THEN 3 WHEN 'Q2' THEN 2 WHEN 'Q1' THEN 1 ELSE 0 END AS fp_rank
  FROM facts_min
""")

# Overwrite snapshots using normalized tag
con.execute("""
  CREATE OR REPLACE TABLE latest_any AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag ORDER BY fy DESC, fp_rank DESC) rn
    FROM facts_ranked
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_any TO '{SNAP}\\latest_any.parquet' (FORMAT PARQUET);")

con.execute("""
  CREATE OR REPLACE TABLE latest_annual AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag, fy ORDER BY fp_rank DESC) rn
    FROM facts_ranked
    WHERE fp = 'FY'
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_annual TO '{SNAP}\\latest_annual.parquet' (FORMAT PARQUET);")

con.execute("""
  CREATE OR REPLACE TABLE latest_quarterly AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag, fy, fp ORDER BY fp_rank DESC) rn
    FROM facts_ranked
    WHERE fp IN ('Q1','Q2','Q3','Q4')
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_quarterly TO '{SNAP}\\latest_quarterly.parquet' (FORMAT PARQUET);")

con.close()
print("✅ Snapshots rebuilt with normalized tag.")
# Re-export Apple (CIK 320193) Revenues — annual + quarterly (normalized tag)
import os, duckdb
import pandas as pd

CIK = 320193
SNAP_ANNUAL    = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
SNAP_QUARTERLY = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_quarterly.parquet"
OUT_DIR        = r"C:\smilefund_project\warehouse\reports"
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

df_annual = con.execute(f"""
  SELECT fy, val AS Revenues
  FROM read_parquet('{SNAP_ANNUAL}')
  WHERE cik = {CIK} AND tag = 'Revenues'
  ORDER BY fy
""").fetch_df()

df_quarterly = con.execute(f"""
  SELECT fy, fp, val AS Revenues
  FROM read_parquet('{SNAP_QUARTERLY}')
  WHERE cik = {CIK} AND tag = 'Revenues' AND fp IN ('Q1','Q2','Q3','Q4')
  ORDER BY fy,
    CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE 0 END
""").fetch_df()

con.close()

annual_csv    = os.path.join(OUT_DIR, "AAPL_revenues_annual.csv")
quarterly_csv = os.path.join(OUT_DIR, "AAPL_revenues_quarterly.csv")
df_annual.to_csv(annual_csv, index=False)
df_quarterly.to_csv(quarterly_csv, index=False)

print("Annual rows:", len(df_annual), "→", annual_csv)
print(df_annual.tail(10))
print("\nQuarterly rows:", len(df_quarterly), "→", quarterly_csv)
print(df_quarterly.tail(12))
# Diagnose Apple's tags + export Revenues with fallbacks
import os, duckdb, pandas as pd

CIK = 320193
SNAP_DIR = r"C:\smilefund_project\warehouse\parquet\snapshots"
ANNUAL   = os.path.join(SNAP_DIR, "latest_annual.parquet")
QUARTER  = os.path.join(SNAP_DIR, "latest_quarterly.parquet")
ANY      = os.path.join(SNAP_DIR, "latest_any.parquet")
OUT_DIR  = r"C:\smilefund_project\warehouse\reports"
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

print("Distinct FP values for this CIK (annual + quarterly):")
print(con.execute(f"""
  SELECT fp, COUNT(*) n
  FROM (
    SELECT fp FROM read_parquet('{ANNUAL}') WHERE cik={CIK}
    UNION ALL
    SELECT fp FROM read_parquet('{QUARTER}') WHERE cik={CIK}
  )
  GROUP BY fp ORDER BY n DESC
""").fetch_df())

print("\nTop tags in latest_annual for this CIK:")
print(con.execute(f"""
  SELECT tag, COUNT(*) n
  FROM read_parquet('{ANNUAL}')
  WHERE cik={CIK}
  GROUP BY tag ORDER BY n DESC LIMIT 30
""").fetch_df())

REVENUE_CANDIDATES = (
    'Revenues',
    'SalesRevenueNet',
    'RevenueFromContractWithCustomerExcludingAssessedTax',
    'SalesRevenueGoodsAndServicesNet',
    'SalesRevenueGoodsNet',
    'SalesRevenueServicesNet',
    'TotalRevenueAndOtherIncome'
)

# Pick first tag that exists in annual
row = con.execute(f"""
  WITH t AS (
    SELECT tag
    FROM read_parquet('{ANNUAL}')
    WHERE cik={CIK} AND tag IN ({','.join([f"'{t}'" for t in REVENUE_CANDIDATES])})
    GROUP BY tag
  )
  SELECT tag FROM t ORDER BY
    CASE tag
      WHEN 'Revenues' THEN 1
      WHEN 'SalesRevenueNet' THEN 2
      WHEN 'RevenueFromContractWithCustomerExcludingAssessedTax' THEN 3
      WHEN 'SalesRevenueGoodsAndServicesNet' THEN 4
      WHEN 'SalesRevenueGoodsNet' THEN 5
      WHEN 'SalesRevenueServicesNet' THEN 6
      WHEN 'TotalRevenueAndOtherIncome' THEN 7
      ELSE 99
    END
  LIMIT 1
""").fetchone()

chosen_tag = row[0] if row else None
print("\nChosen revenue-like tag (annual):", chosen_tag)

# Export annual (prefer FY; if none, use latest_any as fallback)
if chosen_tag:
    df_annual = con.execute(f"""
      SELECT fy, val AS Revenues
      FROM read_parquet('{ANNUAL}')
      WHERE cik={CIK} AND tag='{chosen_tag}'
      ORDER BY fy
    """).fetch_df()
else:
    df_annual = pd.DataFrame(columns=["fy","Revenues"])

if df_annual.empty:
    # fallback to latest_any (single row per (cik, tag))
    print("\nNo annual FY rows found for candidates; falling back to latest_any.")
    df_any = con.execute(f"""
      SELECT tag, val AS Revenues
      FROM read_parquet('{ANY}')
      WHERE cik={CIK} AND tag IN ({','.join([f"'{t}'" for t in REVENUE_CANDIDATES])})
      ORDER BY Revenues DESC NULLS LAST
      LIMIT 1
    """).fetch_df()
    print(df_any)

# Export quarterly using same chosen tag (if exists there), else try other candidates
if chosen_tag:
    df_quarter = con.execute(f"""
      SELECT fy, fp, val AS Revenues
      FROM read_parquet('{QUARTER}')
      WHERE cik={CIK} AND tag='{chosen_tag}' AND fp IN ('Q1','Q2','Q3','Q4')
      ORDER BY fy,
        CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE 0 END
    """).fetch_df()
else:
    df_quarter = pd.DataFrame(columns=["fy","fp","Revenues"])

if df_quarter.empty:
    # try other candidates for quarterly
    df_quarter = con.execute(f"""
      SELECT fy, fp, val AS Revenues, tag
      FROM read_parquet('{QUARTER}')
      WHERE cik={CIK} AND fp IN ('Q1','Q2','Q3','Q4') AND tag IN ({','.join([f"'{t}'" for t in REVENUE_CANDIDATES])})
      ORDER BY fy,
        CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE 0 END
    """).fetch_df()
    print("\nQuarterly fallback rows (with tag shown):")
    print(df_quarter.head(12))

con.close()

annual_csv    = os.path.join(OUT_DIR, "AAPL_revenues_annual.csv")
quarterly_csv = os.path.join(OUT_DIR, "AAPL_revenues_quarterly.csv")
df_annual.to_csv(annual_csv, index=False)
df_quarter.to_csv(quarterly_csv, index=False)

print("\n✅ Wrote CSVs:")
print(" -", annual_csv, f"({len(df_annual)} rows)")
print(" -", quarterly_csv, f"({len(df_quarter)} rows)")
print("\nIf annual is empty or the chosen tag isn't 'Revenues', tell me what the 'Top tags' table shows and we'll lock in the right alias.")
# Rebuild snapshots with normalized tag and filter out blank/__HIVE_DEFAULT_PARTITION__
import os, sys, subprocess
try:
    import duckdb  # type: ignore
except ModuleNotFoundError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "duckdb"])
    import duckdb

V3   = r"C:\smilefund_project\warehouse\parquet\facts_by_group_v3\**\*.parquet"
SNAP = r"C:\smilefund_project\warehouse\parquet\snapshots"
os.makedirs(SNAP, exist_ok=True)

con = duckdb.connect()

# 1) Read from hive-partitioned files, normalize tag, and DROP bad tags
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW facts_min AS
  SELECT
    CASE WHEN position(':' IN tag) > 0 THEN split_part(tag, ':', 2) ELSE tag END AS tag,
    "group", cik, fy, fp, val
  FROM parquet_scan('{V3}', HIVE_PARTITIONING=1)
  WHERE val IS NOT NULL
    AND tag IS NOT NULL
    AND tag <> ''
    AND tag <> '__HIVE_DEFAULT_PARTITION__'
""")

# 2) Rank helper for "latest"
con.execute("""
  CREATE OR REPLACE TEMP VIEW facts_ranked AS
  SELECT
    *,
    CASE fp WHEN 'FY' THEN 5 WHEN 'Q4' THEN 4 WHEN 'Q3' THEN 3 WHEN 'Q2' THEN 2 WHEN 'Q1' THEN 1 ELSE 0 END AS fp_rank
  FROM facts_min
""")

# 3) Write snapshots
con.execute("""
  CREATE OR REPLACE TABLE latest_any AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag ORDER BY fy DESC, fp_rank DESC) rn
    FROM facts_ranked
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_any TO '{SNAP}\\latest_any.parquet' (FORMAT PARQUET);")

con.execute("""
  CREATE OR REPLACE TABLE latest_annual AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag, fy ORDER BY fp_rank DESC) rn
    FROM facts_ranked
    WHERE fp = 'FY'
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_annual TO '{SNAP}\\latest_annual.parquet' (FORMAT PARQUET);")

con.execute("""
  CREATE OR REPLACE TABLE latest_quarterly AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag, fy, fp ORDER BY fp_rank DESC) rn
    FROM facts_ranked
    WHERE fp IN ('Q1','Q2','Q3','Q4')
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_quarterly TO '{SNAP}\\latest_quarterly.parquet' (FORMAT PARQUET);")

# quick sanity: how many rows now for Apple in latest_annual?
apple = con.execute("""
  SELECT tag, COUNT(*) n
  FROM latest_annual
  WHERE cik = 320193
  GROUP BY tag ORDER BY n DESC
""").fetch_df()
con.close()

print("✅ Snapshots rebuilt (bad tags filtered).")
print("Apple tags present in latest_annual now:\n", apple.head(10))
from pathlib import Path
import os, re, time, shutil
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds
import pyarrow.parquet as pq

PROJECT_ROOT = Path(r"C:\smilefund_project")
RAW_DIR      = PROJECT_ROOT / r"warehouse\parquet\_raw_deltas"
OUT_V4       = PROJECT_ROOT / r"warehouse\parquet\facts_by_group_v4"
OUT_V4_SAMPLE= PROJECT_ROOT / r"warehouse\parquet\facts_by_group_v4_sample"

print("RAW_DIR:", RAW_DIR)
if not RAW_DIR.exists():
    raise FileNotFoundError(f"RAW dir does not exist: {RAW_DIR}")

# 1) Dataset + file count
dataset = ds.dataset(str(RAW_DIR), format="parquet")
print("RAW columns:", dataset.schema.names)
print("Number of RAW files:", len(dataset.files))

# 2) Grab the first batch for a quick preview
BATCH_SIZE = 100_000
batches = dataset.to_batches(batch_size=BATCH_SIZE)
first_batch = None
try:
    first_batch = next(batches)
except StopIteration:
    first_batch = None

if first_batch is None:
    raise RuntimeError("RAW dataset appears to be empty—no batches yielded.")

tbl0 = pa.Table.from_batches([first_batch])
print(f"First batch rows: {tbl0.num_rows:,}")
print("Preview columns present:", tbl0.column_names)
show_cols = [c for c in ["cik","xbrl_tag","value","unit_code","fy","fp","period_end","filed_date"] if c in tbl0.column_names]
print("Preview (first 5 rows):")
print(tbl0.select(show_cols).slice(0,5).to_pandas())

# ---- grouping helpers ----
TAG_GROUPS = {
    "Revenue": {"Revenues","SalesRevenueNet",
                "RevenueFromContractWithCustomerExcludingAssessedTax",
                "SalesRevenueGoodsAndServicesNet","SalesRevenueGoodsNet","SalesRevenueServicesNet",
                "TotalRevenueAndOtherIncome"},
    "Expenses": {"CostOfGoodsAndServicesSold","CostOfRevenue",
                 "SellingGeneralAndAdministrativeExpense","ResearchAndDevelopmentExpense",
                 "OperatingExpenses","InterestExpense","RestructuringCharges"},
    "Assets": {"Assets","AssetsCurrent","CashAndCashEquivalentsAtCarryingValue",
               "AccountsReceivableNetCurrent","InventoryNet","Goodwill",
               "IntangibleAssetsNetExcludingGoodwill","PropertyPlantAndEquipmentNet"},
    "Liabilities": {"Liabilities","LiabilitiesCurrent","AccountsPayableCurrent",
                    "LongTermDebtNoncurrent","DeferredRevenueCurrent","DeferredTaxLiabilitiesNoncurrent"},
    "Equity": {"StockholdersEquity","RetainedEarningsAccumulatedDeficit","CommonStockValue",
               "AdditionalPaidInCapital","TreasuryStockValue"},
    "CashFlow": {"NetCashProvidedByUsedInOperatingActivities","NetCashProvidedByUsedInInvestingActivities",
                 "NetCashProvidedByUsedInFinancingActivities","PaymentsForRepurchaseOfCommonStock","PaymentsOfDividends"},
    "OCI": {"OtherComprehensiveIncomeLossNetOfTax"},
    "PerShare": {"EarningsPerShareBasic","EarningsPerShareDiluted","CommonStockDividendsPerShareDeclared"},
    "KPIs": {"CustomersNumber","EmployeesNumber","ActiveUsers","RevenuePerShare"},
}
BASE = {t.lower(): g for g, tags in TAG_GROUPS.items() for t in tags}
PATTERN_RULES = [
    ("Revenue",   r"(revenue|sales)(?!per)"),
    ("Expenses",  r"(expense|costof|costs?|cogs)"),
    ("Assets",    r"(^assets?$|asset)"),
    ("Liabilities", r"(liabilit)"),
    ("Equity",    r"(equity|retainedearnings|paidincapital|treasurystock)"),
    ("CashFlow",  r"(netcash|operatingactivit|investingactivit|financingactivit|dividend|repurchase)"),
    ("OCI",       r"(othercomprehensiveincome)"),
    ("PerShare",  r"(earningspershare|dividendsPerShare|perShare)"),
    ("Revenue",   r"(totalrevenueandotherincome)"),
]
compiled = [(g, re.compile(p, re.IGNORECASE)) for g,p in PATTERN_RULES]

def infer_group(tag_str: str) -> str:
    if not tag_str:
        return "Other"
    base = tag_str.split(":", 1)[-1]
    low = base.lower()
    if low in BASE:
        return BASE[low]
    for g, rx in compiled:
        if rx.search(low):
            return g
    return "Other"

def ensure_col(tbl: pa.Table, name: str, typ: pa.DataType) -> pa.Table:
    if name not in tbl.column_names:
        tbl = tbl.append_column(name, pa.array([None]*tbl.num_rows, type=typ))
    return tbl

def cast_if_possible(tbl: pa.Table, name: str, typ: pa.DataType) -> pa.Table:
    if name in tbl.column_names:
        try:
            tbl = tbl.set_column(tbl.schema.get_field_index(name), name, pc.cast(tbl[name], typ))
        except Exception:
            pass
    return tbl

def normalize_and_partition(tbl: pa.Table) -> pa.Table:
    # Ensure expected RAW columns present & cast
    for name, typ in [
        ("cik", pa.int64()),
        ("xbrl_tag", pa.utf8()),
        ("value", pa.float64()),
        ("unit_code", pa.utf8()),
        ("fy", pa.int32()),
        ("fp", pa.utf8()),
        ("period_end", pa.utf8()),
        ("filed_date", pa.utf8()),
        ("form_type", pa.utf8()),
        ("accession", pa.utf8()),
    ]:
        tbl = ensure_col(tbl, name, typ)
        tbl = cast_if_possible(tbl, name, typ)

    # Canonical rename
    ren_map = {
        "xbrl_tag": "tag",
        "value": "val",
        "unit_code": "unit",
        "period_end": "end",
        "filed_date": "filed",
        "form_type": "form",
        "accession": "accn",
    }
    for src, dst in ren_map.items():
        if src in tbl.column_names:
            col = tbl[src]
            if dst in tbl.column_names:
                tbl = tbl.set_column(tbl.schema.get_field_index(dst), dst, col)
            else:
                tbl = tbl.append_column(dst, col)

    # Clean tag
    tag_utf8 = pc.cast(tbl["tag"], pa.utf8())
    tbl = tbl.set_column(tbl.schema.get_field_index("tag"), "tag", tag_utf8)
    mask = pc.and_(pc.invert(pc.is_null(tag_utf8)), pc.greater(pc.utf8_length(tag_utf8), 0))
    tbl = tbl.filter(mask)

    # Derive group
    groups = pa.array([infer_group(t or "") for t in tag_utf8.to_pylist()], type=pa.utf8())
    tbl = tbl.append_column("group", groups)

    # Consistent order
    keep = [c for c in ["cik","tag","unit","fy","fp","end","filed","form","accn","val","group"] if c in tbl.column_names]
    return tbl.select(keep)

# 3) Tiny dry-run to SAMPLE output (first batch only)
print("\n--- Dry-run: writing 1 batch to facts_by_group_v4_sample ---")
shutil.rmtree(OUT_V4_SAMPLE, ignore_errors=True)
OUT_V4_SAMPLE.mkdir(parents=True, exist_ok=True)

tbl_sample = normalize_and_partition(tbl0)
# write one batch
pq.write_to_dataset(
    table=tbl_sample,
    root_path=str(OUT_V4_SAMPLE),
    partition_cols=["group","tag"],
    compression="snappy",
    existing_data_behavior="overwrite_or_ignore",
)
print("Sample write complete at:", OUT_V4_SAMPLE)
# quick peek at partitions created
created = []
for root, dirs, files in os.walk(OUT_V4_SAMPLE):
    for f in files:
        if f.endswith(".parquet"):
            created.append(os.path.join(root, f))
            if len(created) >= 3:
                break
    if len(created) >= 3:
        break
print("Sample files (up to 3):", created[:3])

# 4) Full run with VERBOSE progress
print("\n--- Full fan-out to facts_by_group_v4 (verbose) ---")
shutil.rmtree(OUT_V4, ignore_errors=True)
OUT_V4.mkdir(parents=True, exist_ok=True)

t0 = time.time()
rows_total = 0
batch_idx = 0

for batch in ds.dataset(str(RAW_DIR), format="parquet").to_batches(batch_size=BATCH_SIZE):
    batch_idx += 1
    tbl = pa.Table.from_batches([batch])
    tbl = normalize_and_partition(tbl)
    rows_total += tbl.num_rows

    pq.write_to_dataset(
        table=tbl,
        root_path=str(OUT_V4),
        partition_cols=["group","tag"],
        compression="snappy",
        existing_data_behavior="overwrite_or_ignore",
    )

    print(f"[v4] wrote batch {batch_idx:,}  rows={tbl.num_rows:,}  total={rows_total:,}")

elapsed = time.time() - t0
print(f"\n✅ DONE: facts_by_group_v4 at {OUT_V4}")
print(f"   Batches: {batch_idx:,}   Rows written: {rows_total:,}   Time: {elapsed:.1f}s")
# Preview valid Parquet rows from facts_by_group_v4_sample
import os, glob
import pyarrow as pa
import pyarrow.dataset as ds
import pyarrow.parquet as pq

SAMPLE_DIR = r"C:\smilefund_project\warehouse\parquet\facts_by_group_v4_sample"

def is_real_parquet(path: str) -> bool:
    try:
        if not os.path.isfile(path) or os.path.getsize(path) < 16:
            return False
        with open(path, "rb") as f:
            f.seek(-4, os.SEEK_END)
            return f.read(4) == b"PAR1"
    except Exception:
        return False

print("Sample dir exists:", os.path.exists(SAMPLE_DIR))
candidates = glob.glob(os.path.join(SAMPLE_DIR, "**", "*.parquet"), recursive=True)
valid_files = [p for p in candidates if is_real_parquet(p)]
print("Valid parquet files:", len(valid_files))
print("First 3 valid files:", valid_files[:3])

if not valid_files:
    raise SystemExit("No valid parquet files detected in sample.")

sample_ds = ds.dataset(valid_files, format="parquet")
print("Columns:", sample_ds.schema.names)

batches = sample_ds.to_batches(batch_size=5)
first_batch = next(batches)
tbl = pa.Table.from_batches([first_batch])  # <-- correct conversion
print("First 5 rows:\n", tbl.to_pandas())
# Delete invalid/corrupted parquet files in facts_by_group_v4 (skip if already gone)
import os
from pathlib import Path
import pyarrow.parquet as pq

V4_DIR = Path(r"C:\smilefund_project\warehouse\parquet\facts_by_group_v4")
deleted, kept, missing = 0, 0, 0

def is_valid_parquet(p: Path) -> bool:
    try:
        if not p.is_file(): 
            return False
        if p.stat().st_size < 16:
            return False
        with p.open("rb") as f:
            f.seek(-4, os.SEEK_END)
            if f.read(4) != b"PAR1":
                return False
        # deep check
        _ = pq.ParquetFile(str(p))
        return True
    except Exception:
        return False

for root, _, files in os.walk(V4_DIR):
    for fn in files:
        if not fn.lower().endswith(".parquet"):
            continue
        p = Path(root) / fn
        try:
            if is_valid_parquet(p):
                kept += 1
            else:
                try:
                    os.remove(str(p))
                    deleted += 1
                except FileNotFoundError:
                    missing += 1  # already gone — fine
        except FileNotFoundError:
            missing += 1

print(f"✅ Clean complete. Kept: {kept:,}  Deleted: {deleted:,}  Missing (already gone): {missing:,}")

# Optional: prune empty directories
empties = 0
for root, dirs, files in os.walk(V4_DIR, topdown=False):
    if not dirs and not files:
        try:
            Path(root).rmdir()
            empties += 1
        except OSError:
            pass
print(f"Pruned empty dirs: {empties}")
import os, duckdb

V4   = r"C:\smilefund_project\warehouse\parquet\facts_by_group_v4\**\*.parquet"
SNAP = r"C:\smilefund_project\warehouse\parquet\snapshots"
os.makedirs(SNAP, exist_ok=True)

con = duckdb.connect()

# Normalize tag & pull hive partitions
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW facts_min AS
  SELECT
    CASE WHEN position(':' IN tag) > 0 THEN split_part(tag,':',2) ELSE tag END AS tag,
    "group", cik, fy, fp, val
  FROM parquet_scan('{V4}', HIVE_PARTITIONING=1)
  WHERE val IS NOT NULL
    AND tag IS NOT NULL AND tag <> '' AND tag <> '__HIVE_DEFAULT_PARTITION__'
""")

# Rank (FY > Q4 > Q3 > Q2 > Q1)
con.execute("""
  CREATE OR REPLACE TEMP VIEW facts_ranked AS
  SELECT *,
    CASE fp WHEN 'FY' THEN 5 WHEN 'Q4' THEN 4 WHEN 'Q3' THEN 3 WHEN 'Q2' THEN 2 WHEN 'Q1' THEN 1 ELSE 0 END AS fp_rank
  FROM facts_min
""")

con.execute("""
  CREATE OR REPLACE TABLE latest_any AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag ORDER BY fy DESC, fp_rank DESC) rn
    FROM facts_ranked
  ) WHERE rn=1
""")
con.execute(f"COPY latest_any TO '{SNAP}\\latest_any.parquet' (FORMAT PARQUET);")

con.execute("""
  CREATE OR REPLACE TABLE latest_annual AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag, fy ORDER BY fp_rank DESC) rn
    FROM facts_ranked
    WHERE fp='FY'
  ) WHERE rn=1
""")
con.execute(f"COPY latest_annual TO '{SNAP}\\latest_annual.parquet' (FORMAT PARQUET);")

con.execute("""
  CREATE OR REPLACE TABLE latest_quarterly AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag, fy, fp ORDER BY fp_rank DESC) rn
    FROM facts_ranked
    WHERE fp IN ('Q1','Q2','Q3','Q4')
  ) WHERE rn=1
""")
con.execute(f"COPY latest_quarterly TO '{SNAP}\\latest_quarterly.parquet' (FORMAT PARQUET);")

print("Apple rows in latest_annual:",
      con.execute(f"SELECT COUNT(*) FROM read_parquet('{SNAP}\\latest_annual.parquet') WHERE cik=320193").fetchone()[0])
print(con.execute(f"""
  SELECT tag, COUNT(*) n
  FROM read_parquet('{SNAP}\\latest_annual.parquet')
  WHERE cik=320193
  GROUP BY tag ORDER BY n DESC LIMIT 10
""").fetch_df())

con.close()
print("✅ snapshots rebuilt from cleaned v4")
# Diagnose: is Apple (CIK 320193) in RAW? If not, show which CIKs/tags ARE present.
import duckdb, os, pandas as pd

RAW   = r"C:\smilefund_project\warehouse\parquet\_raw_deltas\**\*.parquet"
V4    = r"C:\smilefund_project\warehouse\parquet\facts_by_group_v4\**\*.parquet"
SNAPA = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
SNAPQ = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_quarterly.parquet"

con = duckdb.connect()

print("== Check Apple in RAW ==")
apple_raw_count = con.execute(f"""
  SELECT COUNT(*) 
  FROM parquet_scan('{RAW}')
  WHERE TRY_CAST(cik AS BIGINT) = 320193
""").fetchone()[0]
print("RAW rows for CIK 320193:", apple_raw_count)

if apple_raw_count > 0:
    print("\nSample Apple rows (RAW):")
    print(con.execute(f"""
      SELECT cik, xbrl_tag, fy, fp, value
      FROM parquet_scan('{RAW}')
      WHERE TRY_CAST(cik AS BIGINT)=320193
      ORDER BY fy NULLS LAST, fp NULLS LAST
      LIMIT 10
    """).fetch_df())
else:
    print("\nApple not found in RAW. Listing a few CIKs that DO exist in RAW:")
    print(con.execute(f"""
      SELECT TRY_CAST(cik AS BIGINT) AS cik, COUNT(*) n
      FROM parquet_scan('{RAW}')
      GROUP BY 1
      ORDER BY n DESC
      LIMIT 10
    """).fetch_df())

print("\n== Quick look at v4 contents (post-fanout) ==")
print(con.execute(f"""
  SELECT COUNT(*) AS rows_v4 FROM parquet_scan('{V4}', HIVE_PARTITIONING=1)
""").fetch_df())

print("\nTop tags present in v4 (sample):")
print(con.execute(f"""
  SELECT tag, COUNT(*) n
  FROM parquet_scan('{V4}', HIVE_PARTITIONING=1)
  WHERE tag IS NOT NULL AND tag <> '' AND tag <> '__HIVE_DEFAULT_PARTITION__'
  GROUP BY tag
  ORDER BY n DESC
  LIMIT 20
""").fetch_df())

print("\nTop CIKs present in latest_annual snapshot:")
print(con.execute(f"""
  SELECT cik, COUNT(*) n
  FROM read_parquet('{SNAPA}')
  GROUP BY cik
  ORDER BY n DESC
  LIMIT 10
""").fetch_df())

con.close()
# Clean re-fan-out: RAW -> facts_by_group_v4 (progress prints), then verify Apple in v4
from pathlib import Path
import os, re, time, shutil, uuid
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds
import pyarrow.parquet as pq
import duckdb

PROJECT_ROOT = Path(r"C:\smilefund_project")
RAW_DIR      = PROJECT_ROOT / r"warehouse\parquet\_raw_deltas"
OUT_V4       = PROJECT_ROOT / r"warehouse\parquet\facts_by_group_v4"

# 0) Fresh start
shutil.rmtree(OUT_V4, ignore_errors=True)
OUT_V4.mkdir(parents=True, exist_ok=True)

# 1) Helpers
TAG_GROUPS = {
    "Revenue": {"Revenues","SalesRevenueNet","RevenueFromContractWithCustomerExcludingAssessedTax",
                "SalesRevenueGoodsAndServicesNet","SalesRevenueGoodsNet","SalesRevenueServicesNet",
                "TotalRevenueAndOtherIncome"},
    "Expenses": {"CostOfGoodsAndServicesSold","CostOfRevenue",
                 "SellingGeneralAndAdministrativeExpense","ResearchAndDevelopmentExpense",
                 "OperatingExpenses","InterestExpense","RestructuringCharges"},
    "Assets": {"Assets","AssetsCurrent","CashAndCashEquivalentsAtCarryingValue",
               "AccountsReceivableNetCurrent","InventoryNet","Goodwill",
               "IntangibleAssetsNetExcludingGoodwill","PropertyPlantAndEquipmentNet"},
    "Liabilities": {"Liabilities","LiabilitiesCurrent","AccountsPayableCurrent",
                    "LongTermDebtNoncurrent","DeferredRevenueCurrent","DeferredTaxLiabilitiesNoncurrent"},
    "Equity": {"StockholdersEquity","RetainedEarningsAccumulatedDeficit","CommonStockValue",
               "AdditionalPaidInCapital","TreasuryStockValue"},
    "CashFlow": {"NetCashProvidedByUsedInOperatingActivities","NetCashProvidedByUsedInInvestingActivities",
                 "NetCashProvidedByUsedInFinancingActivities","PaymentsForRepurchaseOfCommonStock","PaymentsOfDividends"},
    "OCI": {"OtherComprehensiveIncomeLossNetOfTax"},
    "PerShare": {"EarningsPerShareBasic","EarningsPerShareDiluted","CommonStockDividendsPerShareDeclared"},
    "KPIs": {"CustomersNumber","EmployeesNumber","ActiveUsers","RevenuePerShare"},
}
BASE = {t.lower(): g for g, tags in TAG_GROUPS.items() for t in tags}
compiled = [
    ("Revenue", re.compile(r"(revenue|sales)(?!per)", re.I)),
    ("Expenses", re.compile(r"(expense|costof|costs?|cogs)", re.I)),
    ("Assets", re.compile(r"(^assets?$|asset)", re.I)),
    ("Liabilities", re.compile(r"(liabilit)", re.I)),
    ("Equity", re.compile(r"(equity|retainedearnings|paidincapital|treasurystock)", re.I)),
    ("CashFlow", re.compile(r"(netcash|operatingactivit|investingactivit|financingactivit|dividend|repurchase)", re.I)),
    ("OCI", re.compile(r"(othercomprehensiveincome)", re.I)),
    ("PerShare", re.compile(r"(earningspershare|dividendsPerShare|perShare)", re.I)),
    ("Revenue", re.compile(r"(totalrevenueandotherincome)", re.I)),
]
def infer_group(tag: str) -> str:
    if not tag: return "Other"
    base = tag.split(":", 1)[-1].lower()
    if base in BASE: return BASE[base]
    for g, rx in compiled:
        if rx.search(base): return g
    return "Other"

def ensure(tbl: pa.Table, name: str, typ: pa.DataType) -> pa.Table:
    if name not in tbl.column_names:
        tbl = tbl.append_column(name, pa.array([None]*tbl.num_rows, type=typ))
    return tbl

def cast(tbl: pa.Table, name: str, typ: pa.DataType) -> pa.Table:
    if name in tbl.column_names:
        try:
            tbl = tbl.set_column(tbl.schema.get_field_index(name), name, pc.cast(tbl[name], typ))
        except Exception:
            pass
    return tbl

# 2) Stream RAW -> write v4 with progress
ds_raw = ds.dataset(str(RAW_DIR), format="parquet")
BATCH = 250_000
total_rows = 0
t0 = time.time()

for i, batch in enumerate(ds_raw.to_batches(batch_size=BATCH), start=1):
    tbl = pa.Table.from_batches([batch])

    # normalize columns
    for nm, tp in [("cik", pa.int64()), ("xbrl_tag", pa.utf8()), ("value", pa.float64()),
                   ("unit_code", pa.utf8()), ("fy", pa.int32()), ("fp", pa.utf8()),
                   ("period_end", pa.utf8()), ("filed_date", pa.utf8()),
                   ("form_type", pa.utf8()), ("accession", pa.utf8())]:
        tbl = ensure(tbl, nm, tp); tbl = cast(tbl, nm, tp)

    # rename to canonical
    ren = {"xbrl_tag":"tag","value":"val","unit_code":"unit","period_end":"end",
           "filed_date":"filed","form_type":"form","accession":"accn"}
    for s,d in ren.items():
        if s in tbl.column_names:
            col = tbl[s]
            if d in tbl.column_names:
                tbl = tbl.set_column(tbl.schema.get_field_index(d), d, col)
            else:
                tbl = tbl.append_column(d, col)

    # drop null/empty tags
    tag = pc.cast(tbl["tag"], pa.utf8())
    tbl = tbl.set_column(tbl.schema.get_field_index("tag"), "tag", tag)
    mask = pc.and_(pc.invert(pc.is_null(tag)), pc.greater(pc.utf8_length(tag), 0))
    tbl = tbl.filter(mask)

    # derive group
    groups = pa.array([infer_group(t or "") for t in tag.to_pylist()], type=pa.utf8())
    tbl = tbl.append_column("group", groups)

    # order columns
    keep = [c for c in ["cik","tag","unit","fy","fp","end","filed","form","accn","val","group"] if c in tbl.column_names]
    tbl = tbl.select(keep)

    # write hive partitions
    pq.write_to_dataset(
        table=tbl,
        root_path=str(OUT_V4),
        partition_cols=["group","tag"],
        compression="snappy",
        existing_data_behavior="overwrite_or_ignore",
        basename_template=f"{uuid.uuid4().hex}-{{i}}.parquet"  # reduce chance of name collisions
    )

    total_rows += tbl.num_rows
    print(f"[v4] batch {i} rows={tbl.num_rows:,} total={total_rows:,}", flush=True)

elapsed = time.time() - t0
print(f"✅ v4 complete at {OUT_V4} • rows written: {total_rows:,} • {elapsed:.1f}s", flush=True)

# 3) Immediate Apple presence check in v4
con = duckdb.connect()
count_apple_v4 = con.execute(f"""
  SELECT COUNT(*) FROM parquet_scan('{str(OUT_V4)}\\**\\*.parquet', HIVE_PARTITIONING=1)
  WHERE TRY_CAST(cik AS BIGINT)=320193
""").fetchone()[0]
print("Apple rows present in v4:", count_apple_v4)
# show a couple tags seen for Apple in v4
if count_apple_v4:
    print(con.execute(f"""
      SELECT tag, COUNT(*) n
      FROM parquet_scan('{str(OUT_V4)}\\**\\*.parquet', HIVE_PARTITIONING=1)
      WHERE TRY_CAST(cik AS BIGINT)=320193 AND tag IS NOT NULL AND tag <> ''
      GROUP BY tag ORDER BY n DESC LIMIT 10
    """).fetch_df())
con.close()
# Build a per-CIK Revenue tag choice (alias) from RAW
import os, duckdb

RAW = r"C:\smilefund_project\warehouse\parquet\_raw_deltas\**\*.parquet"
OUT = r"C:\smilefund_project\warehouse\parquet\dimensions\dim_revenue_tag_choice.parquet"
os.makedirs(os.path.dirname(OUT), exist_ok=True)

con = duckdb.connect()

# Normalize RAW once
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW raw_norm AS
  SELECT
    TRY_CAST(cik AS BIGINT) AS cik,
    CASE WHEN position(':' IN xbrl_tag) > 0 THEN split_part(xbrl_tag,':',2) ELSE xbrl_tag END AS tag,
    TRY_CAST(fy AS INTEGER) AS fy,
    CAST(fp AS VARCHAR)     AS fp,
    TRY_CAST(value AS DOUBLE) AS val
  FROM parquet_scan('{RAW}')
  WHERE value IS NOT NULL AND xbrl_tag IS NOT NULL AND xbrl_tag <> ''
""")

# Candidate revenue tags (ordered by preference)
CANDS = (
    'Revenues',
    'SalesRevenueNet',
    'RevenueFromContractWithCustomerExcludingAssessedTax',
    'SalesRevenueGoodsAndServicesNet',
    'SalesRevenueGoodsNet',
    'SalesRevenueServicesNet',
    'TotalRevenueAndOtherIncome'
)
cand_sql = ",".join([f"'{t}'" for t in CANDS])

# Count which candidates each CIK actually reports, prefer by our priority then by volume
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW revenue_candidates AS
  SELECT
    cik, tag, COUNT(*) AS n,
    CASE tag
      WHEN 'Revenues' THEN 1
      WHEN 'SalesRevenueNet' THEN 2
      WHEN 'RevenueFromContractWithCustomerExcludingAssessedTax' THEN 3
      WHEN 'SalesRevenueGoodsAndServicesNet' THEN 4
      WHEN 'SalesRevenueGoodsNet' THEN 5
      WHEN 'SalesRevenueServicesNet' THEN 6
      WHEN 'TotalRevenueAndOtherIncome' THEN 7
      ELSE 99
    END AS pref
  FROM raw_norm
  WHERE tag IN ({cand_sql})
  GROUP BY cik, tag
""")

# One chosen tag per CIK
con.execute("""
  CREATE OR REPLACE TABLE dim_revenue_tag_choice AS
  SELECT cik, tag AS chosen_revenue_tag
  FROM (
    SELECT
      cik, tag, n, pref,
      ROW_NUMBER() OVER (PARTITION BY cik ORDER BY pref ASC, n DESC) AS rn
    FROM revenue_candidates
  )
  WHERE rn = 1
""")

# Persist to Parquet
con.execute(f"COPY dim_revenue_tag_choice TO '{OUT}' (FORMAT PARQUET)")
preview = con.execute("SELECT * FROM dim_revenue_tag_choice ORDER BY cik LIMIT 10").fetch_df()

# Quick proof for Apple
apple = con.execute("""
  SELECT * FROM dim_revenue_tag_choice WHERE cik = 320193
""").fetch_df()

con.close()
print("✅ Wrote:", OUT)
print("Preview:\n", preview)
print("\nApple choice:\n", apple)
# Universal Revenue mart (annual + quarterly) using per-CIK chosen tag
import os, duckdb

RAW      = r"C:\smilefund_project\warehouse\parquet\_raw_deltas\**\*.parquet"
DIM_REV  = r"C:\smilefund_project\warehouse\parquet\dimensions\dim_revenue_tag_choice.parquet"
OUT_DIR  = r"C:\smilefund_project\warehouse\parquet\marts"
REV_ANNU = os.path.join(OUT_DIR, "revenue_annual.parquet")
REV_QTR  = os.path.join(OUT_DIR, "revenue_quarterly.parquet")
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

# Normalize RAW and join to chosen revenue tag per CIK
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW raw_norm AS
  SELECT
    TRY_CAST(cik AS BIGINT) AS cik,
    CASE WHEN position(':' IN xbrl_tag) > 0 THEN split_part(xbrl_tag,':',2) ELSE xbrl_tag END AS tag,
    TRY_CAST(fy AS INTEGER) AS fy,
    CAST(fp AS VARCHAR)     AS fp,
    TRY_CAST(value AS DOUBLE) AS val
  FROM parquet_scan('{RAW}')
  WHERE value IS NOT NULL AND xbrl_tag IS NOT NULL AND xbrl_tag <> ''
""")

con.execute(f"CREATE OR REPLACE TEMP VIEW dim_choice AS SELECT * FROM read_parquet('{DIM_REV}')")

# Restrict RAW to the chosen tag for each CIK
con.execute("""
  CREATE OR REPLACE TEMP VIEW rev_chosen AS
  SELECT r.cik, r.fy, r.fp, r.val
  FROM raw_norm r
  JOIN dim_choice d
    ON r.cik = d.cik AND r.tag = d.chosen_revenue_tag
""")

# Helper to rank FY/Qs
con.execute("""
  CREATE OR REPLACE TEMP VIEW rev_ranked AS
  SELECT *,
         CASE fp WHEN 'FY' THEN 5 WHEN 'Q4' THEN 4 WHEN 'Q3' THEN 3 WHEN 'Q2' THEN 2 WHEN 'Q1' THEN 1 ELSE 0 END AS fp_rank
  FROM rev_chosen
""")

# Annual mart (one row per cik, fy)
con.execute(f"""
  COPY (
    WITH z AS (
      SELECT cik, fy, val,
             ROW_NUMBER() OVER (PARTITION BY cik, fy ORDER BY fp_rank DESC) rn
      FROM rev_ranked
      WHERE fp = 'FY'
    )
    SELECT cik, fy, val AS Revenue
    FROM z WHERE rn = 1
    ORDER BY cik, fy
  ) TO '{REV_ANNU}' (FORMAT PARQUET)
""")

# Quarterly mart (one row per cik, fy, fp)
con.execute(f"""
  COPY (
    WITH z AS (
      SELECT cik, fy, fp, val,
             ROW_NUMBER() OVER (PARTITION BY cik, fy, fp ORDER BY fp_rank DESC) rn
      FROM rev_ranked
      WHERE fp IN ('Q1','Q2','Q3','Q4')
    )
    SELECT cik, fy, fp, val AS Revenue
    FROM z WHERE rn = 1
    ORDER BY cik, fy,
      CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE 0 END
  ) TO '{REV_QTR}' (FORMAT PARQUET)
""")

# Quick Apple sanity (CIK 320193)
apple_annual   = con.execute(f"SELECT * FROM read_parquet('{REV_ANNU}') WHERE cik=320193 ORDER BY fy").fetch_df()
apple_quarter  = con.execute(f"SELECT * FROM read_parquet('{REV_QTR}') WHERE cik=320193 ORDER BY fy, CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 END").fetch_df()
apple_choice   = con.execute("SELECT * FROM dim_choice WHERE cik=320193").fetch_df()

con.close()

print("✅ Wrote:")
print(" -", REV_ANNU)
print(" -", REV_QTR)
print("\nApple chosen tag:\n", apple_choice)
print("\nApple annual (tail):\n", apple_annual.tail(10))
print("\nApple quarterly (tail):\n", apple_quarter.tail(12))
# Rebuild dim_revenue_tag_choice using FY coverage → rebuild revenue marts → re-check Apple
import os, duckdb

RAW      = r"C:\smilefund_project\warehouse\parquet\_raw_deltas\**\*.parquet"
OUT_DIM  = r"C:\smilefund_project\warehouse\parquet\dimensions\dim_revenue_tag_choice.parquet"
OUT_DIR  = r"C:\smilefund_project\warehouse\parquet\marts"
REV_ANNU = os.path.join(OUT_DIR, "revenue_annual.parquet")
REV_QTR  = os.path.join(OUT_DIR, "revenue_quarterly.parquet")
os.makedirs(os.path.dirname(OUT_DIM), exist_ok=True)
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

# 1) Normalize RAW once
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW raw_norm AS
  SELECT
    TRY_CAST(cik AS BIGINT) AS cik,
    CASE WHEN position(':' IN xbrl_tag) > 0 THEN split_part(xbrl_tag,':',2) ELSE xbrl_tag END AS tag,
    TRY_CAST(fy AS INTEGER) AS fy,
    CAST(fp AS VARCHAR)     AS fp,
    TRY_CAST(value AS DOUBLE) AS val
  FROM parquet_scan('{RAW}')
  WHERE value IS NOT NULL AND xbrl_tag IS NOT NULL AND xbrl_tag <> ''
""")

# 2) Candidate revenue tags (ordered by preference)
CANDS = (
  'Revenues',
  'SalesRevenueNet',
  'RevenueFromContractWithCustomerExcludingAssessedTax',
  'SalesRevenueGoodsAndServicesNet',
  'SalesRevenueGoodsNet',
  'SalesRevenueServicesNet',
  'TotalRevenueAndOtherIncome'
)
cand_sql = ",".join([f"'{t}'" for t in CANDS])

# 3) Score tags per CIK:
#    - fy_coverage: how many DISTINCT fiscal years have an FY row for this tag
#    - pref: our human preference order
#    - total_rows: total rows (ties breaker)
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW rev_scores AS
  SELECT
    cik,
    tag,
    COUNT(*) FILTER (WHERE fp='FY')                         AS fy_rows,
    COUNT(DISTINCT CASE WHEN fp='FY' THEN fy END)           AS fy_coverage,
    COUNT(*)                                                AS total_rows,
    MIN(CASE tag
          WHEN 'Revenues' THEN 1
          WHEN 'SalesRevenueNet' THEN 2
          WHEN 'RevenueFromContractWithCustomerExcludingAssessedTax' THEN 3
          WHEN 'SalesRevenueGoodsAndServicesNet' THEN 4
          WHEN 'SalesRevenueGoodsNet' THEN 5
          WHEN 'SalesRevenueServicesNet' THEN 6
          WHEN 'TotalRevenueAndOtherIncome' THEN 7
          ELSE 99 END)                                      AS pref
  FROM raw_norm
  WHERE tag IN ({cand_sql})
  GROUP BY cik, tag
""")

# 4) Choose best tag per CIK:
#    First by fy_coverage (DESC), then by pref (ASC), then by total_rows (DESC)
con.execute("""
  CREATE OR REPLACE TABLE dim_revenue_tag_choice AS
  SELECT cik, tag AS chosen_revenue_tag
  FROM (
    SELECT
      cik, tag, fy_coverage, pref, total_rows,
      ROW_NUMBER() OVER (
        PARTITION BY cik
        ORDER BY fy_coverage DESC, pref ASC, total_rows DESC
      ) AS rn
    FROM rev_scores
  )
  WHERE rn = 1
""")
con.execute(f"COPY dim_revenue_tag_choice TO '{OUT_DIM}' (FORMAT PARQUET)")

# 5) Build marts from the chosen tag
con.execute(f"CREATE OR REPLACE TEMP VIEW dim_choice AS SELECT * FROM read_parquet('{OUT_DIM}')")
con.execute("""
  CREATE OR REPLACE TEMP VIEW rev_chosen AS
  SELECT r.cik, r.fy, r.fp, r.val
  FROM raw_norm r
  JOIN dim_choice d ON r.cik = d.cik AND r.tag = d.chosen_revenue_tag
""")
con.execute("""
  CREATE OR REPLACE TEMP VIEW rev_ranked AS
  SELECT *,
         CASE fp WHEN 'FY' THEN 5 WHEN 'Q4' THEN 4 WHEN 'Q3' THEN 3 WHEN 'Q2' THEN 2 WHEN 'Q1' THEN 1 ELSE 0 END AS fp_rank
  FROM rev_chosen
""")

# Annual mart
con.execute(f"""
  COPY (
    WITH z AS (
      SELECT cik, fy, val,
             ROW_NUMBER() OVER (PARTITION BY cik, fy ORDER BY fp_rank DESC) rn
      FROM rev_ranked
      WHERE fp='FY'
    )
    SELECT cik, fy, val AS Revenue
    FROM z WHERE rn=1
    ORDER BY cik, fy
  ) TO '{REV_ANNU}' (FORMAT PARQUET)
""")

# Quarterly mart
con.execute(f"""
  COPY (
    WITH z AS (
      SELECT cik, fy, fp, val,
             ROW_NUMBER() OVER (PARTITION BY cik, fy, fp ORDER BY fp_rank DESC) rn
      FROM rev_ranked
      WHERE fp IN ('Q1','Q2','Q3','Q4')
    )
    SELECT cik, fy, fp, val AS Revenue
    FROM z WHERE rn=1
    ORDER BY cik, fy,
      CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 END
  ) TO '{REV_QTR}' (FORMAT PARQUET)
""")

# 6) Apple sanity check
apple_choice = con.execute("SELECT * FROM dim_revenue_tag_choice WHERE cik=320193").fetch_df()
apple_annual = con.execute(f"SELECT * FROM read_parquet('{REV_ANNU}') WHERE cik=320193 ORDER BY fy").fetch_df()
apple_quarter= con.execute(f"""
  SELECT * FROM read_parquet('{REV_QTR}') WHERE cik=320193
  ORDER BY fy, CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 END
""").fetch_df()
con.close()

print("✅ Rewrote:", OUT_DIM)
print("Apple chosen tag now:\n", apple_choice)
print("\nApple annual (tail):\n", apple_annual.tail(10))
print("\nApple quarterly (tail):\n", apple_quarter.tail(12))
# Build a view that returns Q1, Q2, Q3, and the FY delta (10-K minus Q1+Q2+Q3) for every cik/fy.
import os, duckdb, pandas as pd

REV_QTR = r"C:\smilefund_project\warehouse\parquet\marts\revenue_quarterly.parquet"
REV_ANNU= r"C:\smilefund_project\warehouse\parquet\marts\revenue_annual.parquet"
OUT_DIR = r"C:\smilefund_project\warehouse\reports"
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

# Create/replace a view you can reuse in any query
con.execute(f"""
CREATE OR REPLACE VIEW vw_revenue_q13_plus_fydelta AS
WITH q AS (
  SELECT
    cik,
    fy,
    fp,
    Revenue AS amount,
    CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE NULL END AS qn
  FROM read_parquet('{REV_QTR}')
  WHERE fp IN ('Q1','Q2','Q3')  -- only first three quarters
),
ytd3 AS (
  SELECT cik, fy, SUM(amount) AS ytd3
  FROM q
  GROUP BY cik, fy
),
fy AS (
  SELECT cik, fy, Revenue AS fy_amount
  FROM read_parquet('{REV_ANNU}')
),
delta AS (
  -- FY delta = 10-K FY - (Q1+Q2+Q3); if a quarter is missing, the delta naturally reflects that
  SELECT
    fy.cik, fy.fy,
    (fy.fy_amount - COALESCE(ytd3.ytd3, 0.0)) AS amount
  FROM fy
  LEFT JOIN ytd3 ON ytd3.cik = fy.cik AND ytd3.fy = fy.fy
),
rows_q AS (
  SELECT cik, fy, fp AS label, amount, qn AS order_in_fy, 'reported' AS source
  FROM q
),
rows_delta AS (
  SELECT cik, fy, '10K delta' AS label, amount, 4 AS order_in_fy, 'computed' AS source
  FROM delta
)
SELECT *
FROM (
  SELECT * FROM rows_q
  UNION ALL
  SELECT * FROM rows_delta
)
WHERE amount IS NOT NULL
-- optional: drop zero/negative deltas if you don't want them
-- AND amount > 0
;
""")

# EXAMPLE: export Apple (CIK 320193) in the desired order
CIK = 320193
df = con.execute(f"""
  SELECT cik, fy, label, amount, order_in_fy
  FROM vw_revenue_q13_plus_fydelta
  WHERE cik = {CIK}
  ORDER BY fy, order_in_fy
""").fetch_df()

csv_path = os.path.join(OUT_DIR, "AAPL_revenue_Q1_Q2_Q3_FYdelta.csv")
df.to_csv(csv_path, index=False)
con.close()

print(f"✅ Wrote {csv_path} ({len(df)} rows)")
print(df.tail(12))
import duckdb, os, pandas as pd

REV_QTR = r"C:\smilefund_project\warehouse\parquet\marts\revenue_quarterly.parquet"
REV_ANNU= r"C:\smilefund_project\warehouse\parquet\marts\revenue_annual.parquet"
OUT_DIR = r"C:\smilefund_project\warehouse\reports"
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

# Build the de-YTD logic as a view
con.execute(f"""
CREATE OR REPLACE VIEW vw_revenue_quarters_true AS
WITH q_raw AS (
  SELECT
    cik, fy, fp,
    Revenue AS ytd_amount,
    CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 END AS qn
  FROM read_parquet('{REV_QTR}')
  WHERE fp IN ('Q1','Q2','Q3')
),
q_delta AS (
  SELECT q1.cik, q1.fy, q1.fp, q1.ytd_amount AS amount, q1.qn
  FROM q_raw q1 WHERE q1.fp='Q1'
  UNION ALL
  SELECT q2.cik, q2.fy, 'Q2' AS fp, (q2.ytd_amount - q1.ytd_amount) AS amount, 2 AS qn
  FROM q_raw q2
  JOIN q_raw q1 ON q1.cik=q2.cik AND q1.fy=q2.fy AND q1.fp='Q1'
  UNION ALL
  SELECT q3.cik, q3.fy, 'Q3' AS fp, (q3.ytd_amount - q2.ytd_amount) AS amount, 3 AS qn
  FROM q_raw q3
  JOIN q_raw q2 ON q2.cik=q3.cik AND q2.fy=q3.fy AND q2.fp='Q2'
),
fy AS (
  SELECT cik, fy, Revenue AS fy_amount
  FROM read_parquet('{REV_ANNU}')
),
ytd3 AS (
  SELECT cik, fy, SUM(amount) AS total_3q
  FROM q_delta
  GROUP BY cik, fy
),
fy_delta AS (
  SELECT f.cik, f.fy, (f.fy_amount - COALESCE(y.total_3q,0)) AS amount, 4 AS qn, '10K delta' AS fp
  FROM fy f LEFT JOIN ytd3 y ON f.cik=y.cik AND f.fy=y.fy
)
SELECT *
FROM (
  SELECT * FROM q_delta
  UNION ALL
  SELECT * FROM fy_delta
)
WHERE amount IS NOT NULL
ORDER BY cik, fy, qn
""")

# Example export: Apple (CIK 320193)
CIK = 320193
df = con.execute(f"""
  SELECT cik, fy, fp, amount AS Revenue
  FROM vw_revenue_quarters_true
  WHERE cik = {CIK}
  ORDER BY fy, CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN '10K delta' THEN 4 END
""").fetch_df()

csv_path = os.path.join(OUT_DIR, "AAPL_revenue_quarters_true.csv")
df.to_csv(csv_path, index=False)
con.close()

print(f"✅ Wrote {csv_path} ({len(df)} rows)")
print(df.tail(12))
import duckdb, os, pandas as pd

REV_QTR = r"C:\smilefund_project\warehouse\parquet\marts\revenue_quarterly.parquet"
REV_ANNU= r"C:\smilefund_project\warehouse\parquet\marts\revenue_annual.parquet"
OUT_DIR = r"C:\smilefund_project\warehouse\reports"
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

con.execute(f"""
CREATE OR REPLACE VIEW vw_revenue_quarters_true AS
WITH q_raw AS (
  SELECT
    cik,
    fy,
    fp,
    Revenue AS ytd_amount,
    CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 END AS qn
  FROM read_parquet('{REV_QTR}')
  WHERE fp IN ('Q1','Q2','Q3')
),
q1 AS (
  SELECT cik, fy, 'Q1' AS fp, ytd_amount AS amount, 1 AS qn
  FROM q_raw
  WHERE fp = 'Q1'
),
q2 AS (
  SELECT q2.cik, q2.fy, 'Q2' AS fp,
         (q2.ytd_amount - COALESCE(q1.ytd_amount, 0)) AS amount,
         2 AS qn
  FROM q_raw q2
  LEFT JOIN q_raw q1
    ON q1.cik = q2.cik AND q1.fy = q2.fy AND q1.fp = 'Q1'
  WHERE q2.fp = 'Q2'
),
q3 AS (
  SELECT q3.cik, q3.fy, 'Q3' AS fp,
         (q3.ytd_amount - COALESCE(q2.ytd_amount, 0)) AS amount,
         3 AS qn
  FROM q_raw q3
  LEFT JOIN q_raw q2
    ON q2.cik = q3.cik AND q2.fy = q3.fy AND q2.fp = 'Q2'
  WHERE q3.fp = 'Q3'
),
rows_q AS (  -- ensure identical column order & names
  SELECT cik, fy, fp, amount, qn FROM q1
  UNION ALL
  SELECT cik, fy, fp, amount, qn FROM q2
  UNION ALL
  SELECT cik, fy, fp, amount, qn FROM q3
),
fy AS (
  SELECT cik, fy, Revenue AS fy_amount
  FROM read_parquet('{REV_ANNU}')
),
ytd3 AS (
  SELECT cik, fy, SUM(amount) AS total_3q
  FROM rows_q
  GROUP BY cik, fy
),
fy_delta AS (
  SELECT
    fy.cik,
    fy.fy,
    '10K delta' AS fp,                       -- place fp third
    (fy.fy_amount - COALESCE(ytd3.total_3q, 0)) AS amount,
    4 AS qn
  FROM fy
  LEFT JOIN ytd3 ON ytd3.cik = fy.cik AND ytd3.fy = fy.fy
)
SELECT *
FROM (
  SELECT cik, fy, fp, amount, qn FROM rows_q
  UNION ALL
  SELECT cik, fy, fp, amount, qn FROM fy_delta
)
WHERE amount IS NOT NULL
ORDER BY cik, fy, qn;
""")

# Example export: Apple
CIK = 320193
df = con.execute(f"""
  SELECT cik, fy, fp, amount AS Revenue
  FROM vw_revenue_quarters_true
  WHERE cik = {CIK}
  ORDER BY fy, CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN '10K delta' THEN 4 END
""").fetch_df()

csv_path = os.path.join(OUT_DIR, "AAPL_revenue_quarters_true.csv")
df.to_csv(csv_path, index=False)
con.close()

print(f"✅ Wrote {csv_path} ({len(df)} rows)")
print(df.tail(12))
import duckdb

RAW = r"C:\smilefund_project\warehouse\parquet\_raw_deltas\**\*.parquet"

con = duckdb.connect()
df = con.execute(f"""
  WITH raw_norm AS (
    SELECT
      TRY_CAST(cik AS BIGINT) AS cik,
      CASE WHEN position(':' IN xbrl_tag) > 0 THEN split_part(xbrl_tag,':',2) ELSE xbrl_tag END AS tag,
      TRY_CAST(fy AS INTEGER) AS fy,
      CAST(fp AS VARCHAR)     AS fp,
      TRY_CAST(value AS DOUBLE) AS val,
      CAST(period_end AS DATE) AS period_end,
      CAST(filed_date AS DATE) AS filed_date,
      form_type AS form
    FROM parquet_scan('{RAW}')
    WHERE TRY_CAST(cik AS BIGINT)=320193
      AND xbrl_tag IS NOT NULL AND xbrl_tag <> ''
      AND value IS NOT NULL
  )
  SELECT fy, fp, period_end, filed_date, form, val
  FROM raw_norm
  WHERE tag IN ('SalesRevenueNet','Revenues','RevenueFromContractWithCustomerExcludingAssessedTax')
    AND fp IN ('Q1','Q2','Q3')
  ORDER BY period_end, filed_date
""").fetch_df()
con.close()

print(df.tail(30))
# Reconnect and rebuild quarterly revenue mart with correct row selection
import duckdb, os

RAW      = r"C:\smilefund_project\warehouse\parquet\_raw_deltas\**\*.parquet"
DIM_REV  = r"C:\smilefund_project\warehouse\parquet\dimensions\dim_revenue_tag_choice.parquet"
OUT_DIR  = r"C:\smilefund_project\warehouse\parquet\marts"
REV_ANNU = os.path.join(OUT_DIR, "revenue_annual.parquet")
REV_QTR  = os.path.join(OUT_DIR, "revenue_quarterly.parquet")

os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

# Normalize + join to chosen revenue tag per CIK
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW raw_norm AS
  SELECT
    TRY_CAST(cik AS BIGINT) AS cik,
    CASE WHEN position(':' IN xbrl_tag) > 0 THEN split_part(xbrl_tag,':',2) ELSE xbrl_tag END AS tag,
    TRY_CAST(fy AS INTEGER) AS fy,
    CAST(fp AS VARCHAR)     AS fp,
    TRY_CAST(value AS DOUBLE) AS val,
    CAST(period_end AS DATE) AS period_end,
    CAST(filed_date AS DATE) AS filed_date,
    form_type AS form
  FROM parquet_scan('{RAW}')
  WHERE value IS NOT NULL AND xbrl_tag IS NOT NULL AND xbrl_tag <> ''
""")
con.execute(f"CREATE OR REPLACE TEMP VIEW dim_choice AS SELECT * FROM read_parquet('{DIM_REV}')")
con.execute("""
  CREATE OR REPLACE TEMP VIEW rev_chosen AS
  SELECT r.*
  FROM raw_norm r
  JOIN dim_choice d ON r.cik = d.cik AND r.tag = d.chosen_revenue_tag
""")

# Rebuild ANNUAL (keep for consistency; uses latest period_end/filed_date)
con.execute(f"""
  COPY (
    WITH ranked AS (
      SELECT *,
             ROW_NUMBER() OVER (
               PARTITION BY cik, fy
               ORDER BY period_end DESC NULLS LAST,
                        filed_date DESC NULLS LAST
             ) AS rn
      FROM rev_chosen
      WHERE fp = 'FY'
    )
    SELECT cik, fy, val AS Revenue
    FROM ranked WHERE rn = 1
    ORDER BY cik, fy
  ) TO '{REV_ANNU}' (FORMAT PARQUET)
""")

# Rebuild QUARTERLY: choose smaller val per (cik, fy, fp) to avoid YTD, then tie-break by dates, prefer 10-Q
con.execute(f"""
  COPY (
    WITH ranked AS (
      SELECT *,
             ROW_NUMBER() OVER (
               PARTITION BY cik, fy, fp
               ORDER BY val ASC,                                -- smaller value = true quarter
                        period_end DESC NULLS LAST,
                        filed_date DESC NULLS LAST,
                        CASE WHEN form IN ('10-Q','10-Q/A') THEN 1 ELSE 2 END ASC
             ) AS rn
      FROM rev_chosen
      WHERE fp IN ('Q1','Q2','Q3','Q4')
    )
    SELECT cik, fy, fp, val AS Revenue, period_end, filed_date, form
    FROM ranked WHERE rn = 1
    ORDER BY cik, fy,
      CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 END
  ) TO '{REV_QTR}' (FORMAT PARQUET)
""")

# Quick Apple check (Q1-Q3)
apple = con.execute(f"""
  SELECT fy, fp, Revenue, period_end
  FROM read_parquet('{REV_QTR}')
  WHERE cik=320193 AND fp IN ('Q1','Q2','Q3')
  ORDER BY fy, CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 END
""").fetch_df()

con.close()
print(apple.tail(9))
# Fix: quarterly mart is already stand-alone, so DON'T de-YTD again.
# Build Q1, Q2, Q3 passthrough + implied Q4 ("10K delta")
import duckdb, os

REV_QTR = r"C:\smilefund_project\warehouse\parquet\marts\revenue_quarterly.parquet"  # stand-alone quarters
REV_ANNU= r"C:\smilefund_project\warehouse\parquet\marts\revenue_annual.parquet"
OUT_DIR = r"C:\smilefund_project\warehouse\parquet\marts"
OUT_PAR = os.path.join(OUT_DIR, "revenue_quarters_true.parquet")
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

con.execute(f"""
COPY (
  WITH q AS (
    SELECT
      cik, fy, fp,
      Revenue AS amount,
      CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 END AS qn
    FROM read_parquet('{REV_QTR}')
    WHERE fp IN ('Q1','Q2','Q3')
  ),
  fy AS (
    SELECT cik, fy, Revenue AS fy_amount
    FROM read_parquet('{REV_ANNU}')
  ),
  sum_q13 AS (
    SELECT cik, fy, SUM(amount) AS q13_sum
    FROM q
    GROUP BY cik, fy
  ),
  q4_implied AS (
    SELECT
      f.cik, f.fy,
      '10K delta' AS fp,
      (f.fy_amount - COALESCE(s.q13_sum,0)) AS amount,
      4 AS qn
    FROM fy f
    LEFT JOIN sum_q13 s
      ON s.cik = f.cik AND s.fy = f.fy
  )
  SELECT cik, fy, fp, amount AS Revenue, qn
  FROM q
  UNION ALL
  SELECT cik, fy, fp, amount, qn FROM q4_implied
  ORDER BY cik, fy, qn
) TO '{OUT_PAR}' (FORMAT PARQUET);
""")

# Quick Apple sanity (CIK 320193)
apple = con.execute(f"""
  SELECT fy, fp, Revenue
  FROM read_parquet('{OUT_PAR}')
  WHERE cik=320193
  ORDER BY fy, CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN '10K delta' THEN 4 END
""").fetch_df()
con.close()

print("✅ Rewrote", OUT_PAR)
print(apple.tail(12))
import pandas as pd

df = pd.read_parquet(r"C:\smilefund_project\warehouse\parquet\marts\revenue_quarters_true.parquet")

mask_2017 = df["fy"] == 2017
print(df[mask_2017])
print("\nSum of Q1–Q3 2017:", df.loc[mask_2017 & df["fp"].isin(["Q1","Q2","Q3"]), "Revenue"].sum())
import pandas as pd

df = pd.read_parquet(r"C:\smilefund_project\warehouse\parquet\marts\revenue_quarters_true.parquet")

# Identify per-company per-year sets
grouped = df.groupby(["cik", "fy"])

def fix_deltas(sub):
    if "10K delta" not in sub["fp"].values:
        return sub
    delta_idx = sub.index[sub["fp"] == "10K delta"]
    delta_val = sub.loc[delta_idx, "Revenue"].iloc[0]

    # Flip if delta is negative but total quarters positive
    if delta_val < 0 and sub.loc[sub["fp"].isin(["Q1", "Q2", "Q3"]), "Revenue"].sum() > 0:
        sub.loc[delta_idx, "Revenue"] = abs(delta_val)
    return sub

df_fixed = grouped.apply(fix_deltas).reset_index(drop=True)

# Overwrite
out_path = r"C:\smilefund_project\warehouse\parquet\marts\revenue_quarters_true.parquet"
df_fixed.to_parquet(out_path)
print("✅ Negative deltas fixed and parquet overwritten")
import pandas as pd
from pathlib import Path

in_path  = r"C:\smilefund_project\warehouse\parquet\marts\revenue_quarters_true.parquet"
out_path = r"C:\smilefund_project\warehouse\parquet\marts\revenue_quarters_true.parquet"
log_csv  = r"C:\smilefund_project\warehouse\parquet\marts\_logs\negative_delta_fixes_2017+.csv"

df = pd.read_parquet(in_path)

# sum of Q1–Q3 per (cik, fy)
qsum = (df[df["fp"].isin(["Q1","Q2","Q3"])]
        .groupby(["cik","fy"], as_index=False)["Revenue"].sum()
        .rename(columns={"Revenue":"q123_sum"}))

df = df.merge(qsum, on=["cik","fy"], how="left")

# rows to flip: 10K delta negative AND Q1–Q3 positive
mask = (df["fp"].eq("10K delta")) & (df["Revenue"] < 0) & (df["q123_sum"] > 0)

# log what we're changing
fix_log = df.loc[mask, ["cik","fy","Revenue","q123_sum"]].copy()
fix_log.rename(columns={"Revenue":"delta_before"}, inplace=True)

# perform flip
df.loc[mask, "Revenue"] = -df.loc[mask, "Revenue"]

# after-values for the log
fix_log["delta_after"] = df.loc[mask, "Revenue"].values

# save outputs
Path(log_csv).parent.mkdir(parents=True, exist_ok=True)
if not fix_log.empty:
    fix_log.to_csv(log_csv, index=False)

df.drop(columns=["q123_sum"], inplace=True)
df.to_parquet(out_path)

print(f"Flipped {mask.sum()} negative 10K deltas. Log: {log_csv if mask.any() else 'no flips'}")
import json, glob

files = glob.glob(r"C:\smilefund_project\data\sec\company_facts\*1750*.json")
with open(files[0], "r") as f:
    j = json.load(f)

for tag in ["Revenues", "SalesRevenueNet", "OperatingRevenue"]:
    if tag in j["facts"].get("us-gaap", {}):
        units = j["facts"]["us-gaap"][tag]["units"]
        for u, arr in units.items():
            print(tag, u, "sample:", arr[:3])
import pandas as pd
from pathlib import Path

# ---- paths ----
BASE = r"C:\smilefund_project"
SRC  = rf"{BASE}\warehouse\parquet\marts\revenue_quarters_true.parquet"
DST  = rf"{BASE}\warehouse\parquet\marts\revenue_ttm_true.parquet"

# ---- load ----
df = pd.read_parquet(SRC)

# keep only real quarters (Q1–Q4)
quarters = df[df["fp"].isin(["Q1", "Q2", "Q3", "Q4"])].copy()
quarters = quarters.sort_values(["cik", "fy", "qn"])

# ---- rolling 4-quarter sum ----
quarters["TTM_Revenue"] = (
    quarters.groupby("cik")["Revenue"]
            .rolling(4, min_periods=4)
            .sum()
            .reset_index(level=0, drop=True)
)

# drop rows without a full 4-quarter window
ttm = quarters.dropna(subset=["TTM_Revenue"]).copy()

# ---- save ----
Path(DST).parent.mkdir(parents=True, exist_ok=True)
ttm.to_parquet(DST)
print(f"✅ Wrote {DST}  |  rows: {len(ttm):,}")
# example: Apple 2017–2018
apple = ttm.query("cik == 320193 and fy in [2017,2018]")
print(apple[["fy","fp","TTM_Revenue"]].sort_values(["fy","fp"]))
import json
from pathlib import Path
from collections import Counter, defaultdict
import pandas as pd

# ------------ CONFIG ------------
BASE = r"C:\smilefund_project"
FACTS_DIR = rf"{BASE}\data\sec\company_facts"  # company_facts/*.json
MARTS = rf"{BASE}\warehouse\parquet\marts"
SRC_TTM = rf"{MARTS}\revenue_ttm_true.parquet"
DST_ALIGNED = rf"{MARTS}\revenue_ttm_aligned.parquet"
LOGS = rf"{MARTS}\_logs"
Path(LOGS).mkdir(parents=True, exist_ok=True)

# same priority used earlier, any of these are fine to read FY end dates from
REV_TAGS = [
    "SalesRevenueNet",
    "Revenues",
    "RevenueFromContractWithCustomerExcludingAssessedTax",
    "SalesRevenueGoodsNet",
    "SalesRevenueServicesNet",
]

# ------------ HELPERS ------------

def month_from_yyyy_mm_dd(s: str) -> int | None:
    # s like "2017-09-30"
    if not s or len(s) < 7 or s[4] != '-' or s[7] != '-':
        return None
    try:
        return int(s[5:7])
    except Exception:
        return None

def infer_fye_month_from_json(path: Path) -> tuple[int, int | None]:
    """
    Return (cik, fye_month or None) inferred from FY facts (10-K where fp=='FY')
    by taking the mode of the 'end' month across years.
    """
    with path.open("r") as f:
        data = json.load(f)
    cik = int(data.get("cik", 0))

    # search revenue tags for FY rows and collect end-months
    months = []
    facts = data.get("facts", {}).get("us-gaap", {})
    for tag in REV_TAGS:
        block = facts.get(tag)
        if not block:
            continue
        units = block.get("units", {})
        for _, arr in units.items():
            for rec in arr:
                if rec.get("fp") != "FY":
                    continue
                # prefer 10-K, but accept if only FY exists elsewhere
                form = rec.get("form")
                if form and form != "10-K":
                    # keep, but lower weight; we’ll still just take mode across all
                    pass
                end_m = month_from_yyyy_mm_dd(rec.get("end"))
                if end_m:
                    months.append(end_m)

    if not months:
        return cik, None

    # mode; if tie, pick the most recent occurrence’s month
    counts = Counter(months)
    top = max(counts.items(), key=lambda kv: (kv[1], kv[0]))
    return cik, top[0]

def build_dim_fiscal_end_months() -> pd.DataFrame:
    rows = []
    files = list(Path(FACTS_DIR).glob("*.json"))
    for i, p in enumerate(files, 1):
        cik, fye = infer_fye_month_from_json(p)
        rows.append((cik, fye))
        if i % 2000 == 0:
            print(f"scanned {i}/{len(files)} for fye_month")
    dim = pd.DataFrame(rows, columns=["cik", "fye_month"]).drop_duplicates("cik")
    return dim

def quarter_end_month_from_qn(qn: int, fye_month: int) -> int:
    """
    Given qn in {1,2,3,4} where q4 ends in fye_month,
    compute quarter end month for qn.
      q4 end = fye_month
      q3 end = fye_month - 3
      q2 end = fye_month - 6
      q1 end = fye_month - 9
    """
    offset = {4: 0, 3: -3, 2: -6, 1: -9}[int(qn)]
    m = ((fye_month + offset - 1) % 12) + 1
    return m

def calendar_year_from_fy_and_end_month(fy: int, end_month: int, fye_month: int) -> int:
    """
    If a quarter ends AFTER the fiscal year-end month, that quarter belongs to the prior calendar year.
    Rule: calendar_year = fy if end_month <= fye_month else fy - 1
    Examples (Apple fye=9/Sep):
      Q1 ends Dec (12) -> calendar_year = fy-1
      Q2 ends Mar (3)  -> calendar_year = fy
      Q3 ends Jun (6)  -> calendar_year = fy
      Q4 ends Sep (9)  -> calendar_year = fy
    """
    return int(fy) if end_month <= fye_month else int(fy) - 1

def cal_quarter_from_month(m: int) -> int:
    return (int(m) - 1) // 3 + 1

# ------------ MAIN ------------
def main():
    # 1) Read TTM (input) and ensure qn available
    ttm = pd.read_parquet(SRC_TTM)
    if "qn" not in ttm.columns:
        # derive qn from fp if needed
        map_qn = {"Q1":1,"Q2":2,"Q3":3,"Q4":4}
        ttm["qn"] = ttm["fp"].map(map_qn).astype("Int64")

    # 2) Build/Load dim_fiscal (cik -> fye_month), then derive start month
    dim = build_dim_fiscal_end_months()
    dim["fiscal_year_start_month"] = ((dim["fye_month"].fillna(12).astype("Int64") % 12) + 1).astype("Int64")

    # log missing fye
    miss = dim[dim["fye_month"].isna()]
    if not miss.empty:
        miss.to_csv(rf"{LOGS}\missing_fye_month.csv", index=False)

    # 3) Attach fye to TTM rows
    a = ttm.merge(dim, on="cik", how="left")

    # 4) Compute quarter_end_month → calendar_year / calendar_quarter
    # Only do for rows that have both qn and fye_month
    has_fye = a["fye_month"].notna() & a["qn"].notna()
    end_month = []
    cal_year = []
    cal_qtr  = []
    for fy, qn, fye in a.loc[has_fye, ["fy","qn","fye_month"]].itertuples(index=False):
        em = quarter_end_month_from_qn(int(qn), int(fye))
        end_month.append(em)
        cal_year.append(calendar_year_from_fy_and_end_month(int(fy), em, int(fye)))
        cal_qtr.append(cal_quarter_from_month(em))

    # assign back (align lengths via index)
    tmp_idx = a.index[has_fye]
    a.loc[tmp_idx, "quarter_end_month"] = end_month
    a.loc[tmp_idx, "calendar_year"] = cal_year
    a.loc[tmp_idx, "calendar_quarter"] = cal_qtr

    # 5) Save aligned mart
    cols = ["cik","fy","fp","qn","TTM_Revenue","fye_month","fiscal_year_start_month",
            "quarter_end_month","calendar_year","calendar_quarter"]
    a[cols].to_parquet(DST_ALIGNED)
    print(f"✅ Wrote {DST_ALIGNED} | rows: {len(a)}")

    # 6) Quick sanity: Apple (320193) should show Q1 with end_month 12 and calendar_year = fy-1 (for Sep FYE)
    sample = a.query("cik == 320193 and fy == 2017")[["fy","fp","qn","TTM_Revenue","fye_month","quarter_end_month","calendar_year","calendar_quarter"]]
    sample.to_csv(rf"{LOGS}\apple_2017_alignment_check.csv", index=False)
    print("🔎 Wrote alignment check CSV for Apple 2017.")

if __name__ == "__main__":
    main()
import pandas as pd
from pathlib import Path

BASE = r"C:\smilefund_project"
MARTS = rf"{BASE}\warehouse\parquet\marts"
SRC   = rf"{MARTS}\revenue_ttm_aligned.parquet"
DST   = rf"{MARTS}\revenue_yoy_true.parquet"
LOGS  = rf"{MARTS}\_logs"

Path(LOGS).mkdir(parents=True, exist_ok=True)

# 1) Load aligned TTM
a = pd.read_parquet(SRC)

# safety: drop rows without calendar_year/qn
a = a.dropna(subset=["calendar_year", "qn", "TTM_Revenue"]).copy()
a["calendar_year"] = a["calendar_year"].astype(int)
a["qn"] = a["qn"].astype(int)

# 2) Take the LAST TTM per (cik, calendar_year) — i.e., the quarter with the max qn in that year
last_ttm = (a.sort_values(["cik","calendar_year","qn"])
              .groupby(["cik","calendar_year"], as_index=False)
              .last()[["cik","calendar_year","TTM_Revenue"]]
              .rename(columns={"TTM_Revenue":"TTM"}))

# 3) Join prior year
prior = last_ttm.copy()
prior["calendar_year"] = prior["calendar_year"] + 1  # shift forward to align prior with current year
prior = prior.rename(columns={"TTM":"TTM_prior"})

yoy = last_ttm.merge(prior, on=["cik","calendar_year"], how="left")

# 4) Compute YoY, handle div-by-zero cleanly
yoy["YoY_Growth"] = (yoy["TTM"] - yoy["TTM_prior"]) / yoy["TTM_prior"]
yoy.loc[yoy["TTM_prior"].isna(), "YoY_Growth"] = pd.NA  # first available year => NA
yoy.loc[yoy["TTM_prior"] == 0, "YoY_Growth"] = pd.NA    # avoid inf

# 5) Save YoY mart
yoy.to_parquet(DST)
print(f"✅ Wrote {DST} | rows: {len(yoy):,}")

# 6) Logs: rows with no prior
no_prior = yoy[yoy["TTM_prior"].isna()][["cik","calendar_year","TTM"]]
if not no_prior.empty:
    no_prior.to_csv(rf"{LOGS}\revenue_yoy_missing_prior.csv", index=False)
    print(f"🔎 Logged missing prior years → {LOGS}\\revenue_yoy_missing_prior.csv")

# 7) Quick sanity: Apple sample
apple = yoy.query("cik == 320193 and calendar_year in [2016,2017,2018]").sort_values("calendar_year")
print("Apple YoY preview:\n", apple)
import pandas as pd
from pathlib import Path

BASE = r"C:\smilefund_project"
MARTS = rf"{BASE}\warehouse\parquet\marts"

q = pd.read_parquet(rf"{MARTS}\revenue_quarters_true.parquet")
ttm = pd.read_parquet(rf"{MARTS}\revenue_ttm_aligned.parquet")
yoy = pd.read_parquet(rf"{MARTS}\revenue_yoy_true.parquet")

# quarters (Q1–Q3 + 10K delta)
fact_q = (q.assign(metric="revenue",
                   measure="quarter",
                   is_ttm=False, is_yoy=False)
            .rename(columns={"Revenue":"value"}))[["cik","fy","fp","qn","metric","measure","value","is_ttm","is_yoy"]]

# TTM (aligned → has calendar_year/quarter)
fact_ttm = (ttm.assign(metric="revenue",
                       measure="ttm",
                       is_ttm=True, is_yoy=False)
              .rename(columns={"TTM_Revenue":"value"}))[["cik","fy","fp","qn","calendar_year","calendar_quarter",
                                                        "metric","measure","value","is_ttm","is_yoy"]]

# YoY (per calendar_year)
fact_yoy = (yoy.assign(metric="revenue",
                       measure="yoy",
                       fp="FY", qn=4,  # anchor YoY to year-end for convenience
                       is_ttm=True, is_yoy=True)
              .rename(columns={"TTM":"value","YoY_Growth":"yoy"}))[["cik","calendar_year","metric","measure","value","yoy","is_ttm","is_yoy"]]

# write
Path(MARTS).mkdir(parents=True, exist_ok=True)
fact_q.to_parquet(rf"{MARTS}\fact_financials_quarters.parquet")
fact_ttm.to_parquet(rf"{MARTS}\fact_financials_ttm.parquet")
fact_yoy.to_parquet(rf"{MARTS}\fact_financials_yoy.parquet")

# optional: single unioned table
fact_all = pd.concat([
    fact_q.assign(calendar_year=pd.NA, calendar_quarter=pd.NA),
    fact_ttm,
    fact_yoy.assign(fy=pd.NA, fp=pd.NA, qn=pd.NA)
], ignore_index=True)

fact_all.to_parquet(rf"{MARTS}\fact_financials_standard.parquet")
print("✅ wrote fact_financials_standard.parquet")
