import sqlite3, pandas as pd

# make a file-backed DB so it persists between sessions
con = sqlite3.connect("smilefund.db")
cur = con.cursor()

cur.execute("""
CREATE TABLE IF NOT EXISTS dim_company (
  company_id INTEGER PRIMARY KEY AUTOINCREMENT,
  cik TEXT NOT NULL UNIQUE,
  name TEXT NOT NULL,
  ticker TEXT,
  sector TEXT,
  industry TEXT,
  fiscal_year_end_month INTEGER CHECK (fiscal_year_end_month BETWEEN 1 AND 12)
);
""")

cur.executemany("""
INSERT INTO dim_company (cik, name, ticker, sector, industry, fiscal_year_end_month)
VALUES (?, ?, ?, ?, ?, ?)
""", [
 ('0001652044','Alphabet Inc.','GOOG','Communication Services','Interactive Media & Services',12),
 ('0000789019','Microsoft Corporation','MSFT','Technology','Software—Infrastructure',6),
 ('0000320193','Apple Inc.','AAPL','Technology','Technology Hardware',9),
])

con.commit()

# inspect
pd.read_sql_query("SELECT * FROM dim_company ORDER BY company_id;", con)
%pip install psycopg2-binary SQLAlchemy pandas
import os
os.chdir("C:/Users/VTN183/OneDrive - University of Tennessee/smilefund_project/")

import os
import pandas as pd

proj = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
os.makedirs(os.path.join(proj, "warehouse", "parquet"), exist_ok=True)
base = os.path.join(proj, "warehouse", "parquet")
base

import pandas as pd

dim_company = pd.DataFrame([
    {"company_id": 1, "cik": "0001652044", "name": "Alphabet Inc.", "ticker": "GOOG",
     "sector": "Communication Services", "industry": "Interactive Media & Services",
     "fiscal_year_end_month": 12},
    {"company_id": 2, "cik": "0000789019", "name": "Microsoft Corporation", "ticker": "MSFT",
     "sector": "Technology", "industry": "Software—Infrastructure",
     "fiscal_year_end_month": 6},
    {"company_id": 3, "cik": "0000320193", "name": "Apple Inc.", "ticker": "AAPL",
     "sector": "Technology", "industry": "Technology Hardware",
     "fiscal_year_end_month": 9},
])

# (optional) set explicit dtypes; helpful later
dim_company = dim_company.astype({
    "company_id": "int64",
    "cik": "string",
    "name": "string",
    "ticker": "string",
    "sector": "string",
    "industry": "string",
    "fiscal_year_end_month": "int64",
})
path_company = os.path.join(base, "dim_company.parquet")
dim_company.to_parquet(path_company, index=False, engine="pyarrow")
pd.read_parquet(path_company)
dim_calendar = pd.DataFrame([
    {"calendar_id": 1, "date": "2023-03-31", "fiscal_year": 2023, "fiscal_quarter": "Q1", "fiscal_month": 3, "period_type": "QTR"},
    {"calendar_id": 2, "date": "2023-06-30", "fiscal_year": 2023, "fiscal_quarter": "Q2", "fiscal_month": 6, "period_type": "QTR"},
    {"calendar_id": 3, "date": "2023-09-30", "fiscal_year": 2023, "fiscal_quarter": "Q3", "fiscal_month": 9, "period_type": "QTR"},
    {"calendar_id": 4, "date": "2023-12-31", "fiscal_year": 2023, "fiscal_quarter": "Q4", "fiscal_month": 12, "period_type": "FY"},
]).astype({
    "calendar_id": "int64","date": "string","fiscal_year": "int64","fiscal_quarter": "string",
    "fiscal_month": "int64","period_type": "string"
})

path_calendar = os.path.join(base, "dim_calendar.parquet")
dim_calendar.to_parquet(path_calendar, index=False)
pd.read_parquet(path_calendar)
dim_metric = pd.DataFrame([
    {"metric_id": 1, "metric_name": "Revenue", "xbrl_tag": "Revenues", "normal_balance": "Credit"},
    {"metric_id": 2, "metric_name": "Net Income", "xbrl_tag": "NetIncomeLoss", "normal_balance": "Credit"},
    {"metric_id": 3, "metric_name": "Total Assets", "xbrl_tag": "Assets", "normal_balance": "Debit"},
]).astype({"metric_id":"int64","metric_name":"string","xbrl_tag":"string","normal_balance":"string"})

path_metric = os.path.join(base, "dim_metric.parquet")
dim_metric.to_parquet(path_metric, index=False)
pd.read_parquet(path_metric)
dim_filing = pd.DataFrame([
    {"filing_id": 1, "form_type": "10-K", "filing_date": "2024-02-03",
     "accepted_date": "2024-02-03", "period_end_date": "2023-12-31",
     "accession_number": "0001652044-24-000050", "filing_url": "https://..."},
]).astype({
    "filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
    "period_end_date":"string","accession_number":"string","filing_url":"string"
})

path_filing = os.path.join(base, "dim_filing.parquet")
dim_filing.to_parquet(path_filing, index=False)
pd.read_parquet(path_filing)
dim_unit = pd.DataFrame([
    {"unit_id":1,"unit_code":"USD","category":"currency","iso_currency":"USD","decimals_hint":2,"description":"U.S. dollars"},
    {"unit_id":2,"unit_code":"shares","category":"shares","iso_currency":None,"decimals_hint":0,"description":"Common shares"},
    {"unit_id":3,"unit_code":"USD_per_share","category":"per_share","iso_currency":"USD","decimals_hint":2,"description":"Dollars per share"},
]).astype({"unit_id":"int64","unit_code":"string","category":"string","iso_currency":"string",
          "decimals_hint":"int64","description":"string"})

path_unit = os.path.join(base, "dim_unit.parquet")
dim_unit.to_parquet(path_unit, index=False)
pd.read_parquet(path_unit)
fact_financials = pd.DataFrame([
    {"financial_id": 1, "company_id": 1, "metric_id": 1, "calendar_id": 4,
     "value": 307394000000.00, "unit_id": 1, "filing_id": 1, "consolidated_flag": "Consolidated"},
    {"financial_id": 2, "company_id": 1, "metric_id": 2, "calendar_id": 4,
     "value": 73795000000.00, "unit_id": 1, "filing_id": 1, "consolidated_flag": "Consolidated"},
]).astype({
    "financial_id":"int64","company_id":"int64","metric_id":"int64","calendar_id":"int64",
    "value":"float64","unit_id":"int64","filing_id":"int64","consolidated_flag":"string"
})

path_fin = os.path.join(base, "fact_financials.parquet")
fact_financials.to_parquet(path_fin, index=False)
pd.read_parquet(path_fin)
fact_holdings = pd.DataFrame([
    {"holding_id":1,"company_id":1,"asof_date":"2025-03-31","weight":0.04200,"shares":145,"market_value":30200,"source":"SMILE_FUND"},
    {"holding_id":2,"company_id":2,"asof_date":"2025-03-31","weight":0.03800,"shares":110,"market_value":29000,"source":"SMILE_FUND"},
]).astype({
    "holding_id":"int64","company_id":"int64","asof_date":"string","weight":"float64",
    "shares":"float64","market_value":"float64","source":"string"
})

path_hold = os.path.join(base, "fact_holdings.parquet")
fact_holdings.to_parquet(path_hold, index=False)
pd.read_parquet(path_hold)
import pandas as pd

path = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\Approved List 9.14.25(Approved List 9.14.csv"
df = pd.read_csv(path)
df.head()
import os, pandas as pd
base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
path_company  = os.path.join(base, "dim_company.parquet")
path_holdings = os.path.join(base, "fact_holdings.parquet")
csv_path = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\Approved List 9.14.25(Approved List 9.14.csv"
df = pd.read_csv(csv_path)

# lower/strip col names
df.columns = [c.strip().lower() for c in df.columns]

# map common variants to our target names
RENAME_MAP = {
    "ticker": "ticker",
    "symbol": "ticker",
    "company": "name",
    "company name": "name",
    "name": "name",
    "sector": "sector",
    "gics sector": "sector",
    "weight": "weight",
    "weight %": "weight",
    "portfolio weight": "weight",
    "asof": "asof_date",
    "as_of": "asof_date",
    "as of": "asof_date",
    "date": "asof_date"
}

df = df.rename(columns={c: RENAME_MAP.get(c, c) for c in df.columns})

# keep only the columns we know how to use (others are ignored for now)
keep_cols = [c for c in ["ticker","name","sector","weight","asof_date"] if c in df.columns]
df = df[keep_cols].copy()
df.head()
import pandas as pd
import re

# df already loaded with a 'ticker' column
tickers_raw = df["ticker"].astype(str)

def normalize_ticker(s: str) -> str:
    s = s.strip().upper()
    s = s.replace(" ", "")
    # convert share-class separator "/" to "." (e.g., BRK/B -> BRK.B)
    s = re.sub(r"/", ".", s)
    return s

df["ticker_norm"] = tickers_raw.map(normalize_ticker)

# show before/after for first few rows
preview = df[["ticker", "ticker_norm"]].head(10)
preview
import os, pandas as pd

base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
path_company = os.path.join(base, "dim_company.parquet")

# if the file exists, load it; else start empty with the right columns
if os.path.exists(path_company):
    dim_company = pd.read_parquet(path_company)
else:
    dim_company = pd.DataFrame(columns=[
        "company_id","cik","name","ticker","sector","industry","fiscal_year_end_month"
    ])
    dim_company = dim_company.astype({
        "company_id":"int64", "cik":"string","name":"string","ticker":"string",
        "sector":"string","industry":"string","fiscal_year_end_month":"float64"
    })

dim_company.head()
# make sure we have the normalized ticker column from the prior step
assert "ticker_norm" in df.columns, "Run the normalization step first."

# unique tickers from your CSV
incoming = (df[["ticker_norm"]]
            .dropna()
            .drop_duplicates()
            .rename(columns={"ticker_norm":"ticker"}))

# left-join to see which are missing from dim_company
check = incoming.merge(dim_company[["ticker","company_id"]], on="ticker", how="left")
to_add = check[check["company_id"].isna()].drop(columns=["company_id"]).copy()

print(f"New tickers to add: {len(to_add)}")
to_add.head(20)
if len(to_add) > 0:
    # next sequential company_id
    next_id = (dim_company["company_id"].max() + 1) if len(dim_company) else 1
    new_ids = list(range(next_id, next_id + len(to_add)))

    n = len(to_add)  # store length once

    new_rows = pd.DataFrame({
        "company_id": new_ids,
        "cik": ["" for _ in range(n)],
        "name": ["" for _ in range(n)],
        "ticker": to_add["ticker"].astype("string"),
        "sector": ["" for _ in range(n)],
        "industry": ["" for _ in range(n)],
        "fiscal_year_end_month": [None for _ in range(n)]
    })

    dim_company = pd.concat([dim_company, new_rows], ignore_index=True)
    dim_company = dim_company.drop_duplicates(subset=["ticker"], keep="first").reset_index(drop=True)

    dim_company.to_parquet(path_company, index=False)

# reload to confirm
dim_company = pd.read_parquet(path_company)
print(f"dim_company rows: {len(dim_company)}")
dim_company.tail(10)
import pandas as pd
import os
from datetime import date

base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
path_company  = os.path.join(base, "dim_company.parquet")
path_holdings = os.path.join(base, "fact_holdings.parquet")

# 1) reload company dim (has company_id + ticker)
dim_company = pd.read_parquet(path_company)[["company_id","ticker"]]

# 2) make a clean ticker list from your dataframe (the one with ticker_norm)
assert "ticker_norm" in df.columns, "Run the normalization step first."
tickers = (df[["ticker_norm"]]
           .dropna()
           .drop_duplicates()
           .rename(columns={"ticker_norm":"ticker"}))

# 3) attach company_id to each ticker
hold = tickers.merge(dim_company, on="ticker", how="left")

# 4) add minimal holding fields
asof = "2025-03-31"   # ← set the date you want (or str(date.today()))
preview_holdings = pd.DataFrame({
    "company_id": hold["company_id"].astype("Int64"),  # allow nulls to show if any missing
    "asof_date":  asof,
    "weight":     pd.NA,        # will fill later
    "shares":     pd.NA,
    "market_value": pd.NA,
    "source":     "SMILE_FUND"
})

preview_holdings.head(10)
# load existing file if present
if os.path.exists(path_holdings):
    existing = pd.read_parquet(path_holdings)
else:
    existing = pd.DataFrame(columns=[
        "holding_id","company_id","asof_date","weight","shares","market_value","source"
    ])

# build new block with temporary ids
new_block = preview_holdings.copy()
new_block = new_block.astype({
    "company_id":"int64",
    "asof_date":"string",
    "source":"string"
})

# combine and assign holding_id sequentially
combined = pd.concat([existing, new_block], ignore_index=True)

# de-dup just in case (one row per company per as-of date)
combined = combined.drop_duplicates(subset=["company_id","asof_date"], keep="last").reset_index(drop=True)

combined["holding_id"] = range(1, len(combined)+1)

# save
combined.to_parquet(path_holdings, index=False)

# quick look
pd.read_parquet(path_holdings).head(12)
import os, pandas as pd

base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
out_dir = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\outputs"
os.makedirs(out_dir, exist_ok=True)

path_company  = os.path.join(base, "dim_company.parquet")
path_holdings = os.path.join(base, "fact_holdings.parquet")

dim_company = pd.read_parquet(path_company)[["company_id","ticker","name","sector","industry"]]
holdings    = pd.read_parquet(path_holdings)

review = (holdings
          .merge(dim_company, on="company_id", how="left")
          .sort_values(["asof_date","ticker"]))

# quick on-screen check
review.head(15)
print("rows:", len(review), "| unique companies:", review["company_id"].nunique())

# save for eyeballing in Excel
review_csv = os.path.join(out_dir, "holdings_review.csv")
review_parq = os.path.join(out_dir, "holdings_review.parquet")
review.to_csv(review_csv, index=False)
review.to_parquet(review_parq, index=False)
print("Wrote:", review_csv)
import os, pandas as pd

proj_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
parq_base = os.path.join(proj_base, "warehouse", "parquet")
os.makedirs(parq_base, exist_ok=True)

path_filing   = os.path.join(parq_base, "dim_filing.parquet")

# create if missing
if not os.path.exists(path_filing):
    dim_filing = pd.DataFrame(columns=[
        "filing_id","form_type","filing_date","accepted_date",
        "period_end_date","accession_number","filing_url"
    ]).astype({
        "filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
        "period_end_date":"string","accession_number":"string","filing_url":"string"
    })
    dim_filing.to_parquet(path_filing, index=False)
else:
    dim_filing = pd.read_parquet(path_filing)

dim_filing.tail(3)
import os, glob

base_path = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"
files = glob.glob(os.path.join(base_path, "*.json"))

print(f"Found {len(files):,} JSON files")
print("Example:", files[:3])
import pandas as pd
from pathlib import Path

# --- 1️⃣  Revenue tags ---
revenue_tags = [
    "Revenues",
    "RevenueFromContractWithCustomerExcludingAssessedTax",
    "RevenueFromContractWithCustomerIncludingAssessedTax",
    "SalesRevenueNet",
    "SalesRevenueGoodsNet",
    "SalesRevenueServicesNet",
    "TotalRevenuesAndOtherIncome",
    "PremiumsEarnedNet",                # insurance
    "InterestIncomeOperating"           # banks
]

# --- 2️⃣  Net Income tags ---
net_income_tags = [
    "NetIncomeLoss",
    "ProfitLoss",
    "IncomeLossFromContinuingOperations",
    "NetIncomeLossAvailableToCommonStockholdersBasic",
    "NetIncomeLossAttributableToParent"
]

# --- 3️⃣  Asset tags ---
asset_tags = [
    "Assets",
    "AssetsCurrent",
    "TotalAssets",
    "AssetsFairValueDisclosure"
]

# --- 4️⃣  Combine into rows ---
records = []

for tag in revenue_tags:
    records.append({"xbrl_tag": tag, "metric_name": "Revenue", "metric_group": "Revenue", "normal_balance": "Credit"})

for tag in net_income_tags:
    records.append({"xbrl_tag": tag, "metric_name": "Net Income", "metric_group": "Income", "normal_balance": "Credit"})

for tag in asset_tags:
    records.append({"xbrl_tag": tag, "metric_name": "Assets", "metric_group": "Assets", "normal_balance": "Debit"})

# --- 5️⃣  Build DataFrame ---
df_metric = pd.DataFrame(records)
df_metric.insert(0, "metric_id", range(1, len(df_metric) + 1))

# --- 6️⃣  Export to Parquet ---
out_path = Path(r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet\dim_metric.parquet")
df_metric.to_parquet(out_path, index=False)

print("✅ dim_metric.parquet created with", len(df_metric), "rows")
df_metric.head()
import os, json, pandas as pd

# 1) paths
parq_base  = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
facts_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"

path_metric = os.path.join(parq_base, "dim_metric.parquet")

# 2) choose one company file (edit CIK if you want another)
cik = "0001652044"  # Alphabet; change to any CIK you like
cik_file = os.path.join(facts_base, f"CIK{cik}.json")
print("Using:", cik_file)

# 3) load company JSON
with open(cik_file, "r") as f:
    data = json.load(f)

entity = data.get("entityName", "(unknown)")
usgaap_tags = set(data.get("facts", {}).get("us-gaap", {}).keys())
print("Entity:", entity)
print("us-gaap tag count:", len(usgaap_tags))

# 4) load your master metric map
dim_metric = pd.read_parquet(path_metric)

# 5) check tag presence by metric_group
def presence_for_group(group_name):
    subset = dim_metric[dim_metric["metric_group"] == group_name].copy()
    subset["present"] = subset["xbrl_tag"].apply(lambda t: t in usgaap_tags)
    return subset[["xbrl_tag","metric_name","metric_group","present"]].sort_values("xbrl_tag")

for grp in ["Revenue", "Income", "Assets"]:
    print(f"\n=== {grp} tags present? ===")
    display(presence_for_group(grp).head(50))
import os, json, pandas as pd

# paths + target
facts_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"
cik = "0001652044"                       # Alphabet (change if you want another company)
tag = "Revenues"                         # from your presence table
cik_file = os.path.join(facts_base, f"CIK{cik}.json")

with open(cik_file, "r") as f:
    data = json.load(f)

# get all unit series for the tag (e.g., USD, USDm, shares, etc.)
series = data.get("facts", {}).get("us-gaap", {}).get(tag, {}).get("units", {})
if not series:
    raise ValueError(f"No units found for {tag}")

# prefer USD if present; otherwise take the first numeric-looking unit
preferred_units = ["USD", "usd", "USDm", "USDth", "USD $"]  # common variants
unit_key = next((u for u in preferred_units if u in series), next(iter(series.keys())))
rows = pd.DataFrame(series[unit_key])

# keep **annual** rows (fp == 'FY')
annual = rows[rows["fp"].str.upper().eq("FY")].copy()

# choose the latest fiscal year available
annual["fy"] = pd.to_numeric(annual["fy"], errors="coerce")
latest = annual.sort_values("fy").tail(1).iloc[0]

preview = {
    "entity": data.get("entityName"),
    "cik": data.get("cik"),
    "tag": tag,
    "unit": unit_key,
    "fy": int(latest["fy"]),
    "period_end": latest["end"],         # YYYY-MM-DD
    "value": float(latest["val"]),
    "form": latest.get("form", ""),
    "accession": latest.get("accn", ""),
    "filed": latest.get("filed", latest["end"])
}
preview
import os, pandas as pd

proj_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
parq_base = os.path.join(proj_base, "warehouse", "parquet")

# ---- load all tables (create empty if missing)
def read_or_empty(path, cols, dtypes):
    if os.path.exists(path):
        return pd.read_parquet(path)
    df = pd.DataFrame(columns=cols).astype(dtypes)
    df.to_parquet(path, index=False)
    return df

path_company = os.path.join(parq_base, "dim_company.parquet")
path_metric  = os.path.join(parq_base, "dim_metric.parquet")
path_unit    = os.path.join(parq_base, "dim_unit.parquet")
path_cal     = os.path.join(parq_base, "dim_calendar.parquet")
path_filing  = os.path.join(parq_base, "dim_filing.parquet")
path_fact    = os.path.join(parq_base, "fact_financials.parquet")

dim_company = read_or_empty(path_company,
    ["company_id","cik","name","ticker","sector","industry","fiscal_year_end_month"],
    {"company_id":"int64","cik":"string","name":"string","ticker":"string",
     "sector":"string","industry":"string","fiscal_year_end_month":"float64"}
)
dim_metric = pd.read_parquet(path_metric)  # you created this already
dim_unit   = read_or_empty(path_unit,
    ["unit_id","unit_code","category","iso_currency","decimals_hint","description"],
    {"unit_id":"int64","unit_code":"string","category":"string","iso_currency":"string",
     "decimals_hint":"float64","description":"string"}
)
dim_calendar = read_or_empty(path_cal,
    ["calendar_id","date","fiscal_year","fiscal_quarter","fiscal_month","period_type"],
    {"calendar_id":"int64","date":"string","fiscal_year":"int64","fiscal_quarter":"string",
     "fiscal_month":"int64","period_type":"string"}
)
dim_filing = read_or_empty(path_filing,
    ["filing_id","form_type","filing_date","accepted_date","period_end_date","accession_number","filing_url"],
    {"filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
     "period_end_date":"string","accession_number":"string","filing_url":"string"}
)
if os.path.exists(path_fact):
    fact_fin = pd.read_parquet(path_fact)
else:
    fact_fin = pd.DataFrame(columns=[
        "financial_id","company_id","metric_id","calendar_id","value","unit_id","filing_id","consolidated_flag"
    ]).astype({"financial_id":"int64","company_id":"int64","metric_id":"int64","calendar_id":"int64",
               "value":"float64","unit_id":"int64","filing_id":"int64","consolidated_flag":"string"})

# ---- tiny helpers (inline) to "ensure" IDs exist
def ensure_company_by_cik(cik_str, ticker=""):
    global dim_company
    hit = dim_company.loc[dim_company["cik"]==cik_str]
    if not hit.empty: 
        return int(hit.iloc[0]["company_id"])
    next_id = (dim_company["company_id"].max()+1) if len(dim_company) else 1
    row = {"company_id":next_id,"cik":cik_str,"name":"","ticker":ticker,
           "sector":"","industry":"","fiscal_year_end_month":None}
    dim_company = pd.concat([dim_company, pd.DataFrame([row])], ignore_index=True)
    dim_company.to_parquet(path_company, index=False)
    return next_id

def ensure_metric_by_tag(xbrl_tag, metric_name=None, normal_balance="Credit", metric_group=None):
    global dim_metric
    hit = dim_metric.loc[dim_metric["xbrl_tag"]==xbrl_tag]
    if not hit.empty: return int(hit.iloc[0]["metric_id"])
    next_id = (dim_metric["metric_id"].max()+1) if len(dim_metric) else 1
    row = {"metric_id":next_id,"xbrl_tag":xbrl_tag,
           "metric_name":metric_name or xbrl_tag,
           "metric_group":metric_group or metric_name or xbrl_tag,
           "normal_balance":normal_balance}
    dim_metric = pd.concat([dim_metric, pd.DataFrame([row])], ignore_index=True)
    dim_metric.to_parquet(path_metric, index=False)
    return next_id

def ensure_unit(unit_code, category="currency", iso="USD", decimals=2, desc=""):
    global dim_unit
    hit = dim_unit.loc[dim_unit["unit_code"]==unit_code]
    if not hit.empty: return int(hit.iloc[0]["unit_id"])
    next_id = (dim_unit["unit_id"].max()+1) if len(dim_unit) else 1
    row = {"unit_id":next_id,"unit_code":unit_code,"category":category,
           "iso_currency":iso,"decimals_hint":decimals,"description":desc}
    dim_unit = pd.concat([dim_unit, pd.DataFrame([row])], ignore_index=True)
    dim_unit.to_parquet(path_unit, index=False)
    return next_id

def ensure_calendar(period_end, fiscal_year, fiscal_quarter, fiscal_month, period_type="FY"):
    global dim_calendar
    hit = dim_calendar.loc[(dim_calendar["date"]==period_end) & (dim_calendar["period_type"]==period_type)]
    if not hit.empty: return int(hit.iloc[0]["calendar_id"])
    next_id = (dim_calendar["calendar_id"].max()+1) if len(dim_calendar) else 1
    row = {"calendar_id":next_id,"date":period_end,"fiscal_year":int(fiscal_year),
           "fiscal_quarter":fiscal_quarter,"fiscal_month":int(fiscal_month),"period_type":period_type}
    dim_calendar = pd.concat([dim_calendar, pd.DataFrame([row])], ignore_index=True)
    dim_calendar.to_parquet(path_cal, index=False)
    return next_id

def ensure_filing(form_type, filing_date, period_end, accession=""):
    global dim_filing
    hit = dim_filing.loc[(dim_filing["form_type"]==form_type)&
                         (dim_filing["filing_date"]==filing_date)&
                         (dim_filing["period_end_date"]==period_end)]
    if accession:
        hit = dim_filing.loc[dim_filing["accession_number"]==accession] if hit.empty else hit
    if not hit.empty: return int(hit.iloc[0]["filing_id"])
    next_id = (dim_filing["filing_id"].max()+1) if len(dim_filing) else 1
    row = {"filing_id":next_id,"form_type":form_type,"filing_date":filing_date,
           "accepted_date":filing_date,"period_end_date":period_end,
           "accession_number":accession,"filing_url":""}
    dim_filing = pd.concat([dim_filing, pd.DataFrame([row])], ignore_index=True)
    dim_filing.to_parquet(path_filing, index=False)
    return next_id

# ---- take values from your preview dict (which you printed above)
pv = preview.copy()
cik_str = str(pv["cik"]).zfill(10)     # zero-pad CIK to 10 characters
fy = int(pv["fy"])
fm = int(pv["period_end"][5:7])        # month from YYYY-MM-DD
fq = "Q4"                              # convention for FY rows

company_id  = ensure_company_by_cik(cik_str)
metric_id   = ensure_metric_by_tag(pv["tag"], metric_name="Revenue", normal_balance="Credit", metric_group="Revenue")
unit_id     = ensure_unit(pv["unit"], category="currency", iso="USD", decimals=2, desc="U.S. dollars")
calendar_id = ensure_calendar(pv["period_end"], fy, fq, fm, period_type="FY")
filing_id   = ensure_filing(pv["form"], pv["filed"], pv["period_end"], pv.get("accession",""))

# ---- write/merge into fact_financials
natural_key = ["company_id","metric_id","calendar_id","filing_id"]
new_row = pd.DataFrame([{
    "company_id": company_id,
    "metric_id": metric_id,
    "calendar_id": calendar_id,
    "value": float(pv["value"]),
    "unit_id": unit_id,
    "filing_id": filing_id,
    "consolidated_flag": "Consolidated"
}])

fact_fin = pd.concat([fact_fin.drop(columns=[], errors="ignore"), new_row], ignore_index=True)
fact_fin = fact_fin.drop_duplicates(subset=natural_key, keep="last").reset_index(drop=True)
fact_fin["financial_id"] = range(1, len(fact_fin)+1)
fact_fin.to_parquet(path_fact, index=False)

print("✅ Wrote one fact row. Current fact_financials size:", len(fact_fin))
fact_fin.tail(3)
import os, json, pandas as pd

parq_base  = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\warehouse\parquet"
facts_base = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project\data\sec\company_facts"
cik = "0001652044"  # GOOG

# load metric map and company json
dim_metric = pd.read_parquet(os.path.join(parq_base, "dim_metric.parquet"))
with open(os.path.join(facts_base, f"CIK{cik}.json"), "r") as f:
    data = json.load(f)

usgaap = data.get("facts", {}).get("us-gaap", {})
income_tags = dim_metric.loc[dim_metric["metric_group"]=="Income", "xbrl_tag"].tolist()

# choose the first available tag in this company file
tag_income = next((t for t in income_tags if t in usgaap), None)
assert tag_income, "No income tag from your list appears in this file."

series = usgaap[tag_income]["units"]
unit_key = "USD" if "USD" in series else next(iter(series.keys()))
rows = pd.DataFrame(series[unit_key])

annual = rows[rows["fp"].str.upper().eq("FY")].copy()
annual["fy"] = pd.to_numeric(annual["fy"], errors="coerce")
latest = annual.sort_values("fy").tail(1).iloc[0]

preview_income = {
    "entity": data.get("entityName"),
    "cik": str(data.get("cik")).zfill(10),
    "tag": tag_income,
    "unit": unit_key,
    "fy": int(latest["fy"]),
    "period_end": latest["end"],
    "value": float(latest["val"]),
    "form": latest.get("form",""),
    "accession": latest.get("accn",""),
    "filed": latest.get("filed", latest["end"]),
}
preview_income
# reuse the small ensure-* helpers you already ran earlier in the notebook
# (company, metric, unit, calendar, filing). If they’re not in memory anymore,
# re-run the earlier cell that defined ensure_company_by_cik, ensure_metric_by_tag, etc.

pv = preview_income
fy = int(pv["fy"])
fm = int(pv["period_end"][5:7])
fq = "Q4"

company_id  = ensure_company_by_cik(pv["cik"])
metric_id   = ensure_metric_by_tag(pv["tag"], metric_name="Net Income", normal_balance="Credit", metric_group="Income")
unit_id     = ensure_unit(pv["unit"], category="currency", iso="USD", decimals=2, desc="U.S. dollars")
calendar_id = ensure_calendar(pv["period_end"], fy, fq, fm, period_type="FY")
filing_id   = ensure_filing(pv["form"], pv["filed"], pv["period_end"], pv.get("accession",""))

# load fact table, append, dedupe by natural key
path_fact = os.path.join(parq_base, "fact_financials.parquet")
fact_fin = pd.read_parquet(path_fact) if os.path.exists(path_fact) else pd.DataFrame(columns=[
    "financial_id","company_id","metric_id","calendar_id","value","unit_id","filing_id","consolidated_flag"
]).astype({"financial_id":"int64","company_id":"int64","metric_id":"int64","calendar_id":"int64","value":"float64","unit_id":"int64","filing_id":"int64","consolidated_flag":"string"})

new_row = pd.DataFrame([{
    "company_id": company_id,
    "metric_id": metric_id,
    "calendar_id": calendar_id,
    "value": float(pv["value"]),
    "unit_id": unit_id,
    "filing_id": filing_id,
    "consolidated_flag": "Consolidated"
}])

natural_key = ["company_id","metric_id","calendar_id","filing_id"]
fact_fin = pd.concat([fact_fin, new_row], ignore_index=True)
fact_fin = fact_fin.drop_duplicates(subset=natural_key, keep="last").reset_index(drop=True)
fact_fin["financial_id"] = range(1, len(fact_fin)+1)
fact_fin.to_parquet(path_fact, index=False)

print("Added/updated Net Income row. Rows now:", len(fact_fin))
import os

parq_base = "parq_base"   # or any path you prefer, e.g., "C:/SMILEFund/parq_base"
os.makedirs(parq_base, exist_ok=True)
print(f"Parquet base folder created at: {os.path.abspath(parq_base)}")
import pandas as pd, os, json

parq = "parq_base"  # <- your folder

metric_pref = pd.DataFrame([
    # Revenue family
    ("RevenueStandard", "us-gaap:Revenues", 1),
    ("RevenueStandard", "us-gaap:RevenueFromContractWithCustomerExcludingAssessedTax", 2),
    ("RevenueStandard", "us-gaap:SalesRevenueNet", 3),
    ("RevenueStandard", "us-gaap:SalesRevenueGoodsNet", 4),
    ("RevenueStandard", "us-gaap:SalesRevenueServicesNet", 5),
    # Net income family (already in your fact table but keep for symmetry)
    ("NetIncomeStandard", "us-gaap:NetIncomeLoss", 1),
], columns=["metric_standard", "xbrl_tag", "pref_rank"])

metric_pref.to_parquet(os.path.join(parq, "dim_metric_preference.parquet"), index=False)
import os

PROJ = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
PARQ = os.path.join(PROJ, "warehouse", "parquet")
os.makedirs(PARQ, exist_ok=True)

print("PARQ =", PARQ)
import pandas as pd, os

metric_pref = pd.DataFrame([
    # Revenue family (lower rank = more preferred)
    ("RevenueStandard", "Revenues", 1),
    ("RevenueStandard", "RevenueFromContractWithCustomerExcludingAssessedTax", 2),
    ("RevenueStandard", "SalesRevenueNet", 3),
    ("RevenueStandard", "SalesRevenueGoodsNet", 4),
    ("RevenueStandard", "SalesRevenueServicesNet", 5),

    # Net income family
    ("NetIncomeStandard", "NetIncomeLoss", 1),
], columns=["metric_standard", "xbrl_tag", "pref_rank"])

metric_pref_path = os.path.join(PARQ, "dim_metric_preference.parquet")
metric_pref.to_parquet(metric_pref_path, index=False)
metric_pref.head()
import pandas as pd, os

# Inputs
fact_path   = os.path.join(PARQ, "fact_financials.parquet")
metric_path = os.path.join(PARQ, "dim_metric.parquet")
cal_path    = os.path.join(PARQ, "dim_calendar.parquet")
co_path     = os.path.join(PARQ, "dim_company.parquet")
unit_path   = os.path.join(PARQ, "dim_unit.parquet")
filing_path = os.path.join(PARQ, "dim_filing.parquet")
pref_path   = os.path.join(PARQ, "dim_metric_preference.parquet")

fact = pd.read_parquet(fact_path)
dim_metric = pd.read_parquet(metric_path)[["metric_id","xbrl_tag","metric_name"]]
dim_calendar = pd.read_parquet(cal_path)[["calendar_id","date","fiscal_year","period_type"]]
dim_company = pd.read_parquet(co_path)[["company_id","ticker","cik"]]
dim_unit = pd.read_parquet(unit_path) if os.path.exists(unit_path) else pd.DataFrame(columns=["unit_id","unit_code"])
metric_pref = pd.read_parquet(pref_path)

if os.path.exists(filing_path):
    dim_filing_raw = pd.read_parquet(filing_path)
    # normalize filing date col to a common name
    if "filed_date" in dim_filing_raw.columns:
        dim_filing = dim_filing_raw[["filing_id","filed_date","form_type" if "form_type" in dim_filing_raw.columns else "form"]]
    else:
        # your dim_filing uses filing_date/form_type
        dim_filing = dim_filing_raw.rename(columns={"filing_date":"filed_date"})[
            ["filing_id","filed_date","form_type" if "form_type" in dim_filing_raw.columns else "form"]
        ]
else:
    dim_filing = pd.DataFrame(columns=["filing_id","filed_date","form_type"])

# Join everything
df = (fact
      .merge(dim_metric, on="metric_id", how="left")
      .merge(metric_pref, on="xbrl_tag", how="left")        # <-- matches your tag names
      .merge(dim_calendar, on="calendar_id", how="left")
      .merge(dim_company, on="company_id", how="left")
      .merge(dim_filing, on="filing_id", how="left"))

# Keep only metrics we’re standardizing
df = df[df["metric_standard"].notna()].copy()

# Tie-breakers
df["is_consolidated"] = (df.get("consolidated_flag","").astype(str).str.lower()=="consolidated").astype(int)
# Prefer USD: your dim_unit has unit_id=1 for USD
usd_unit_ids = set(dim_unit.loc[dim_unit["unit_code"].str.upper()=="USD", "unit_id"]) if "unit_code" in dim_unit.columns else {1}
df["is_usd"] = df["unit_id"].isin(usd_unit_ids).astype(int)
df["filed_date"] = pd.to_datetime(df.get("filed_date"))

# Sort (best first), then de-dup to keep the winner per company×period×metric_standard
df = df.sort_values([
    "company_id","calendar_id","metric_standard",
    "pref_rank",        # lower is better
    "is_consolidated",  # consolidated preferred
    "is_usd",           # USD preferred
    "filed_date"        # latest preferred
], ascending=[True,True,True, True, False, False, False])

std = (df
       .drop_duplicates(["company_id","calendar_id","metric_standard"], keep="first")
       .rename(columns={"value":"value_numeric"})
       [["company_id","calendar_id","metric_standard","value_numeric",
         "ticker","cik","fiscal_year","period_type","date"]])

# Persist
std_path = os.path.join(PARQ, "fact_financials_standard.parquet")
std.to_parquet(std_path, index=False)

# QA
assert not std.duplicated(["company_id","calendar_id","metric_standard"]).any()
print("Standardized rows:", len(std))
print(std.pivot_table(index="metric_standard", values="value_numeric", aggfunc="count"))

# Wide convenience view
wide = (std
        .pivot_table(index=["company_id","calendar_id","ticker","cik","fiscal_year","period_type","date"],
                     columns="metric_standard", values="value_numeric", aggfunc="first")
        .reset_index())
wide.columns.name = None
wide_path = os.path.join(PARQ, "vw_financials_wide.parquet")
wide.to_parquet(wide_path, index=False)
print("Wrote wide view:", wide_path, "rows:", len(wide))
co_raw = pd.read_parquet(paths["dim_company"])
if "ticker" not in co_raw.columns:
    co_raw["ticker"] = pd.NA  # add empty ticker column
    co_raw.to_parquet(paths["dim_company"], index=False)
    print("Added empty 'ticker' column to dim_company.")
import os, pandas as pd

# --- Paths
PROJ = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
PARQ = os.path.join(PROJ, "warehouse", "parquet")

path_hold = os.path.join(PARQ, "fact_holdings.parquet")
path_wide = os.path.join(PARQ, "vw_financials_wide.parquet")
path_co   = os.path.join(PARQ, "dim_company.parquet")
out_path  = os.path.join(PARQ, "vw_holdings_with_financials.parquet")

# --- Helpers
def safe_select(df, cols):
    have = [c for c in cols if c in df.columns]
    return df[have].copy()

# --- Load
hold = pd.read_parquet(path_hold)          # expects company_id, asof_date, weight...
finw = pd.read_parquet(path_wide)          # has company_id, date, RevenueStandard/NetIncomeStandard, maybe ticker
co   = pd.read_parquet(path_co)            # has company_id, maybe ticker/name/sector/industry

# Ensure datetimes
finw["date"] = pd.to_datetime(finw["date"])
hold["asof_date"] = pd.to_datetime(hold["asof_date"])

# --- Pick most recent financial period per company <= portfolio as-of max
asof_max = hold["asof_date"].max()
finw_recent = (finw[finw["date"] <= asof_max]
               .sort_values(["company_id","date"])
               .groupby("company_id", as_index=False)
               .tail(1)
               .copy())

# Add company meta if missing from finw
meta_want = ["ticker","name","sector","industry"]
meta_to_add = [c for c in meta_want if c not in finw_recent.columns and c in co.columns]
if meta_to_add:
    finw_recent = finw_recent.merge(safe_select(co, ["company_id"] + meta_to_add),
                                    on="company_id", how="left")

# Build the final dataset (only keep columns that exist)
metric_cols = [c for c in ["RevenueStandard","NetIncomeStandard"] if c in finw_recent.columns]
latest_fin = safe_select(finw_recent, ["company_id","date"] + metric_cols + [c for c in meta_want if c in finw_recent.columns])
latest_fin = latest_fin.rename(columns={"date":"fin_period_end"})

# Merge onto holdings
hold_fin = hold.merge(latest_fin, on="company_id", how="left", suffixes=("","_fin"))

# Save
hold_fin.to_parquet(out_path, index=False)
print(f"✅ Wrote: {out_path} | rows: {len(hold_fin)}")

# Peek
hold_fin.head(10)
# Compute TTM Revenue and attach to holdings (robust to FY-only data)
finw = pd.read_parquet(path_wide).copy()
finw["date"] = pd.to_datetime(finw["date"])

rev = finw[["company_id","date","period_type","RevenueStandard"]].dropna()
q = rev[rev["period_type"].str.upper().eq("QTR")].sort_values(["company_id","date"]).copy()
a = rev[rev["period_type"].str.upper().eq("FY")].sort_values(["company_id","date"]).copy()

# Rolling 4q TTM
q["rev_ttm_from_quarters"] = q.groupby("company_id")["RevenueStandard"]\
                               .rolling(4, min_periods=4).sum().reset_index(level=0, drop=True)

# Latest FY as fallback
a["rev_fy_latest"] = a.groupby("company_id")["RevenueStandard"].cummax()

# Align on dates (inner/outer union), then choose quarters TTM else FY
ttm = (q[["company_id","date","rev_ttm_from_quarters"]]
       .merge(a[["company_id","date","rev_fy_latest"]], on=["company_id","date"], how="outer")
       .sort_values(["company_id","date"]))
ttm["revenue_ttm"] = ttm["rev_ttm_from_quarters"].fillna(ttm["rev_fy_latest"])

# For each company, pick latest TTM <= holdings as-of max
asof_max = pd.read_parquet(path_hold)["asof_date"].pipe(pd.to_datetime).max()
ttm_latest = (ttm[ttm["date"] <= asof_max]
              .groupby("company_id", as_index=False)
              .tail(1)[["company_id","revenue_ttm"]])

# Attach to holdings-with-financials and save
hold_fin = pd.read_parquet(out_path)
hold_fin_ttm = hold_fin.merge(ttm_latest, on="company_id", how="left")
out_ttm = os.path.join(PARQ, "vw_holdings_with_ttm.parquet")
hold_fin_ttm.to_parquet(out_ttm, index=False)
print(f"✅ Wrote: {out_ttm} | rows: {len(hold_fin_ttm)}")
hold_fin_ttm.head(10)
import os, json, pandas as pd
from glob import glob
from datetime import datetime

PROJ = r"C:\Users\VTN183\OneDrive - University of Tennessee\smilefund_project"
PARQ = os.path.join(PROJ, "warehouse", "parquet")
FACTS = os.path.join(PROJ, "data", "sec", "company_facts")
os.makedirs(PARQ, exist_ok=True)

# ---------- load or initialize dims/fact ----------
def read_or_empty(path, cols, dtypes):
    if os.path.exists(path):
        return pd.read_parquet(path)
    df = pd.DataFrame(columns=cols).astype(dtypes)
    df.to_parquet(path, index=False)
    return df

path_company = os.path.join(PARQ, "dim_company.parquet")
path_metric  = os.path.join(PARQ, "dim_metric.parquet")
path_unit    = os.path.join(PARQ, "dim_unit.parquet")
path_cal     = os.path.join(PARQ, "dim_calendar.parquet")
path_filing  = os.path.join(PARQ, "dim_filing.parquet")
path_fact    = os.path.join(PARQ, "fact_financials.parquet")

dim_company = read_or_empty(path_company,
    ["company_id","cik","name","ticker","sector","industry","fiscal_year_end_month"],
    {"company_id":"int64","cik":"string","name":"string","ticker":"string",
     "sector":"string","industry":"string","fiscal_year_end_month":"float64"}
)
dim_metric = pd.read_parquet(path_metric) if os.path.exists(path_metric) else pd.DataFrame(
    columns=["metric_id","metric_name","xbrl_tag","metric_group","normal_balance"]
).astype({"metric_id":"int64","metric_name":"string","xbrl_tag":"string","metric_group":"string","normal_balance":"string"})
dim_unit   = read_or_empty(path_unit,
    ["unit_id","unit_code","category","iso_currency","decimals_hint","description"],
    {"unit_id":"int64","unit_code":"string","category":"string","iso_currency":"string",
     "decimals_hint":"float64","description":"string"}
)
dim_calendar = read_or_empty(path_cal,
    ["calendar_id","date","fiscal_year","fiscal_quarter","fiscal_month","period_type"],
    {"calendar_id":"int64","date":"string","fiscal_year":"int64","fiscal_quarter":"string",
     "fiscal_month":"int64","period_type":"string"}
)
dim_filing = read_or_empty(path_filing,
    ["filing_id","form_type","filing_date","accepted_date","period_end_date","accession_number","filing_url"],
    {"filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
     "period_end_date":"string","accession_number":"string","filing_url":"string"}
)
fact_fin = read_or_empty(path_fact,
    ["financial_id","company_id","metric_id","calendar_id","value","unit_id","filing_id","consolidated_flag"],
    {"financial_id":"int64","company_id":"int64","metric_id":"int64","calendar_id":"int64",
     "value":"float64","unit_id":"int64","filing_id":"int64","consolidated_flag":"string"}
)

# ---------- ensure-* helpers (in-cell so it runs standalone) ----------
def ensure_company_by_cik(cik_str, name=""):
    global dim_company
    hit = dim_company.loc[dim_company["cik"]==cik_str]
    if not hit.empty:
        return int(hit.iloc[0]["company_id"])
    next_id = (dim_company["company_id"].max()+1) if len(dim_company) else 1
    row = {"company_id":next_id,"cik":cik_str,"name":name or "", "ticker":pd.NA,
           "sector":pd.NA,"industry":pd.NA,"fiscal_year_end_month":pd.NA}
    dim_company = pd.concat([dim_company, pd.DataFrame([row])], ignore_index=True)
    dim_company.to_parquet(path_company, index=False)
    return next_id

def ensure_metric_by_tag(xbrl_tag, metric_name, metric_group, normal_balance):
    global dim_metric
    hit = dim_metric.loc[dim_metric["xbrl_tag"]==xbrl_tag]
    if not hit.empty:
        return int(hit.iloc[0]["metric_id"])
    next_id = (dim_metric["metric_id"].max()+1) if len(dim_metric) else 1
    row = {"metric_id":next_id,"metric_name":metric_name,"xbrl_tag":xbrl_tag,
           "metric_group":metric_group,"normal_balance":normal_balance}
    dim_metric = pd.concat([dim_metric, pd.DataFrame([row])], ignore_index=True)
    dim_metric.to_parquet(path_metric, index=False)
    return next_id

def ensure_unit(unit_code):
    global dim_unit
    hit = dim_unit.loc[dim_unit["unit_code"]==unit_code]
    if not hit.empty:
        return int(hit.iloc[0]["unit_id"])
    next_id = (dim_unit["unit_id"].max()+1) if len(dim_unit) else 1
    row = {"unit_id":next_id,"unit_code":unit_code,"category":"currency",
           "iso_currency":unit_code if len(unit_code)==3 else "USD","decimals_hint":pd.NA,"description":""}
    dim_unit = pd.concat([dim_unit, pd.DataFrame([row])], ignore_index=True)
    dim_unit.to_parquet(path_unit, index=False)
    return next_id

def ensure_calendar(period_end, fy, fp, period_type):
    """fp like 'FY','Q1'...'Q4'. fiscal_month from period_end."""
    global dim_calendar
    fiscal_month = int(period_end[5:7])
    hit = dim_calendar.loc[(dim_calendar["date"]==period_end) & (dim_calendar["period_type"]==period_type)]
    if not hit.empty:
        return int(hit.iloc[0]["calendar_id"])
    next_id = (dim_calendar["calendar_id"].max()+1) if len(dim_calendar) else 1
    row = {"calendar_id":next_id,"date":period_end,"fiscal_year":int(fy),
           "fiscal_quarter":fp if period_type=="QTR" else "Q4",
           "fiscal_month":fiscal_month,"period_type":period_type}
    dim_calendar = pd.concat([dim_calendar, pd.DataFrame([row])], ignore_index=True)
    dim_calendar.to_parquet(path_cal, index=False)
    return next_id

def ensure_filing(form_type, filed_date, period_end, accession=""):
    global dim_filing
    hit = dim_filing.loc[
        (dim_filing["form_type"]==form_type) &
        (dim_filing["filing_date"]==filed_date) &
        (dim_filing["period_end_date"]==period_end)
    ]
    if accession:
        hit = dim_filing.loc[dim_filing["accession_number"]==accession] if hit.empty else hit
    if not hit.empty:
        return int(hit.iloc[0]["filing_id"])
    next_id = (dim_filing["filing_id"].max()+1) if len(dim_filing) else 1
    row = {"filing_id":next_id,"form_type":form_type or "",
           "filing_date":filed_date or period_end,"accepted_date":filed_date or period_end,
           "period_end_date":period_end,"accession_number":accession or "","filing_url":""}
    dim_filing = pd.concat([dim_filing, pd.DataFrame([row])], ignore_index=True)
    dim_filing.to_parquet(path_filing, index=False)
    return next_id

# ---------- metric families we’ll ingest ----------
REVENUE_TAGS = [
    "Revenues",
    "RevenueFromContractWithCustomerExcludingAssessedTax",
    "SalesRevenueNet","SalesRevenueGoodsNet","SalesRevenueServicesNet",
    "TotalRevenuesAndOtherIncome","PremiumsEarnedNet","InterestIncomeOperating"
]
INCOME_TAGS = [
    "NetIncomeLoss","ProfitLoss",
    "IncomeLossFromContinuingOperations",
    "NetIncomeLossAvailableToCommonStockholdersBasic",
    "NetIncomeLossAttributableToParent"
]

def rows_from_tag(obj):
    """Return list of rows for preferred unit (USD if present else first unit)."""
    units = obj.get("units", {})
    if not units: 
        return None, pd.DataFrame()
    unit_key = "USD" if "USD" in units else next(iter(units.keys()))
    return unit_key, pd.DataFrame(units[unit_key])

def fp_to_quarter(fp):
    fp = (fp or "").upper()
    if fp in ["Q1","Q2","Q3","Q4"]: return fp, "QTR"
    if fp in ["FY",""]: return "Q4", "FY"
    return "Q4","FY"

# ---------- iterate all JSONs ----------
json_files = sorted(glob(os.path.join(FACTS, "CIK*.json")))
print(f"Found {len(json_files):,} company_facts files")

new_rows = []

for i, path in enumerate(json_files, 1):
    try:
        with open(path, "r") as f:
            data = json.load(f)
    except Exception as e:
        print("!! error reading", path, e); 
        continue

    cik10 = str(data.get("cik","")).zfill(10)
    entity = data.get("entityName","")
    co_id = ensure_company_by_cik(cik10, entity)

    usgaap = data.get("facts", {}).get("us-gaap", {})

    # Collect both families present for this company
    families = [
        ("Revenue", "Credit", REVENUE_TAGS),
        ("Net Income", "Credit", INCOME_TAGS)
    ]

    for metric_name, normal_balance, tag_list in families:
        # pick the first tag that exists in this file; we will still store the real tag we used
        tag_used = next((t for t in tag_list if t in usgaap), None)
        if not tag_used:
            continue

        unit_key, df = rows_from_tag(usgaap[tag_used])
        if df.empty: 
            continue

        # Keep only periodized rows that have fy, fp, end, val
        cols_need = {"fy","fp","end","val"}
        df = df[[c for c in df.columns if c in cols_need.union({"form","accn","filed"})]]

        for _, r in df.iterrows():
            fy = r.get("fy")
            fp = r.get("fp")
            end = r.get("end")
            val = r.get("val")
            if pd.isna(fy) or pd.isna(fp) or pd.isna(end) or pd.isna(val):
                continue

            fp_norm, period_type = fp_to_quarter(fp)
            met_id = ensure_metric_by_tag(tag_used, metric_name=metric_name,
                                          metric_group=("Revenue" if metric_name=="Revenue" else "Income"),
                                          normal_balance=normal_balance)
            unit_id = ensure_unit(unit_key)
            cal_id  = ensure_calendar(end, int(fy), fp_norm, period_type)
            fil_id  = ensure_filing(r.get("form",""), r.get("filed", end), end, r.get("accn",""))

            new_rows.append({
                "company_id": co_id,
                "metric_id":  met_id,
                "calendar_id": cal_id,
                "value": float(val),
                "unit_id": unit_id,
                "filing_id": fil_id,
                "consolidated_flag": "Consolidated"
            })

    if i % 250 == 0:
        print(f"...processed {i:,}/{len(json_files):,}")

# Append & de-duplicate by natural key
if new_rows:
    incoming = pd.DataFrame(new_rows)
    if len(fact_fin):
        start_rows = len(fact_fin)
        fact_fin = pd.concat([fact_fin.drop(columns=[], errors="ignore"), incoming], ignore_index=True)
    else:
        start_rows = 0
        fact_fin = incoming.copy()

    natural_key = ["company_id","metric_id","calendar_id","filing_id"]
    fact_fin = fact_fin.drop_duplicates(subset=natural_key, keep="last").reset_index(drop=True)
    fact_fin["financial_id"] = range(1, len(fact_fin)+1)
    fact_fin.to_parquet(path_fact, index=False)
    print(f"✅ Ingested {len(fact_fin)-start_rows:,} new fact rows; total = {len(fact_fin):,}")
else:
    print("No new rows found.")
import os, time, pandas as pd
# === SMILEFund: Fast SEC ingest (Threaded, vectorized, local SSD, Windows-safe) ===
import os, glob, time, warnings, pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

warnings.filterwarnings("ignore", category=FutureWarning)

# ---------- FAST JSON LOADER ----------
try:
    import orjson as _json; loads = _json.loads
except Exception:
    import json as _json; loads = _json.loads
print("JSON loader:", loads.__module__)

# ---------- PATHS ----------
PROJ  = r"C:\smilefund_project"
PARQ  = os.path.join(PROJ, "warehouse", "parquet")
FACTS = r"C:\smilefund_project\data\sec\company_facts"
TMP_RAW = os.path.join(PARQ, "_raw_deltas")
os.makedirs(PARQ, exist_ok=True)
os.makedirs(TMP_RAW, exist_ok=True)
print("FACTS exists:", os.path.exists(FACTS), "| PARQ exists:", os.path.exists(PARQ))

# ---------- HELPERS ----------
def is_money_unit(u: str) -> bool:
    u = (u or "").upper()
    return (u in {"USD","USDM","USDTH","USD$"} or (len(u)==3 and u.isalpha()))

def parse_one(path):
    with open(path, "rb") as f:
        data = loads(f.read())
    out = []
    cik10 = str(data.get("cik","")).zfill(10)
    name  = data.get("entityName","")
    usgaap = (data.get("facts") or {}).get("us-gaap", {})
    for tag, obj in usgaap.items():
        units = (obj or {}).get("units", {})
        if not units:
            continue
        unit_key = next((u for u in units.keys() if is_money_unit(u)), None)
        if not unit_key:
            continue
        for r in obj["units"][unit_key]:
            fy, fp, end, val = r.get("fy"), (r.get("fp") or ""), r.get("end"), r.get("val")
            if fy is None or not fp or not end or val is None:
                continue
            out.append({
                "cik": cik10,
                "company_name": name,
                "xbrl_tag": tag,
                "unit_code": unit_key,
                "fy": int(fy),
                "fp": fp.upper(),
                "period_end": end,
                "value": float(val),
                "form_type": r.get("form",""),
                "filed_date": r.get("filed", end),
                "accession": r.get("accn","")
            })
    return out

def read_or_empty(path, cols, dtypes):
    if os.path.exists(path): 
        return pd.read_parquet(path)
    df = pd.DataFrame(columns=cols).astype(dtypes)
    df.to_parquet(path, index=False)
    return df

# ---------- PHASE A: Threaded scan → raw parquet deltas ----------
files = sorted(glob.glob(os.path.join(FACTS, "CIK*.json")))
print(f"Scanning {len(files):,} JSONs with threads …", flush=True)

MAX_WORKERS = min(16, (os.cpu_count() or 12))   # try 12–16
BATCH       = 2000                               # write every ~2000 files
PRINT_SECS  = 8

start = time.time()
last  = start
delta_paths = []
rows_total  = 0

for k in range(0, len(files), BATCH):
    batch = files[k:k+BATCH]
    recs  = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futs = {ex.submit(parse_one, p): p for p in batch}
        done = 0
        for fut in as_completed(futs):
            recs.extend(fut.result())
            done += 1
            now = time.time()
            if (now - last) >= PRINT_SECS or done == len(batch):
                overall = k + done
                rate = overall / max(1e-6, (now - start))
                eta  = (len(files) - overall) / max(rate, 1e-6)
                print(f"...{overall:,}/{len(files):,} files | rate: {rate:,.1f}/s | "
                      f"elapsed: {(now-start)/60:,.1f}m | ETA: {eta/60:,.1f}m", flush=True)
                last = now

    out_path = os.path.join(TMP_RAW, f"raw_{k+1:06d}_{k+len(batch):06d}.parquet")
    pd.DataFrame.from_records(recs).to_parquet(out_path, index=False)
    delta_paths.append(out_path)
    rows_total += len(recs)
    print(f"💾 wrote raw delta: {os.path.basename(out_path)} (+{len(recs):,} rows)", flush=True)

print(f"✅ Phase A complete: {len(delta_paths)} raw deltas, {rows_total:,} rows")

# ---------- PHASE B: Vectorized build → dims + fact_financials ----------
path_company = os.path.join(PARQ, "dim_company.parquet")
path_metric  = os.path.join(PARQ, "dim_metric.parquet")
path_unit    = os.path.join(PARQ, "dim_unit.parquet")
path_cal     = os.path.join(PARQ, "dim_calendar.parquet")
path_filing  = os.path.join(PARQ, "dim_filing.parquet")
path_fact    = os.path.join(PARQ, "fact_financials.parquet")

dim_company = read_or_empty(path_company,
    ["company_id","cik","name","ticker","sector","industry","fiscal_year_end_month"],
    {"company_id":"int64","cik":"string","name":"string","ticker":"string","sector":"string","industry":"string","fiscal_year_end_month":"float64"})
dim_metric = read_or_empty(path_metric,
    ["metric_id","metric_name","xbrl_tag","metric_group","normal_balance"],
    {"metric_id":"int64","metric_name":"string","xbrl_tag":"string","metric_group":"string","normal_balance":"string"})
dim_unit = read_or_empty(path_unit,
    ["unit_id","unit_code","category","iso_currency","decimals_hint","description"],
    {"unit_id":"int64","unit_code":"string","category":"string","iso_currency":"string","decimals_hint":"float64","description":"string"})
dim_calendar = read_or_empty(path_cal,
    ["calendar_id","date","fiscal_year","fiscal_quarter","fiscal_month","period_type"],
    {"calendar_id":"int64","date":"string","fiscal_year":"int64","fiscal_quarter":"string","fiscal_month":"int64","period_type":"string"})
dim_filing = read_or_empty(path_filing,
    ["filing_id","form_type","filing_date","accepted_date","period_end_date","accession_number","filing_url"],
    {"filing_id":"int64","form_type":"string","filing_date":"string","accepted_date":"string",
     "period_end_date":"string","accession_number":"string","filing_url":"string"})

# Load all raw deltas
raw_parts = [pd.read_parquet(p) for p in sorted(delta_paths)]
raw = pd.concat(raw_parts, ignore_index=True) if raw_parts else pd.DataFrame(columns=[
    "cik","company_name","xbrl_tag","unit_code","fy","fp","period_end","value","form_type","filed_date","accession"
])

# --- Company (bulk)
new_co = raw[["cik","company_name"]].drop_duplicates().rename(columns={"company_name":"name"})
dim_company = dim_company.drop_duplicates(subset=["cik"])
missing_co = new_co[~new_co["cik"].isin(dim_company["cik"])]
if not missing_co.empty:
    start_id = int(dim_company["company_id"].max() or 0) + 1
    missing_co = missing_co.reset_index(drop=True)
    missing_co.insert(0, "company_id", range(start_id, start_id + len(missing_co)))
    for col, val in [("ticker",""),("sector",""),("industry",""),("fiscal_year_end_month",pd.NA)]:
        missing_co[col] = val
    dim_company = pd.concat([dim_company, missing_co], ignore_index=True)
dim_company.to_parquet(path_company, index=False)

# --- Metric (bulk)
missing_met = raw[["xbrl_tag"]].drop_duplicates()
missing_met = missing_met[~missing_met["xbrl_tag"].isin(dim_metric["xbrl_tag"])]
if not missing_met.empty:
    start = int(dim_metric["metric_id"].max() or 0) + 1
    add = pd.DataFrame({
        "metric_id": range(start, start+len(missing_met)),
        "metric_name": missing_met["xbrl_tag"].astype("string"),
        "xbrl_tag": missing_met["xbrl_tag"].astype("string"),
        "metric_group": "Auto",
        "normal_balance": ""
    })
    dim_metric = pd.concat([dim_metric, add], ignore_index=True)
dim_metric.to_parquet(path_metric, index=False)

# --- Unit (bulk)
missing_unit = raw[["unit_code"]].drop_duplicates()
missing_unit = missing_unit[~missing_unit["unit_code"].isin(dim_unit["unit_code"])]
if not missing_unit.empty:
    start = int(dim_unit["unit_id"].max() or 0) + 1
    add = pd.DataFrame({
        "unit_id": range(start, start+len(missing_unit)),
        "unit_code": missing_unit["unit_code"].astype("string"),
        "category": "currency",
        "iso_currency": missing_unit["unit_code"].where(missing_unit["unit_code"].str.len()==3, "USD").astype("string"),
        "decimals_hint": pd.NA,
        "description": ""
    })
    dim_unit = pd.concat([dim_unit, add], ignore_index=True)
dim_unit.to_parquet(path_unit, index=False)

# --- Calendar (bulk)
cal = raw[["period_end","fy","fp"]].drop_duplicates()
cal["period_type"]   = cal["fp"].where(cal["fp"].isin(["Q1","Q2","Q3","Q4"]), "FY")
cal["fiscal_quarter"] = cal["fp"].where(cal["period_type"].eq("QTR"), "Q4")
cal["fiscal_month"]  = cal["period_end"].str[5:7].astype("int64")
new_cal = cal.rename(columns={"period_end":"date","fy":"fiscal_year"})
new_cal = new_cal[~new_cal.set_index(["date","period_type"]).index.isin(dim_calendar.set_index(["date","period_type"]).index)]
if not new_cal.empty:
    start = int(dim_calendar["calendar_id"].max() or 0) + 1
    new_cal = new_cal.reset_index(drop=True)
    new_cal.insert(0, "calendar_id", range(start, start+len(new_cal)))
    dim_calendar = pd.concat([dim_calendar, new_cal], ignore_index=True)
dim_calendar.to_parquet(path_cal, index=False)

# --- Filing (bulk)
fil = raw[["form_type","filed_date","period_end","accession"]].drop_duplicates()
key_old = dim_filing.assign(_k = dim_filing["form_type"].fillna("")+"|"+dim_filing["filing_date"].fillna("")+"|"+dim_filing["period_end_date"].fillna("")+"|"+dim_filing["accession_number"].fillna(""))
key_new = fil.assign(_k = fil["form_type"].fillna("")+"|"+fil["filed_date"].fillna("")+"|"+fil["period_end"].fillna("")+"|"+fil["accession"].fillna(""))
to_add = key_new[~key_new["_k"].isin(key_old["_k"])].drop(columns="_k")
if not to_add.empty:
    start = int(dim_filing["filing_id"].max() or 0) + 1
    add = to_add.rename(columns={"filed_date":"filing_date","period_end":"period_end_date","accession":"accession_number"})
    add = add.reset_index(drop=True)
    add.insert(0, "filing_id", range(start, start+len(add)))
    add["accepted_date"] = add["filing_date"]; add["filing_url"] = ""
    dim_filing = pd.concat([dim_filing, add], ignore_index=True)
dim_filing.to_parquet(path_filing, index=False)

# --- Map to IDs & write fact_financials
df = raw.merge(dim_company[["company_id","cik"]], on="cik", how="left") \
        .merge(dim_metric[["metric_id","xbrl_tag"]], on="xbrl_tag", how="left") \
        .merge(dim_unit[["unit_id","unit_code"]], on="unit_code", how="left")

cal_key = dim_calendar[["calendar_id","date","period_type"]]
tmp = raw[["period_end","fp"]].copy()
tmp["period_type"] = tmp["fp"].where(tmp["fp"].isin(["Q1","Q2","Q3","Q4"]), "FY")
df = df.merge(tmp[["period_end","period_type"]].drop_duplicates(),
              on=["period_end","period_type"], how="left") \
       .merge(cal_key, left_on=["period_end","period_type"], right_on=["date","period_type"], how="left") \
       .drop(columns=["date"])

fil_map = dim_filing[["filing_id","form_type","filing_date","period_end_date","accession_number"]]
df = df.merge(fil_map, left_on=["form_type","filed_date","period_end","accession"],
              right_on=["form_type","filing_date","period_end_date","accession_number"], how="left") \
       .drop(columns=["filing_date","period_end_date","accession_number"])

fact_cols = ["company_id","metric_id","calendar_id","value","unit_id","filing_id"]
incoming = df[fact_cols].copy()
incoming["consolidated_flag"] = "Consolidated"

path_fact = os.path.join(PARQ, "fact_financials.parquet")
if os.path.exists(path_fact):
    fact_existing = pd.read_parquet(path_fact)
else:
    fact_existing = pd.DataFrame(columns=["financial_id"] + fact_cols + ["consolidated_flag"])

nk = ["company_id","metric_id","calendar_id","filing_id"]
combined = pd.concat([fact_existing.drop(columns=[], errors="ignore"), incoming], ignore_index=True)
combined = combined.drop_duplicates(subset=nk, keep="last").reset_index(drop=True)
combined["financial_id"] = range(1, len(combined)+1)
combined.to_parquet(path_fact, index=False)

print(f"✅ Done. fact_financials rows: {len(combined):,} | unique tags: {dim_metric['xbrl_tag'].nunique():,} | elapsed: {(time.time()-start)/60:,.1f} min")

# === PHASE B+ (ALL TAGS + GROUPED FAN-OUT) ===
from pathlib import Path
import traceback
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds
import pyarrow.parquet as pq

# ---------------- CONFIG (your path) ----------------
PROJECT_ROOT = Path(r"C:\smilefund_project")
RAW_DIR      = PROJECT_ROOT / r"warehouse\parquet\_raw_deltas"
OUT_DIR      = PROJECT_ROOT / r"warehouse\parquet\facts_all"
OUT_FILE     = OUT_DIR / "facts_all.parquet"
FANOUT_DIR   = PROJECT_ROOT / r"warehouse\parquet\facts_by_group"

BATCH_SIZE     = 250_000
ROW_GROUP_SIZE = 256 * 1024
PARQUET_CODEC  = "snappy"

# -------- Tag groups (extend anytime; unmapped => 'Other') --------
TAG_GROUPS = {
    "Revenue": {
        "Revenues","RevenueFromContractWithCustomerExcludingAssessedTax",
        "SalesRevenueNet","SalesRevenueGoodsNet","SalesRevenueServicesNet",
        "SalesRevenueGoodsAndServicesNet","TotalRevenueAndOtherIncome"
    },
    "Expenses": {
        "CostOfGoodsAndServicesSold","CostOfRevenue","SellingGeneralAndAdministrativeExpense",
        "ResearchAndDevelopmentExpense","OperatingExpenses","InterestExpense","RestructuringCharges"
    },
    "Assets": {
        "Assets","AssetsCurrent","CashAndCashEquivalentsAtCarryingValue","AccountsReceivableNetCurrent",
        "InventoryNet","Goodwill","IntangibleAssetsNetExcludingGoodwill","PropertyPlantAndEquipmentNet"
    },
    "Liabilities": {
        "Liabilities","LiabilitiesCurrent","AccountsPayableCurrent","LongTermDebtNoncurrent",
        "DeferredRevenueCurrent","DeferredTaxLiabilitiesNoncurrent"
    },
    "Equity": {
        "StockholdersEquity","RetainedEarningsAccumulatedDeficit",
        "CommonStockValue","AdditionalPaidInCapital","TreasuryStockValue"
    },
    "CashFlow": {
        "NetCashProvidedByUsedInOperatingActivities","NetCashProvidedByUsedInInvestingActivities",
        "NetCashProvidedByUsedInFinancingActivities","PaymentsForRepurchaseOfCommonStock","PaymentsOfDividends"
    },
    "OCI": {"OtherComprehensiveIncomeLossNetOfTax"},
    "PerShare": {"EarningsPerShareBasic","EarningsPerShareDiluted","CommonStockDividendsPerShareDeclared"},
    "KPIs": {"CustomersNumber","EmployeesNumber","ActiveUsers","RevenuePerShare"},
}
_TAG_TO_GROUP = {t.lower(): g for g, tags in TAG_GROUPS.items() for t in tags}

def map_group_for_batch(tag_array: pa.Array) -> pa.Array:
    # strip namespaces like 'us-gaap:' and map; default 'Other'
    tag_utf8 = pc.cast(tag_array, pa.utf8())
    stripped = pc.replace_substring_regex(tag_utf8, r"^[^:]+:", "")
    lowered  = pc.ascii_lower(stripped)
    py = lowered.to_pylist()
    out = [_TAG_TO_GROUP.get(t, "Other") if t is not None else "Other" for t in py]
    return pa.array(out, type=pa.utf8())

# ---------------- Pre-flight ----------------
if not RAW_DIR.exists():
    raise FileNotFoundError(f"Raw parquet folder not found: {RAW_DIR}")

OUT_DIR.mkdir(parents=True, exist_ok=True)
FANOUT_DIR.mkdir(parents=True, exist_ok=True)

dataset = ds.dataset(str(RAW_DIR), format="parquet")
schema = dataset.schema
wanted_cols = [c for c in [
    "cik","tag","unit","fy","fp","frame","start","end",
    "filed","form","accn","val","value","decimals","qtrs","segment","uom"
] if c in schema.names]

print(f"Found {len(dataset.files)} raw chunks in: {RAW_DIR}")
print("Columns available:", wanted_cols)

# ---------------- (1) Write monolithic facts_all ----------------
writer = None
total_rows = 0
try:
    for batch in dataset.to_batches(columns=wanted_cols, batch_size=BATCH_SIZE):
        tbl = pa.Table.from_batches([batch])
        if "value" in tbl.column_names and "val" not in tbl.column_names:
            tbl = tbl.append_column("val", tbl["value"]).drop(["value"])
        # normalize numeric types (best-effort)
        for col, typ in {"cik": pa.int64(), "fy": pa.int32(), "val": pa.float64()}.items():
            if col in tbl.column_names:
                try:
                    tbl = tbl.set_column(tbl.schema.get_field_index(col), col, pc.cast(tbl[col], typ))
                except Exception:
                    pass
        if writer is None:
            writer = pq.ParquetWriter(OUT_FILE, tbl.schema, compression=PARQUET_CODEC)
        writer.write_table(tbl, row_group_size=ROW_GROUP_SIZE)
        total_rows += tbl.num_rows
        if total_rows % 1_000_000 < BATCH_SIZE:
            print(f"[facts_all] wrote {total_rows:,} rows...")
finally:
    if writer:
        writer.close()
print(f"✅ facts_all DONE — {total_rows:,} rows → {OUT_FILE}")

# ---------------- (2) Fan-out by group/tag (partitioned) ----------------
partitioning = ds.partitioning(flavor="hive")  # group=.../tag=...
written_rows = 0

for batch in dataset.to_batches(columns=wanted_cols, batch_size=BATCH_SIZE):
    tbl = pa.Table.from_batches([batch])
    if "value" in tbl.column_names and "val" not in tbl.column_names:
        tbl = tbl.append_column("val", tbl["value"]).drop(["value"])
    # attach group column
    if "tag" in tbl.column_names:
        group_arr = map_group_for_batch(tbl["tag"])
    else:
        group_arr = pa.array(["Other"] * tbl.num_rows, type=pa.utf8())
    tbl = tbl.append_column("group", group_arr)

    ds.write_dataset(
        data=tbl,
        base_dir=str(FANOUT_DIR),
        format="parquet",
        partitioning=partitioning,
        partitioning_cols=["group","tag"],
        existing_data_behavior="overwrite_or_ignore",  # idempotent runs
    )
    written_rows += tbl.num_rows
    if written_rows % 1_000_000 < BATCH_SIZE:
        print(f"[facts_by_group] wrote {written_rows:,} rows...")

print(f"✅ facts_by_group DONE — {written_rows:,} rows → {FANOUT_DIR}")
print(r"Example partition: facts_by_group\group=Revenue\tag=Revenues\part-*.parquet")
# ---------------- (2) Fan-out by group/tag (partitioned) ----------------
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds

# Define a partitioning schema (this replaces `partitioning_cols=...`)
partition_schema = pa.schema([
    pa.field("group", pa.utf8()),
    pa.field("tag",   pa.utf8()),
])
partitioning = ds.partitioning(partition_schema, flavor="hive")

written_rows = 0

for batch in dataset.to_batches(columns=wanted_cols, batch_size=BATCH_SIZE):
    tbl = pa.Table.from_batches([batch])

    # unify 'value' -> 'val'
    if "value" in tbl.column_names and "val" not in tbl.column_names:
        tbl = tbl.append_column("val", tbl["value"]).drop(["value"])

    # ensure 'tag' is utf8 (required for partitioning)
    if "tag" in tbl.column_names:
        tag_utf8 = pc.cast(tbl["tag"], pa.utf8())
        tbl = tbl.set_column(tbl.column_names.index("tag"), "tag", tag_utf8)
        # add 'group' derived from tag
        group_arr = map_group_for_batch(tag_utf8)
    else:
        group_arr = pa.array(["Other"] * tbl.num_rows, type=pa.utf8())

    tbl = tbl.append_column("group", group_arr)

    ds.write_dataset(
        data=tbl,
        base_dir=str(FANOUT_DIR),
        format="parquet",
        partitioning=partitioning,                     # <-- use schema-based partitioning
        existing_data_behavior="overwrite_or_ignore",  # safe to re-run
    )

    written_rows += tbl.num_rows
    if written_rows % 1_000_000 < BATCH_SIZE:
        print(f"[facts_by_group] wrote {written_rows:,} rows...")

print(f"✅ facts_by_group DONE — {written_rows:,} rows → {FANOUT_DIR}")
# --------- Robust fan-out: tolerate missing 'tag' in some batches ---------
from pathlib import Path
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds
import pyarrow.parquet as pq

PROJECT_ROOT = Path(r"C:\smilefund_project")
RAW_DIR      = PROJECT_ROOT / r"warehouse\parquet\_raw_deltas"
FANOUT_DIR   = PROJECT_ROOT / r"warehouse\parquet\facts_by_group_v2"
FANOUT_DIR.mkdir(parents=True, exist_ok=True)

# --- same TAG_GROUPS and _TAG_TO_GROUP as before ---
TAG_GROUPS = {
    "Revenue": {"Revenues","RevenueFromContractWithCustomerExcludingAssessedTax",
                "SalesRevenueNet","SalesRevenueGoodsNet","SalesRevenueServicesNet",
                "SalesRevenueGoodsAndServicesNet","TotalRevenueAndOtherIncome"},
    "Expenses": {"CostOfGoodsAndServicesSold","CostOfRevenue","SellingGeneralAndAdministrativeExpense",
                 "ResearchAndDevelopmentExpense","OperatingExpenses","InterestExpense","RestructuringCharges"},
    "Assets": {"Assets","AssetsCurrent","CashAndCashEquivalentsAtCarryingValue","AccountsReceivableNetCurrent",
               "InventoryNet","Goodwill","IntangibleAssetsNetExcludingGoodwill","PropertyPlantAndEquipmentNet"},
    "Liabilities": {"Liabilities","LiabilitiesCurrent","AccountsPayableCurrent","LongTermDebtNoncurrent",
                    "DeferredRevenueCurrent","DeferredTaxLiabilitiesNoncurrent"},
    "Equity": {"StockholdersEquity","RetainedEarningsAccumulatedDeficit","CommonStockValue",
               "AdditionalPaidInCapital","TreasuryStockValue"},
    "CashFlow": {"NetCashProvidedByUsedInOperatingActivities","NetCashProvidedByUsedInInvestingActivities",
                 "NetCashProvidedByUsedInFinancingActivities","PaymentsForRepurchaseOfCommonStock","PaymentsOfDividends"},
    "OCI": {"OtherComprehensiveIncomeLossNetOfTax"},
    "PerShare": {"EarningsPerShareBasic","EarningsPerShareDiluted","CommonStockDividendsPerShareDeclared"},
    "KPIs": {"CustomersNumber","EmployeesNumber","ActiveUsers","RevenuePerShare"},
}
_TAG_TO_GROUP = {t.lower(): g for g, tags in TAG_GROUPS.items() for t in tags}

def map_group_for_batch(tag_arr: pa.Array) -> pa.Array:
    # tag_arr is guaranteed utf8 by caller
    stripped = pc.replace_substring_regex(tag_arr, r"^[^:]+:", "")  # drop namespaces e.g. 'us-gaap:'
    lowered  = pc.ascii_lower(stripped)
    py = lowered.to_pylist()
    out = [_TAG_TO_GROUP.get(t, "Other") if t is not None else "Other" for t in py]
    return pa.array(out, type=pa.utf8())

# Build dataset; we’ll request the columns we care about
dataset = ds.dataset(str(RAW_DIR), format="parquet")
wanted_cols = [c for c in [
    "cik","tag","unit","fy","fp","frame","start","end",
    "filed","form","accn","val","value","decimals","qtrs","segment","uom"
] if c in dataset.schema.names]

for i, batch in enumerate(dataset.to_batches(columns=wanted_cols, batch_size=250_000), start=1):
    tbl = pa.Table.from_batches([batch])

    # unify 'value' -> 'val'
    if "value" in tbl.column_names and "val" not in tbl.column_names:
        tbl = tbl.append_column("val", tbl["value"]).drop(["value"])

    # ensure 'tag' column exists and is utf8
    if "tag" not in tbl.column_names:
        # create empty tag column to satisfy partitioner
        tag_utf8 = pa.array([None] * tbl.num_rows, type=pa.utf8())
        tbl = tbl.append_column("tag", tag_utf8)
        # with no tags, whole batch goes to group 'Other'
        group_arr = pa.array(["Other"] * tbl.num_rows, type=pa.utf8())
    else:
        tag_utf8 = pc.cast(tbl["tag"], pa.utf8())
        tbl = tbl.set_column(tbl.column_names.index("tag"), "tag", tag_utf8)
        group_arr = map_group_for_batch(tag_utf8)

    # append group column
    tbl = tbl.append_column("group", group_arr)

    # write to hive partitions group=/tag=
    pq.write_to_dataset(
        table=tbl,
        root_path=str(FANOUT_DIR),
        partition_cols=["group", "tag"],
        compression="snappy",
        existing_data_behavior="overwrite_or_ignore",
    )

    if i % 20 == 0:
        print(f"[fanout] processed {i} batches...")

print("✅ facts_by_group_v2 written at:", FANOUT_DIR)
import duckdb, os, pandas as pd

FANOUT_DIR = r"C:\smilefund_project\warehouse\parquet\facts_by_group_v2"
OUT_CSV    = r"C:\smilefund_project\warehouse\reports\top_other_tags.csv"
os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)

con = duckdb.connect()
# Read all partitioned files (hive: group=/tag= folders create columns)
con.execute(f"""
    WITH base AS (
      SELECT "group", tag
      FROM parquet_scan('{FANOUT_DIR}\\**\\*.parquet', HIVE_PARTITIONING=1)
    )
    SELECT tag, COUNT(*) AS n
    FROM base
    WHERE "group" = 'Other' AND tag IS NOT NULL AND tag <> ''
    GROUP BY 1
    ORDER BY n DESC
    LIMIT 500
""")
df = con.fetch_df()
con.close()

print(df.head(20))
df.to_csv(OUT_CSV, index=False)
print("✅ wrote:", OUT_CSV)
pip install duckdb
# === AUTO-GROUP TAGS BY PATTERN + RE-FAN-OUT TO v3 ===
from pathlib import Path
import re
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.dataset as ds
import pyarrow.parquet as pq

PROJECT_ROOT = Path(r"C:\smilefund_project")
FACTS_ALL    = PROJECT_ROOT / r"warehouse\parquet\facts_all\facts_all.parquet"
RAW_DIR      = PROJECT_ROOT / r"warehouse\parquet\_raw_deltas"  # we stream from raw again (fast, low RAM)
OUT_V3       = PROJECT_ROOT / r"warehouse\parquet\facts_by_group_v3"
OUT_V3.mkdir(parents=True, exist_ok=True)

# 1) Base curated groups you already used
TAG_GROUPS = {
    "Revenue": {
        "Revenues","RevenueFromContractWithCustomerExcludingAssessedTax",
        "SalesRevenueNet","SalesRevenueGoodsNet","SalesRevenueServicesNet",
        "SalesRevenueGoodsAndServicesNet","TotalRevenueAndOtherIncome"
    },
    "Expenses": {
        "CostOfGoodsAndServicesSold","CostOfRevenue","SellingGeneralAndAdministrativeExpense",
        "ResearchAndDevelopmentExpense","OperatingExpenses","InterestExpense","RestructuringCharges"
    },
    "Assets": {
        "Assets","AssetsCurrent","CashAndCashEquivalentsAtCarryingValue","AccountsReceivableNetCurrent",
        "InventoryNet","Goodwill","IntangibleAssetsNetExcludingGoodwill","PropertyPlantAndEquipmentNet"
    },
    "Liabilities": {
        "Liabilities","LiabilitiesCurrent","AccountsPayableCurrent","LongTermDebtNoncurrent",
        "DeferredRevenueCurrent","DeferredTaxLiabilitiesNoncurrent"
    },
    "Equity": {
        "StockholdersEquity","RetainedEarningsAccumulatedDeficit",
        "CommonStockValue","AdditionalPaidInCapital","TreasuryStockValue"
    },
    "CashFlow": {
        "NetCashProvidedByUsedInOperatingActivities","NetCashProvidedByUsedInInvestingActivities",
        "NetCashProvidedByUsedInFinancingActivities","PaymentsForRepurchaseOfCommonStock","PaymentsOfDividends"
    },
    "OCI": {"OtherComprehensiveIncomeLossNetOfTax"},
    "PerShare": {"EarningsPerShareBasic","EarningsPerShareDiluted","CommonStockDividendsPerShareDeclared"},
    "KPIs": {"CustomersNumber","EmployeesNumber","ActiveUsers","RevenuePerShare"},
}
BASE = {t.lower(): g for g, tags in TAG_GROUPS.items() for t in tags}

# 2) Regex pattern rules (order matters — first match wins)
PATTERN_RULES = [
    ("Revenue",   r"(revenue|sales)(?!per|turnover)"),   # revenue-ish
    ("Expenses",  r"(expense|costof|costs?)"),
    ("Assets",    r"(^assets?$|asset)"),
    ("Liabilities", r"(liabilit)"),
    ("Equity",    r"(equity|retainedearnings|paidincapital|treasurystock)"),
    ("CashFlow",  r"(netcash|operatingactivit|investingactivit|financingactivit|dividend|repurchase)"),
    ("OCI",       r"(othercomprehensiveincome)"),
    ("PerShare",  r"(perShare|earningspershare|dividendsPerShare)"),
    ("KPIs",      r"(customersnumber|employeesnumber|activeusers)"),
    # Income statements specifics:
    ("Expenses",  r"(cogs|costofgoods)"),
    ("Revenue",   r"(totalrevenueandotherincome)"),
]

# 3) Build a fast tag -> group mapper
compiled = [(g, re.compile(pat, re.IGNORECASE)) for g, pat in PATTERN_RULES]

def infer_group(tag_utf8: str) -> str:
    if not tag_utf8:
        return "Other"
    # strip namespace like 'us-gaap:'
    base = tag_utf8.split(":", 1)[-1]
    # 3a) curated base
    if base.lower() in BASE:
        return BASE[base.lower()]
    # 3b) regex rules
    for g, rx in compiled:
        if rx.search(base):
            return g
    return "Other"

# 4) Re-fan-out from raw parquet with new inferred groups
dataset = ds.dataset(str(RAW_DIR), format="parquet")
wanted = [c for c in [
    "cik","tag","unit","fy","fp","frame","start","end",
    "filed","form","accn","val","value","decimals","qtrs","segment","uom"
] if c in dataset.schema.names]

# Wipe v3 if re-running from scratch (optional)
# import shutil; shutil.rmtree(OUT_V3, ignore_errors=True); OUT_V3.mkdir(parents=True, exist_ok=True)

bnum = 0
for batch in dataset.to_batches(columns=wanted, batch_size=250_000):
    bnum += 1
    tbl = pa.Table.from_batches([batch])

    # unify value->val
    if "value" in tbl.column_names and "val" not in tbl.column_names:
        tbl = tbl.append_column("val", tbl["value"]).drop(["value"])

    # ensure 'tag' exists
    if "tag" not in tbl.column_names:
        tag_col = pa.array([None] * tbl.num_rows, type=pa.utf8())
        tbl = tbl.append_column("tag", tag_col)
    else:
        tag_col = pc.cast(tbl["tag"], pa.utf8())
        tbl = tbl.set_column(tbl.column_names.index("tag"), "tag", tag_col)

    # vectorized-ish map to group
    py_tags = tbl["tag"].to_pylist()
    groups = [infer_group(t or "") for t in py_tags]
    group_arr = pa.array(groups, type=pa.utf8())
    tbl = tbl.append_column("group", group_arr)

    # write to hive partitions group=/tag=
    pq.write_to_dataset(
        table=tbl,
        root_path=str(OUT_V3),
        partition_cols=["group", "tag"],
        compression="snappy",
        existing_data_behavior="overwrite_or_ignore",
    )

    if bnum % 20 == 0:
        print(f"[v3] processed {bnum} batches...")

print("✅ facts_by_group_v3 at:", OUT_V3)
from pathlib import Path
import json
import pyarrow as pa, pyarrow.parquet as pq, pyarrow.dataset as ds, pyarrow.compute as pc

PROJECT_ROOT = Path(r"C:\smilefund_project")
TICKERS_JSON = PROJECT_ROOT / r"data\sec\company_tickers.json"
FACTS_ALL   = PROJECT_ROOT / r"warehouse\parquet\facts_all\facts_all.parquet"
OUT_DIR     = PROJECT_ROOT / r"warehouse\parquet\dimensions"
OUT_DIR.mkdir(parents=True, exist_ok=True)
OUT_FILE    = OUT_DIR / "dim_company.parquet"

if TICKERS_JSON.exists():
    data = json.loads(TICKERS_JSON.read_text(encoding="utf-8"))
    rows = []
    for rec in data.values():
        try:
            rows.append({
                "cik": int(rec.get("cik_str")),
                "ticker": rec.get("ticker"),
                "name": rec.get("title"),
            })
        except Exception:
            pass
    tbl = pa.Table.from_pylist(rows)
    # Deduplicate by CIK (keep latest occurrence)
    # If pyarrow>=16: tbl = tbl.drop_duplicates(keys=["cik"])
    # Portable way:
    unique_cik = pc.unique(tbl["cik"])
    mask = pc.is_in(tbl["cik"], value_set=unique_cik)  # retains all, we'll just write as-is; parquet readers can distinct later
    tbl = tbl.filter(pc.invert(pc.is_null(tbl["cik"])))
    pq.write_table(tbl, str(OUT_FILE))
    print(f"✅ Wrote {OUT_FILE} with {tbl.num_rows:,} rows (from company_tickers.json)")
else:
    # Fallback: build CIK-only from facts_all
    d = ds.dataset(str(FACTS_ALL), format="parquet")
    ciks = d.to_table(columns=["cik"]).drop_null()
    unique_cik = pc.unique(ciks["cik"])
    tbl = pa.Table.from_arrays([unique_cik], names=["cik"])
    pq.write_table(tbl, str(OUT_FILE))
    print(f"⚠️ company_tickers.json not found. Wrote {OUT_FILE} with CIK only ({tbl.num_rows:,} rows).")
# Build latest snapshots from v3 using fy/fp ranking (no reliance on "end"/"filed")
import os, sys, subprocess

# Ensure duckdb
try:
    import duckdb  # type: ignore
except ModuleNotFoundError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "duckdb"])
    import duckdb  # noqa

V3   = r"C:\smilefund_project\warehouse\parquet\facts_by_group_v3\**\*.parquet"
SNAP = r"C:\smilefund_project\warehouse\parquet\snapshots"
os.makedirs(SNAP, exist_ok=True)

con = duckdb.connect()

# Use parquet_scan with HIVE_PARTITIONING=1 to expose partition cols (group, tag)
# Select only columns that we know exist across all partitions
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW facts_min AS
  SELECT
    tag,
    "group",
    cik,
    fy,
    fp,
    val
  FROM parquet_scan('{V3}', HIVE_PARTITIONING=1)
  WHERE val IS NOT NULL
""")

# Rank helper: annual/quarter ordering weight
# FY > Q4 > Q3 > Q2 > Q1 > anything else (0)
con.execute("""
  CREATE OR REPLACE TEMP VIEW facts_ranked AS
  SELECT
    *,
    CASE fp
      WHEN 'FY' THEN 5
      WHEN 'Q4' THEN 4
      WHEN 'Q3' THEN 3
      WHEN 'Q2' THEN 2
      WHEN 'Q1' THEN 1
      ELSE 0
    END AS fp_rank
  FROM facts_min
""")

# 1) Latest ANY per (cik, tag): choose highest fy, then best fp_rank
con.execute("""
  CREATE OR REPLACE TABLE latest_any AS
  SELECT *
  FROM (
    SELECT
      cik, tag, fy, fp, val,
      ROW_NUMBER() OVER (
        PARTITION BY cik, tag
        ORDER BY fy DESC, fp_rank DESC
      ) rn
    FROM facts_ranked
  )
  WHERE rn = 1
""")
con.execute(f"COPY latest_any TO '{SNAP}\\latest_any.parquet' (FORMAT PARQUET);")

# 2) Latest ANNUAL per (cik, tag, fy): keep FY only, choose best (ties unlikely)
con.execute("""
  CREATE OR REPLACE TABLE latest_annual AS
  SELECT *
  FROM (
    SELECT
      cik, tag, fy, fp, val,
      ROW_NUMBER() OVER (
        PARTITION BY cik, tag, fy
        ORDER BY fp_rank DESC
      ) rn
    FROM facts_ranked
    WHERE fp = 'FY'
  )
  WHERE rn = 1
""")
con.execute(f"COPY latest_annual TO '{SNAP}\\latest_annual.parquet' (FORMAT PARQUET);")

# 3) Latest QUARTERLY per (cik, tag, fy, fp): keep Q1–Q4, 1 row per combo
con.execute("""
  CREATE OR REPLACE TABLE latest_quarterly AS
  SELECT *
  FROM (
    SELECT
      cik, tag, fy, fp, val,
      ROW_NUMBER() OVER (
        PARTITION BY cik, tag, fy, fp
        ORDER BY fp_rank DESC
      ) rn
    FROM facts_ranked
    WHERE fp IN ('Q1','Q2','Q3','Q4')
  )
  WHERE rn = 1
""")
con.execute(f"COPY latest_quarterly TO '{SNAP}\\latest_quarterly.parquet' (FORMAT PARQUET);")

con.close()
print("✅ Snapshots written:\n -", SNAP + "\\latest_any.parquet",
      "\n -", SNAP + "\\latest_annual.parquet",
      "\n -", SNAP + "\\latest_quarterly.parquet")
# Step 3 — KPI wide (annual) from latest_annual snapshot
import os
import duckdb

SNAP_ANNUAL = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
OUT_PATH    = r"C:\smilefund_project\warehouse\parquet\marts\kpi_wide_annual.parquet"
os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)

# Choose common tags (add more later any time)
TAGS = (
    'Revenues',
    'NetIncomeLoss',
    'GrossProfit',
    'OperatingIncomeLoss',
    'EarningsPerShareDiluted',
    'Assets', 'Liabilities', 'StockholdersEquity',
    'NetCashProvidedByUsedInOperatingActivities'
)

con = duckdb.connect()
con.execute(f"CREATE OR REPLACE TEMP VIEW la AS SELECT * FROM read_parquet('{SNAP_ANNUAL}')")

# Build wide table (one row per cik & fiscal year)
con.execute(f"""
  COPY (
    WITH base AS (
      SELECT cik, fy, tag, val
      FROM la
      WHERE tag IN ({','.join([f"'{t}'" for t in TAGS])})
    ),
    pivoted AS (
      SELECT
        cik, fy,
        MAX(CASE WHEN tag='Revenues' THEN val END)                                 AS Revenues,
        MAX(CASE WHEN tag='NetIncomeLoss' THEN val END)                            AS NetIncome,
        MAX(CASE WHEN tag='GrossProfit' THEN val END)                              AS GrossProfit,
        MAX(CASE WHEN tag='OperatingIncomeLoss' THEN val END)                      AS OperatingIncome,
        MAX(CASE WHEN tag='EarningsPerShareDiluted' THEN val END)                  AS EPS_Diluted,
        MAX(CASE WHEN tag='Assets' THEN val END)                                   AS Assets,
        MAX(CASE WHEN tag='Liabilities' THEN val END)                              AS Liabilities,
        MAX(CASE WHEN tag='StockholdersEquity' THEN val END)                       AS Equity,
        MAX(CASE WHEN tag='NetCashProvidedByUsedInOperatingActivities' THEN val END) AS CFO
      FROM base
      GROUP BY cik, fy
    )
    SELECT * FROM pivoted
    ORDER BY cik, fy
  ) TO '{OUT_PATH}' (FORMAT PARQUET);
""")
con.close()

print("✅ Wrote:", OUT_PATH)
# Step 4 — YoY growth & margins on KPI wide (annual)
import os
import duckdb

WIDE = r"C:\smilefund_project\warehouse\parquet\marts\kpi_wide_annual.parquet"
OUT  = r"C:\smilefund_project\warehouse\parquet\marts\kpi_growth_annual.parquet"
os.makedirs(os.path.dirname(OUT), exist_ok=True)

con = duckdb.connect()
con.execute(f"CREATE OR REPLACE TEMP VIEW w AS SELECT * FROM read_parquet('{WIDE}')")

# Build growth/margin table
con.execute(f"""
  COPY (
    WITH base AS (
      SELECT
        cik, fy,
        Revenues,
        NetIncome,
        GrossProfit,
        OperatingIncome,
        EPS_Diluted AS EPS,
        Assets, Liabilities, Equity,
        CFO
      FROM w
    ),
    with_lag AS (
      SELECT
        *,
        LAG(Revenues) OVER (PARTITION BY cik ORDER BY fy) AS Revenues_prev,
        LAG(EPS)      OVER (PARTITION BY cik ORDER BY fy) AS EPS_prev
      FROM base
    )
    SELECT
      *,
      CASE WHEN Revenues_prev IS NOT NULL AND Revenues_prev <> 0
           THEN (Revenues - Revenues_prev) / NULLIF(Revenues_prev,0) END AS YoY_Revenue_Growth,
      CASE WHEN Revenues IS NOT NULL AND Revenues <> 0
           THEN NetIncome   / NULLIF(Revenues,0) END AS NetMargin,
      CASE WHEN Revenues IS NOT NULL AND Revenues <> 0
           THEN GrossProfit / NULLIF(Revenues,0) END AS GrossMargin,
      CASE WHEN Revenues IS NOT NULL AND Revenues <> 0
           THEN OperatingIncome / NULLIF(Revenues,0) END AS OpMargin,
      CASE WHEN EPS_prev IS NOT NULL AND EPS_prev <> 0
           THEN (EPS - EPS_prev) / NULLIF(EPS_prev,0) END AS YoY_EPS_Growth
    FROM with_lag
    ORDER BY cik, fy
  ) TO '{OUT}' (FORMAT PARQUET);
""")
con.close()

print("✅ Wrote:", OUT)
# Step 5 — lightweight helpers over your snapshots/marts
import duckdb
import pandas as pd

SNAP_ANNUAL     = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
SNAP_QUARTERLY  = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_quarterly.parquet"

con = duckdb.connect()
con.execute(f"CREATE OR REPLACE TEMP VIEW la AS SELECT * FROM read_parquet('{SNAP_ANNUAL}')")
con.execute(f"CREATE OR REPLACE TEMP VIEW lq AS SELECT * FROM read_parquet('{SNAP_QUARTERLY}')")

def list_top_tags(n: int = 50) -> pd.DataFrame:
    q = f"""
      SELECT tag, COUNT(*) AS companies
      FROM la
      GROUP BY tag
      ORDER BY companies DESC
      LIMIT {n}
    """
    return con.execute(q).fetch_df()

def top_companies_for_tag(tag: str, n: int = 20) -> pd.DataFrame:
    # Shows who reports the largest latest-annual value for this tag
    q = f"""
      SELECT cik, fy, val
      FROM la
      WHERE tag = '{tag}'
      ORDER BY val DESC NULLS LAST
      LIMIT {n}
    """
    return con.execute(q).fetch_df()

def cik_history(cik: int, tag: str) -> pd.DataFrame:
    # Combines annual + quarterly into a single sorted series
    q = f"""
      WITH annual AS (
        SELECT 'FY' AS freq, cik, fy, fp, val
        FROM la
        WHERE cik = {cik} AND tag = '{tag}'
      ),
      quarterly AS (
        SELECT 'Q' AS freq, cik, fy, fp, val
        FROM lq
        WHERE cik = {cik} AND tag = '{tag}'
      )
      SELECT * FROM annual
      UNION ALL
      SELECT * FROM quarterly
      ORDER BY fy, 
        CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 WHEN 'FY' THEN 5 ELSE 0 END
    """
    return con.execute(q).fetch_df()

print("✅ Helpers ready:")
print(" - list_top_tags(n=50)")
print(" - top_companies_for_tag('Revenues', n=20)")
print(" - cik_history(0000320193, 'Revenues')  # example CIK (Apple)")
# Step 6 — Top companies by latest-annual Revenues
import os
import duckdb
import pandas as pd

SNAP_ANNUAL = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
OUT_CSV     = r"C:\smilefund_project\warehouse\reports\top_revenues_latest_annual.csv"
os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)

con = duckdb.connect()
df = con.execute(f"""
  SELECT cik, fy, val AS Revenues
  FROM read_parquet('{SNAP_ANNUAL}')
  WHERE tag = 'Revenues'
  ORDER BY Revenues DESC NULLS LAST
  LIMIT 50
""").fetch_df()
con.close()

print(df.head(20))
df.to_csv(OUT_CSV, index=False)
print("✅ Saved:", OUT_CSV)
# Step 7 — Apple (CIK 0000320193) Revenue history (annual + quarterly)
import os, sys, subprocess
import duckdb
import pandas as pd

# Optional: ensure matplotlib present for charts
try:
    import matplotlib.pyplot as plt
except ModuleNotFoundError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "matplotlib"])
    import matplotlib.pyplot as plt

CIK = 320193  # Apple
SNAP_ANNUAL    = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
SNAP_QUARTERLY = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_quarterly.parquet"
OUT_DIR        = r"C:\smilefund_project\warehouse\reports"
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

# Annual Revenues (FY)
df_annual = con.execute(f"""
  SELECT fy, val AS Revenues
  FROM read_parquet('{SNAP_ANNUAL}')
  WHERE cik = {CIK} AND tag = 'Revenues'
  ORDER BY fy
""").fetch_df()

# Quarterly Revenues (Q1–Q4)
df_quarterly = con.execute(f"""
  SELECT fy, fp, val AS Revenues
  FROM read_parquet('{SNAP_QUARTERLY}')
  WHERE cik = {CIK} AND tag = 'Revenues' AND fp IN ('Q1','Q2','Q3','Q4')
  ORDER BY fy,
    CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE 0 END
""").fetch_df()

con.close()

# Save CSVs
annual_csv    = os.path.join(OUT_DIR, "AAPL_revenues_annual.csv")
quarterly_csv = os.path.join(OUT_DIR, "AAPL_revenues_quarterly.csv")
df_annual.to_csv(annual_csv, index=False)
df_quarterly.to_csv(quarterly_csv, index=False)
print("✅ Saved:\n -", annual_csv, "\n -", quarterly_csv)

# Charts (simple)
if not df_annual.empty:
    plt.figure(figsize=(9,4.5))
    plt.plot(df_annual["fy"], df_annual["Revenues"], marker="o")
    plt.title("Apple (CIK 320193) — Annual Revenues")
    plt.xlabel("Fiscal Year")
    plt.ylabel("Revenues")
    plt.grid(True, alpha=0.3)
    plt.show()

if not df_quarterly.empty:
    # build x-axis labels like 2023-Q1 etc.
    df_quarterly = df_quarterly.copy()
    df_quarterly["period"] = df_quarterly["fy"].astype(str) + "-" + df_quarterly["fp"]
    plt.figure(figsize=(11,4.5))
    plt.plot(df_quarterly["period"], df_quarterly["Revenues"], marker=".")
    plt.title("Apple (CIK 320193) — Quarterly Revenues")
    plt.xlabel("Fiscal Period")
    plt.ylabel("Revenues")
    plt.xticks(rotation=60)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
# Re-export Apple (CIK 320193) Revenues using normalized tag (strip 'us-gaap:' etc.)
import os, sys, subprocess, duckdb
import pandas as pd

# ensure matplotlib (for quick charts)
try:
    import matplotlib.pyplot as plt
except ModuleNotFoundError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "matplotlib"])
    import matplotlib.pyplot as plt

CIK = 320193
SNAP_ANNUAL    = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
SNAP_QUARTERLY = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_quarterly.parquet"
OUT_DIR        = r"C:\smilefund_project\warehouse\reports"
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

# Peek at how tag is spelled for this CIK (sanity)
preview = con.execute(f"""
  SELECT tag, COUNT(*) n
  FROM read_parquet('{SNAP_ANNUAL}')
  WHERE cik = {CIK}
  GROUP BY tag
  ORDER BY n DESC
  LIMIT 10
""").fetch_df()
print("Tag preview for CIK 320193:\n", preview)

# Annual Revenues (normalize tag)
df_annual = con.execute(f"""
  WITH src AS (
    SELECT cik,
           CASE WHEN position(':' IN tag) > 0 THEN split_part(tag, ':', 2) ELSE tag END AS tag_norm,
           fy, val
    FROM read_parquet('{SNAP_ANNUAL}')
    WHERE cik = {CIK}
  )
  SELECT fy, val AS Revenues
  FROM src
  WHERE tag_norm = 'Revenues'
  ORDER BY fy
""").fetch_df()

# Quarterly Revenues (normalize tag)
df_quarterly = con.execute(f"""
  WITH src AS (
    SELECT cik,
           CASE WHEN position(':' IN tag) > 0 THEN split_part(tag, ':', 2) ELSE tag END AS tag_norm,
           fy, fp, val
    FROM read_parquet('{SNAP_QUARTERLY}')
    WHERE cik = {CIK}
  )
  SELECT fy, fp, val AS Revenues
  FROM src
  WHERE tag_norm = 'Revenues' AND fp IN ('Q1','Q2','Q3','Q4')
  ORDER BY fy,
    CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE 0 END
""").fetch_df()

con.close()

# Save CSVs
annual_csv    = os.path.join(OUT_DIR, "AAPL_revenues_annual.csv")
quarterly_csv = os.path.join(OUT_DIR, "AAPL_revenues_quarterly.csv")
df_annual.to_csv(annual_csv, index=False)
df_quarterly.to_csv(quarterly_csv, index=False)
print("✅ Re-saved:\n -", annual_csv, "\n -", quarterly_csv)

# Charts
if not df_annual.empty:
    plt.figure(figsize=(9,4.5))
    plt.plot(df_annual["fy"], df_annual["Revenues"], marker="o")
    plt.title("Apple — Annual Revenues")
    plt.xlabel("Fiscal Year"); plt.ylabel("Revenues"); plt.grid(True, alpha=0.3); plt.show()

if not df_quarterly.empty:
    df_quarterly = df_quarterly.copy()
    df_quarterly["period"] = df_quarterly["fy"].astype(str) + "-" + df_quarterly["fp"]
    plt.figure(figsize=(11,4.5))
    plt.plot(df_quarterly["period"], df_quarterly["Revenues"], marker=".")
    plt.title("Apple — Quarterly Revenues")
    plt.xlabel("Fiscal Period"); plt.ylabel("Revenues"); plt.xticks(rotation=60)
    plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()
# Rebuild snapshots with normalized tag (no namespace) so filters like tag='Revenues' work everywhere
import os, sys, subprocess
try:
    import duckdb  # type: ignore
except ModuleNotFoundError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "duckdb"])
    import duckdb

V3   = r"C:\smilefund_project\warehouse\parquet\facts_by_group_v3\**\*.parquet"
SNAP = r"C:\smilefund_project\warehouse\parquet\snapshots"
os.makedirs(SNAP, exist_ok=True)

con = duckdb.connect()

# Source from hive-partitioned v3; normalize tag -> tag_norm, then expose it as 'tag'
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW facts_min AS
  SELECT
    CASE WHEN position(':' IN tag) > 0 THEN split_part(tag, ':', 2) ELSE tag END AS tag,  -- normalized
    "group",
    cik, fy, fp, val
  FROM parquet_scan('{V3}', HIVE_PARTITIONING=1)
  WHERE val IS NOT NULL
""")

# Rank helper for choosing "latest"
con.execute("""
  CREATE OR REPLACE TEMP VIEW facts_ranked AS
  SELECT
    *,
    CASE fp WHEN 'FY' THEN 5 WHEN 'Q4' THEN 4 WHEN 'Q3' THEN 3 WHEN 'Q2' THEN 2 WHEN 'Q1' THEN 1 ELSE 0 END AS fp_rank
  FROM facts_min
""")

# Overwrite snapshots using normalized tag
con.execute("""
  CREATE OR REPLACE TABLE latest_any AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag ORDER BY fy DESC, fp_rank DESC) rn
    FROM facts_ranked
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_any TO '{SNAP}\\latest_any.parquet' (FORMAT PARQUET);")

con.execute("""
  CREATE OR REPLACE TABLE latest_annual AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag, fy ORDER BY fp_rank DESC) rn
    FROM facts_ranked
    WHERE fp = 'FY'
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_annual TO '{SNAP}\\latest_annual.parquet' (FORMAT PARQUET);")

con.execute("""
  CREATE OR REPLACE TABLE latest_quarterly AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag, fy, fp ORDER BY fp_rank DESC) rn
    FROM facts_ranked
    WHERE fp IN ('Q1','Q2','Q3','Q4')
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_quarterly TO '{SNAP}\\latest_quarterly.parquet' (FORMAT PARQUET);")

con.close()
print("✅ Snapshots rebuilt with normalized tag.")
# Re-export Apple (CIK 320193) Revenues — annual + quarterly (normalized tag)
import os, duckdb
import pandas as pd

CIK = 320193
SNAP_ANNUAL    = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_annual.parquet"
SNAP_QUARTERLY = r"C:\smilefund_project\warehouse\parquet\snapshots\latest_quarterly.parquet"
OUT_DIR        = r"C:\smilefund_project\warehouse\reports"
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

df_annual = con.execute(f"""
  SELECT fy, val AS Revenues
  FROM read_parquet('{SNAP_ANNUAL}')
  WHERE cik = {CIK} AND tag = 'Revenues'
  ORDER BY fy
""").fetch_df()

df_quarterly = con.execute(f"""
  SELECT fy, fp, val AS Revenues
  FROM read_parquet('{SNAP_QUARTERLY}')
  WHERE cik = {CIK} AND tag = 'Revenues' AND fp IN ('Q1','Q2','Q3','Q4')
  ORDER BY fy,
    CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE 0 END
""").fetch_df()

con.close()

annual_csv    = os.path.join(OUT_DIR, "AAPL_revenues_annual.csv")
quarterly_csv = os.path.join(OUT_DIR, "AAPL_revenues_quarterly.csv")
df_annual.to_csv(annual_csv, index=False)
df_quarterly.to_csv(quarterly_csv, index=False)

print("Annual rows:", len(df_annual), "→", annual_csv)
print(df_annual.tail(10))
print("\nQuarterly rows:", len(df_quarterly), "→", quarterly_csv)
print(df_quarterly.tail(12))
# Diagnose Apple's tags + export Revenues with fallbacks
import os, duckdb, pandas as pd

CIK = 320193
SNAP_DIR = r"C:\smilefund_project\warehouse\parquet\snapshots"
ANNUAL   = os.path.join(SNAP_DIR, "latest_annual.parquet")
QUARTER  = os.path.join(SNAP_DIR, "latest_quarterly.parquet")
ANY      = os.path.join(SNAP_DIR, "latest_any.parquet")
OUT_DIR  = r"C:\smilefund_project\warehouse\reports"
os.makedirs(OUT_DIR, exist_ok=True)

con = duckdb.connect()

print("Distinct FP values for this CIK (annual + quarterly):")
print(con.execute(f"""
  SELECT fp, COUNT(*) n
  FROM (
    SELECT fp FROM read_parquet('{ANNUAL}') WHERE cik={CIK}
    UNION ALL
    SELECT fp FROM read_parquet('{QUARTER}') WHERE cik={CIK}
  )
  GROUP BY fp ORDER BY n DESC
""").fetch_df())

print("\nTop tags in latest_annual for this CIK:")
print(con.execute(f"""
  SELECT tag, COUNT(*) n
  FROM read_parquet('{ANNUAL}')
  WHERE cik={CIK}
  GROUP BY tag ORDER BY n DESC LIMIT 30
""").fetch_df())

REVENUE_CANDIDATES = (
    'Revenues',
    'SalesRevenueNet',
    'RevenueFromContractWithCustomerExcludingAssessedTax',
    'SalesRevenueGoodsAndServicesNet',
    'SalesRevenueGoodsNet',
    'SalesRevenueServicesNet',
    'TotalRevenueAndOtherIncome'
)

# Pick first tag that exists in annual
row = con.execute(f"""
  WITH t AS (
    SELECT tag
    FROM read_parquet('{ANNUAL}')
    WHERE cik={CIK} AND tag IN ({','.join([f"'{t}'" for t in REVENUE_CANDIDATES])})
    GROUP BY tag
  )
  SELECT tag FROM t ORDER BY
    CASE tag
      WHEN 'Revenues' THEN 1
      WHEN 'SalesRevenueNet' THEN 2
      WHEN 'RevenueFromContractWithCustomerExcludingAssessedTax' THEN 3
      WHEN 'SalesRevenueGoodsAndServicesNet' THEN 4
      WHEN 'SalesRevenueGoodsNet' THEN 5
      WHEN 'SalesRevenueServicesNet' THEN 6
      WHEN 'TotalRevenueAndOtherIncome' THEN 7
      ELSE 99
    END
  LIMIT 1
""").fetchone()

chosen_tag = row[0] if row else None
print("\nChosen revenue-like tag (annual):", chosen_tag)

# Export annual (prefer FY; if none, use latest_any as fallback)
if chosen_tag:
    df_annual = con.execute(f"""
      SELECT fy, val AS Revenues
      FROM read_parquet('{ANNUAL}')
      WHERE cik={CIK} AND tag='{chosen_tag}'
      ORDER BY fy
    """).fetch_df()
else:
    df_annual = pd.DataFrame(columns=["fy","Revenues"])

if df_annual.empty:
    # fallback to latest_any (single row per (cik, tag))
    print("\nNo annual FY rows found for candidates; falling back to latest_any.")
    df_any = con.execute(f"""
      SELECT tag, val AS Revenues
      FROM read_parquet('{ANY}')
      WHERE cik={CIK} AND tag IN ({','.join([f"'{t}'" for t in REVENUE_CANDIDATES])})
      ORDER BY Revenues DESC NULLS LAST
      LIMIT 1
    """).fetch_df()
    print(df_any)

# Export quarterly using same chosen tag (if exists there), else try other candidates
if chosen_tag:
    df_quarter = con.execute(f"""
      SELECT fy, fp, val AS Revenues
      FROM read_parquet('{QUARTER}')
      WHERE cik={CIK} AND tag='{chosen_tag}' AND fp IN ('Q1','Q2','Q3','Q4')
      ORDER BY fy,
        CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE 0 END
    """).fetch_df()
else:
    df_quarter = pd.DataFrame(columns=["fy","fp","Revenues"])

if df_quarter.empty:
    # try other candidates for quarterly
    df_quarter = con.execute(f"""
      SELECT fy, fp, val AS Revenues, tag
      FROM read_parquet('{QUARTER}')
      WHERE cik={CIK} AND fp IN ('Q1','Q2','Q3','Q4') AND tag IN ({','.join([f"'{t}'" for t in REVENUE_CANDIDATES])})
      ORDER BY fy,
        CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 ELSE 0 END
    """).fetch_df()
    print("\nQuarterly fallback rows (with tag shown):")
    print(df_quarter.head(12))

con.close()

annual_csv    = os.path.join(OUT_DIR, "AAPL_revenues_annual.csv")
quarterly_csv = os.path.join(OUT_DIR, "AAPL_revenues_quarterly.csv")
df_annual.to_csv(annual_csv, index=False)
df_quarter.to_csv(quarterly_csv, index=False)

print("\n✅ Wrote CSVs:")
print(" -", annual_csv, f"({len(df_annual)} rows)")
print(" -", quarterly_csv, f"({len(df_quarter)} rows)")
print("\nIf annual is empty or the chosen tag isn't 'Revenues', tell me what the 'Top tags' table shows and we'll lock in the right alias.")
# Rebuild snapshots with normalized tag and filter out blank/__HIVE_DEFAULT_PARTITION__
import os, sys, subprocess
try:
    import duckdb  # type: ignore
except ModuleNotFoundError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "duckdb"])
    import duckdb

V3   = r"C:\smilefund_project\warehouse\parquet\facts_by_group_v3\**\*.parquet"
SNAP = r"C:\smilefund_project\warehouse\parquet\snapshots"
os.makedirs(SNAP, exist_ok=True)

con = duckdb.connect()

# 1) Read from hive-partitioned files, normalize tag, and DROP bad tags
con.execute(f"""
  CREATE OR REPLACE TEMP VIEW facts_min AS
  SELECT
    CASE WHEN position(':' IN tag) > 0 THEN split_part(tag, ':', 2) ELSE tag END AS tag,
    "group", cik, fy, fp, val
  FROM parquet_scan('{V3}', HIVE_PARTITIONING=1)
  WHERE val IS NOT NULL
    AND tag IS NOT NULL
    AND tag <> ''
    AND tag <> '__HIVE_DEFAULT_PARTITION__'
""")

# 2) Rank helper for "latest"
con.execute("""
  CREATE OR REPLACE TEMP VIEW facts_ranked AS
  SELECT
    *,
    CASE fp WHEN 'FY' THEN 5 WHEN 'Q4' THEN 4 WHEN 'Q3' THEN 3 WHEN 'Q2' THEN 2 WHEN 'Q1' THEN 1 ELSE 0 END AS fp_rank
  FROM facts_min
""")

# 3) Write snapshots
con.execute("""
  CREATE OR REPLACE TABLE latest_any AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag ORDER BY fy DESC, fp_rank DESC) rn
    FROM facts_ranked
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_any TO '{SNAP}\\latest_any.parquet' (FORMAT PARQUET);")

con.execute("""
  CREATE OR REPLACE TABLE latest_annual AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag, fy ORDER BY fp_rank DESC) rn
    FROM facts_ranked
    WHERE fp = 'FY'
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_annual TO '{SNAP}\\latest_annual.parquet' (FORMAT PARQUET);")

con.execute("""
  CREATE OR REPLACE TABLE latest_quarterly AS
  SELECT * FROM (
    SELECT cik, tag, fy, fp, val,
           ROW_NUMBER() OVER (PARTITION BY cik, tag, fy, fp ORDER BY fp_rank DESC) rn
    FROM facts_ranked
    WHERE fp IN ('Q1','Q2','Q3','Q4')
  ) WHERE rn = 1
""")
con.execute(f"COPY latest_quarterly TO '{SNAP}\\latest_quarterly.parquet' (FORMAT PARQUET);")

# quick sanity: how many rows now for Apple in latest_annual?
apple = con.execute("""
  SELECT tag, COUNT(*) n
  FROM latest_annual
  WHERE cik = 320193
  GROUP BY tag ORDER BY n DESC
""").fetch_df()
con.close()

print("✅ Snapshots rebuilt (bad tags filtered).")
print("Apple tags present in latest_annual now:\n", apple.head(10))
